{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 MNIST dataset\n",
    "## 1.1.1 Download MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "CPU times: user 521 ms, sys: 267 ms, total: 788 ms\n",
      "Wall time: 3.95 s\n"
     ]
    }
   ],
   "source": [
    "# will download automatically when there is no data in \"MNIST_data/\"\n",
    "%time mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55000, 784)\n",
      "(55000, 10)\n"
     ]
    }
   ],
   "source": [
    "# check size of training set\n",
    "print(mnist.train.images.shape)\n",
    "print(mnist.train.labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 784)\n",
      "(5000, 10)\n"
     ]
    }
   ],
   "source": [
    "# check size of validation set\n",
    "print(mnist.validation.images.shape)\n",
    "print(mnist.validation.labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 784)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# check size of test set\n",
    "print(mnist.test.images.shape)\n",
    "print(mnist.test.labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# check the 0th image\n",
    "print(mnist.train.images[0, :20])\n",
    "\n",
    "# check its label\n",
    "print(mnist.train.labels[0, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1.2 Save MNIST dataset into images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.misc\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# folder to save images\n",
    "save_dir = 'MNIST_data/images/'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28.2 ms, sys: 1.94 ms, total: 30.2 ms\n",
      "Wall time: 111 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saucecat/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:7: DeprecationWarning: `toimage` is deprecated!\n",
      "`toimage` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use Pillow's ``Image.fromarray`` directly instead.\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# save the top 20 images\n",
    "for i in range(20):\n",
    "    image_array = mnist.train.images[i, :]\n",
    "    image_array = image_array.reshape(28, 28)\n",
    "    \n",
    "    filename = os.path.join(save_dir, 'mnist_train_%d.jpg' % i)\n",
    "    scipy.misc.toimage(image_array, cmin=0.0, cmax=1.0).save(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1.3 Label one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnist_train_0.jpg label: 7\n",
      "mnist_train_1.jpg label: 3\n",
      "mnist_train_2.jpg label: 4\n",
      "mnist_train_3.jpg label: 6\n",
      "mnist_train_4.jpg label: 1\n",
      "mnist_train_5.jpg label: 8\n",
      "mnist_train_6.jpg label: 1\n",
      "mnist_train_7.jpg label: 0\n",
      "mnist_train_8.jpg label: 9\n",
      "mnist_train_9.jpg label: 8\n",
      "mnist_train_10.jpg label: 0\n",
      "mnist_train_11.jpg label: 3\n",
      "mnist_train_12.jpg label: 1\n",
      "mnist_train_13.jpg label: 2\n",
      "mnist_train_14.jpg label: 7\n",
      "mnist_train_15.jpg label: 0\n",
      "mnist_train_16.jpg label: 2\n",
      "mnist_train_17.jpg label: 9\n",
      "mnist_train_18.jpg label: 6\n",
      "mnist_train_19.jpg label: 0\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    one_hot_label = mnist.train.labels[i, :]\n",
    "    label = np.argmax(one_hot_label)\n",
    "    print('mnist_train_%d.jpg label: %d' %(i, label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 Classification task with TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# x represents input images\n",
    "# None means we don't know the batch size yet\n",
    "x = tf.placeholder(dtype=tf.float32, shape=[None, 784])\n",
    "\n",
    "# W represents trainable parameters, use Variable\n",
    "W = tf.Variable(tf.zeros(shape=[784, 10]))\n",
    "b = tf.Variable(tf.zeros(shape=[10]))\n",
    "\n",
    "# y represents model output\n",
    "probs = tf.nn.softmax(tf.matmul(x, W) + b)\n",
    "\n",
    "# we need to specify the true label\n",
    "# y need to be one-hot encoded\n",
    "y = tf.placeholder(dtype=tf.float32, shape=[None, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate loss using probs and y\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y * tf.log(probs)))\n",
    "\n",
    "# reduce the loss by optimizing parameters\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training...\n"
     ]
    }
   ],
   "source": [
    "# create a session to train the model\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# initialize all parameters\n",
    "tf.global_variables_initializer().run()\n",
    "print('start training...')\n",
    "\n",
    "for i in range(1000):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(batch_size=100)\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y: batch_ys})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9179\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "correct_prediction = tf.equal(tf.argmax(probs, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "print(sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.2 CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# some useful functions\n",
    "\n",
    "# The generated values follow a normal distribution with \n",
    "# specified mean and standard deviation\n",
    "# except that values whose magnitude is more than 2 standard deviations \n",
    "# from the mean are dropped and re-picked\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "    \n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "    \n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the graph\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "\n",
    "# padding = SAME\n",
    "W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "b_conv1 = bias_variable([32])\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7 * 7 * 64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "W_fc2 = weight_variable([1024, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the loss\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=y_conv))\n",
    "\n",
    "# define train step\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "# define accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a session to train model\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(20000):\n",
    "    batch = mnist.train.next_batch(32)\n",
    "    if i % 100 == 0:\n",
    "        # when test accuracy, need to turn off the dropout\n",
    "        train_accuracy = accuracy.eval(feed_dict={x: batch[0], y: batch[1], keep_prob: 1.0})\n",
    "        print('step %d, training accuracy %g' %(i, train_accuracy))\n",
    "    \n",
    "    train_step.run(feed_dict={x: batch[0], y: batch[1], keep_prob: 0.5})\n",
    "    \n",
    "print('test accuracy %g' % accuracy.eval(feed_dict={x: mnist.test.images, y: mnist.test.labels, keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## try to use tf.layers instead of tf.nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct using tf.layers\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "\n",
    "h_conv1 = tf.layers.conv2d(inputs=x_image, filters=32, kernel_size=5, strides=(1, 1), padding='same', \n",
    "                           activation=tf.nn.relu, use_bias=True, \n",
    "                           kernel_initializer=tf.truncated_normal_initializer(stddev=0.1), \n",
    "                           bias_initializer=tf.constant_initializer(value=0.1))\n",
    "h_pool1 = tf.layers.max_pooling2d(inputs=h_conv1, pool_size=2, strides=(2, 2), padding='same')\n",
    "\n",
    "h_conv2 = tf.layers.conv2d(inputs=h_pool1, filters=64, kernel_size=5, strides=(1, 1), padding='same', \n",
    "                           activation=tf.nn.relu, use_bias=True, \n",
    "                           kernel_initializer=tf.truncated_normal_initializer(stddev=0.1), \n",
    "                           bias_initializer=tf.constant_initializer(value=0.1))\n",
    "h_pool2 = tf.layers.max_pooling2d(inputs=h_conv2, pool_size=2, strides=(2, 2), padding='same')\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7 * 7 * 64])\n",
    "h_fc1 = tf.layers.dense(inputs=h_pool2_flat, units=1024, activation=tf.nn.relu, use_bias=True, \n",
    "                        kernel_initializer=tf.truncated_normal_initializer(stddev=0.1), \n",
    "                        bias_initializer=tf.constant_initializer(value=0.1))\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.layers.dropout(inputs=h_fc1, rate=1.-keep_prob)\n",
    "\n",
    "y_conv = tf.layers.dense(inputs=h_fc1_drop, units=10, use_bias=True, \n",
    "                         kernel_initializer=tf.truncated_normal_initializer(stddev=0.1), \n",
    "                         bias_initializer=tf.constant_initializer(value=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the loss\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=y_conv))\n",
    "\n",
    "# define train step\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "# define accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a session to train model\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(20000):\n",
    "    batch = mnist.train.next_batch(32)\n",
    "    if i % 100 == 0:\n",
    "        # when test accuracy, need to turn off the dropout\n",
    "        train_accuracy = accuracy.eval(feed_dict={x: batch[0], y: batch[1], keep_prob: 1.0})\n",
    "        print('step %d, training accuracy %g' %(i, train_accuracy))\n",
    "    \n",
    "    train_step.run(feed_dict={x: batch[0], y: batch[1], keep_prob: 0.5})\n",
    "    \n",
    "print('test accuracy %g' % accuracy.eval(feed_dict={x: mnist.test.images, y: mnist.test.labels, keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Official Website for MNIST: http://yann.lecun.com/exdb/mnist/\n",
    "- TensorFlow official tutorial with MNIST: https://www.tensorflow.org/get_started/mnist/beginners and https://www.tensorflow.org/get_started/mnist/pros\n",
    "- Tensor: https://www.tensorflow.org/programmers_guide/tensors\n",
    "- Variables: https://www.tensorflow.org/programmers_guide/variables\n",
    "- Session: https://www.tensorflow.org/programmers_guide/graphs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
