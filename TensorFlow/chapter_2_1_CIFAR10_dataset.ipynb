{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 CIFAR-10 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.1 CIFAR-10 summary\n",
    "- 3 color channels\n",
    "- width 32 height 32\n",
    "- 10 classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.2 Download CIFAR-10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cifar10\n",
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FLAGS = tf.app.flags.FLAGS\n",
    "FLAGS.data_dir = 'cifar10_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we don't use the function directly\n",
    "# we try to look into the function in depth\n",
    "# cifar10.maybe_download_and_extract()\n",
    "\n",
    "def maybe_download_and_extract():\n",
    "    # download and extract the tarball from Alex's website\n",
    "    dest_directory = FLAGS.data_dir\n",
    "    if not os.path.exists(dest_directory):\n",
    "        os.makedirs(dest_directory)\n",
    "    \n",
    "    DATA_URL = 'http://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz'\n",
    "    filename = DATA_URL.split('/')[-1]\n",
    "    filepath = os.path.join(dest_directory, filename)\n",
    "    if not os.path.exists(filepath):\n",
    "        def _progress(count, block_size, total_size):\n",
    "            sys.stdout.write('\\r>> Downloading %s %.1f%%' \n",
    "                             % (filename, float(count * block_size) / float(total_size) * 100.))\n",
    "            sys.stdout.flush()\n",
    "        \n",
    "        filepath, _ = urllib.urlretrieve(DATA_URL, filepath, _progress)\n",
    "        print()\n",
    "        statinfo = os.stat(filepath)\n",
    "        print('Successfully downloaded', filename, statinfo.st_size, 'bytes.')\n",
    "    \n",
    "    extracted_dir_path = os.path.join(dest_directory, 'cifar-10-batches-bin')\n",
    "    if not os.path.exists(extracted_dir_path):\n",
    "        tarfile.open(filepath, 'r:gz').extractall(dest_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Downloading cifar-10-binary.tar.gz 100.0%()\n",
      "('Successfully downloaded', 'cifar-10-binary.tar.gz', 170052171, 'bytes.')\n"
     ]
    }
   ],
   "source": [
    "maybe_download_and_extract()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.3 TensorFlow data reading mechanisim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('read'):\n",
    "    os.makedirs('read/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    # 3 images to read\n",
    "    filename = ['A.jpg', 'B.jpg', 'C.jpg']\n",
    "    \n",
    "    # string_input_producer will produce a queue of file names\n",
    "    filename_queue = tf.train.string_input_producer(filename, shuffle=False, num_epochs=5)\n",
    "    \n",
    "    # reader read data from file paths\n",
    "    reader = tf.WholeFileReader()\n",
    "    key, value = reader.read(filename_queue)\n",
    "    \n",
    "    tf.local_variables_initializer().run()\n",
    "    threads = tf.train.start_queue_runners(sess=sess)\n",
    "    \n",
    "    i = 0\n",
    "    while True:\n",
    "        i += 1\n",
    "        image_data = sess.run(value)\n",
    "        with open('read/test_%d.jpg' % i, 'wb') as f:\n",
    "            f.write(image_data)\n",
    "            \n",
    "# at the end, there would be an OutOfRangeError\n",
    "# which means the queue is finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.ops.data_flow_ops.FIFOQueue at 0x7f39cc3d6710>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename_queue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.4 Save CIFAR-10 dataset as images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_cifar10(filename_queue):\n",
    "    # read and parse examples from CIFAR10 data files\n",
    "    class CIFAR10Record(object):\n",
    "        pass\n",
    "    result = CIFAR10Record()\n",
    "    \n",
    "    # dimensions of images in the CIFAR-10 dataset\n",
    "    label_bytes = 1\n",
    "    result.height = 32\n",
    "    result.width = 32\n",
    "    result.depth = 3\n",
    "    image_bytes = result.height * result.width * result.depth\n",
    "    record_bytes = label_bytes + image_bytes\n",
    "    \n",
    "    reader = tf.FixedLengthRecordReader(record_bytes=record_bytes)\n",
    "    result.key, value = reader.read(filename_queue)\n",
    "    \n",
    "    record_bytes = tf.decode_raw(value, tf.uint8)\n",
    "    result.label = tf.cast(tf.strided_slice(record_bytes, [0], [label_bytes]), tf.int32)\n",
    "    \n",
    "    depth_major = tf.reshape(\n",
    "        tf.strided_slice(record_bytes, [label_bytes], [label_bytes + image_bytes]), \n",
    "        [result.depth, result.height, result.width])\n",
    "    result.uint8image = tf.transpose(depth_major, [1, 2, 0])\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def inputs_origin(data_dir):\n",
    "    filenames = [os.path.join(data_dir, 'data_batch_%d.bin' % i) for i in xrange(1, 6)]\n",
    "    for f in filenames:\n",
    "        if not tf.gfile.Exists(f):\n",
    "            raise ValueError('Failed to find file: ' + f)\n",
    "    \n",
    "    filename_queue = tf.train.string_input_producer(filenames)\n",
    "    read_input = read_cifar10(filename_queue)\n",
    "    \n",
    "    reshaped_image = tf.cast(read_input.uint8image, tf.float32)\n",
    "    \n",
    "    return reshaped_image\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    reshaped_image = inputs_origin('cifar10_data/cifar-10-batches-bin')\n",
    "    threads = tf.train.start_queue_runners(sess=sess)\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    if not os.path.exists('cifar10_data/images/'):\n",
    "        os.makedirs('cifar10_data/images/')\n",
    "    \n",
    "    for i in range(30):\n",
    "        image_array = sess.run(reshaped_image)\n",
    "        scipy.misc.toimage(image_array).save('cifar10_data/images/%d.jpg' % i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
