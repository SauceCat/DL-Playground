{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2.1 Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saucecat/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## meta data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 24\n",
    "NUM_CLASSES = 10\n",
    "NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN = 50000\n",
    "NUM_EXAMPLES_PER_EPOCH_FOR_EVAL = 10000\n",
    "\n",
    "MOVING_AVERAGE_DECAY = 0.9999   \n",
    "NUM_EPOCHS_PER_DECAY = 350.0   \n",
    "LEARNING_RATE_DECAY_FACTOR = 0.1\n",
    "INITIAL_LEARNING_RATE = 0.1\n",
    "TOWER_NAME = 'tower'\n",
    "\n",
    "# this is mostly for terminal run\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "# Basic model parameters.\n",
    "tf.app.flags.DEFINE_integer('batch_size', 128,\n",
    "                            \"\"\"Number of images to process in a batch.\"\"\")\n",
    "tf.app.flags.DEFINE_string('data_dir', 'cifar10_data',\n",
    "                           \"\"\"Path to the CIFAR-10 data directory.\"\"\")\n",
    "tf.app.flags.DEFINE_boolean('use_fp16', False,\n",
    "                            \"\"\"Train the model using fp16.\"\"\")\n",
    "tf.app.flags.DEFINE_string('train_dir', '/tmp/cifar10_train',\n",
    "                           \"\"\"Directory where to write event logs \"\"\"\n",
    "                           \"\"\"and checkpoint.\"\"\")\n",
    "tf.app.flags.DEFINE_integer('max_steps', 10000,\n",
    "                            \"\"\"Number of batches to run.\"\"\")\n",
    "tf.app.flags.DEFINE_boolean('log_device_placement', False,\n",
    "                            \"\"\"Whether to log device placement.\"\"\")\n",
    "tf.app.flags.DEFINE_integer('log_frequency', 10,\n",
    "                            \"\"\"How often to log results to the console.\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create data batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_cifar10(filename_queue):\n",
    "    # read and parse examples from CIFAR10 data files\n",
    "    class CIFAR10Record(object):\n",
    "        pass\n",
    "    result = CIFAR10Record()\n",
    "    \n",
    "    # dimensions of images in the CIFAR-10 dataset\n",
    "    label_bytes = 1\n",
    "    result.height = 32\n",
    "    result.width = 32\n",
    "    result.depth = 3\n",
    "    image_bytes = result.height * result.width * result.depth\n",
    "    record_bytes = label_bytes + image_bytes\n",
    "    \n",
    "    reader = tf.FixedLengthRecordReader(record_bytes=record_bytes)\n",
    "    result.key, value = reader.read(filename_queue)\n",
    "    \n",
    "    record_bytes = tf.decode_raw(value, tf.uint8)\n",
    "    result.label = tf.cast(tf.strided_slice(record_bytes, [0], [label_bytes]), tf.int32)\n",
    "    \n",
    "    depth_major = tf.reshape(\n",
    "        tf.strided_slice(record_bytes, [label_bytes], [label_bytes + image_bytes]), \n",
    "        [result.depth, result.height, result.width])\n",
    "    result.uint8image = tf.transpose(depth_major, [1, 2, 0])\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def _generate_image_and_label_batch(image, label, min_queue_examples, batch_size, shuffle):\n",
    "    # create a queue that shuffles the examples\n",
    "    # and then read 'batch_size' images + labels from the example queue\n",
    "    \n",
    "    num_preprocess_threads = 16\n",
    "    if shuffle:\n",
    "        images, label_batch = tf.train.shuffle_batch(\n",
    "            [image, label], \n",
    "            batch_size=batch_size, \n",
    "            num_threads=num_preprocess_threads, \n",
    "            capacity=min_queue_examples + 3 * batch_size, \n",
    "            min_after_dequeue=min_queue_examples)\n",
    "    else:\n",
    "        images, label_batch = tf.train.batch(\n",
    "            [image, label], \n",
    "            batch_size=batch_size, \n",
    "            num_threads=num_preprocess_threads, \n",
    "            capacity=min_queue_examples + 3 * batch_size)\n",
    "        \n",
    "    tf.summary.image('images', images)\n",
    "    \n",
    "    return images, tf.reshape(label_batch, [batch_size])\n",
    "\n",
    "def distorted_inputs(data_dir, batch_size):\n",
    "    # construct distorteed input for CIFAR training \n",
    "    filenames = [os.path.join(data_dir, 'data_batch_%d.bin' % i) for i in xrange(1, 6)]\n",
    "    \n",
    "    for f in filenames:\n",
    "        if not tf.gfile.Exists(f):\n",
    "            raise ValueError('Failed to find file: ' + f)\n",
    "            \n",
    "    # create a queue that produces the filenames to read\n",
    "    filename_queue = tf.train.string_input_producer(filenames)\n",
    "    \n",
    "    # read examples from files in the filename queue\n",
    "    read_input = read_cifar10(filename_queue)\n",
    "    reshaped_image = tf.cast(read_input.uint8image, tf.float32)\n",
    "    \n",
    "    height = width = IMAGE_SIZE\n",
    "    \n",
    "    # image processing for training the network\n",
    "    # many distortions applied to the image\n",
    "    \n",
    "    # randomly crop a [height, width] section of the image\n",
    "    distorted_image = tf.random_crop(reshaped_image, [height, width, 3])\n",
    "    \n",
    "    # randomly flip the image horizontally\n",
    "    distorted_image = tf.image.random_flip_left_right(distorted_image)\n",
    "    \n",
    "    distorted_image = tf.image.random_brightness(distorted_image, max_delta=63)\n",
    "    distorted_image = tf.image.random_contrast(distorted_image, lower=0.2, upper=1.8)\n",
    "    \n",
    "    # substract off the mean and divide by the variance of the pixels\n",
    "    float_image = tf.image.per_image_standardization(distorted_image)\n",
    "    \n",
    "    # set the shapes of tensors\n",
    "    float_image.set_shape([height, width, 3])\n",
    "    read_input.label.set_shape([1])\n",
    "    \n",
    "    # ensure that the random shuffling has good mixing properties\n",
    "    min_fraction_of_examples_in_queue = 0.4\n",
    "    min_queue_examples = int(NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN * min_fraction_of_examples_in_queue)\n",
    "    print('Filling queue with %d CIFAR images before starting to train. '\n",
    "          'This will take a few minutes. ' % min_queue_examples)\n",
    "    \n",
    "    # generate a batch of images and labels by building up a queue of examples\n",
    "    return _generate_image_and_label_batch(float_image, read_input.label, \n",
    "                                           min_queue_examples, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 2.2.2 Recognition model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _variable_with_weight_decay(name, shape, stddev, wd):\n",
    "    # create an initialized variable with weight decay\n",
    "    initializer = tf.truncated_normal_initializer(stddev=stddev, dtype=tf.float32)\n",
    "    var = tf.get_variable(name, shape, initializer=initializer, dtype=tf.float32)\n",
    "    \n",
    "    if wd is not None:\n",
    "        weight_decay = tf.multiply(tf.nn.l2_loss(var), wd, name='weight_loss')\n",
    "        # add l2 loss into global losses collection\n",
    "        tf.add_to_collection('losses', weight_decay)\n",
    "    \n",
    "    return var\n",
    "\n",
    "def _activation_summary(x):\n",
    "    # tensorboard summary\n",
    "    tensor_name = x.op.name\n",
    "    tf.summary.histogram(tensor_name + '/activations', x)\n",
    "    tf.summary.scalar(tensor_name + '/sparsity', tf.nn.zero_fraction(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inference(images):\n",
    "    # build the CIFAR-10 model\n",
    "    # we instantiate all variables using tf.get_variable() instead of\n",
    "    # tf.Variable() in order to share variables across multiple GPU training runs\n",
    "    with tf.variable_scope('conv1') as scope:\n",
    "        kernel = _variable_with_weight_decay('weights', shape=[5, 5, 3, 64], stddev=5e-2, wd=0.0)\n",
    "        conv = tf.nn.conv2d(images, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "        biases = tf.get_variable('biases', [64], initializer=tf.constant_initializer(0.0), dtype=tf.float32)\n",
    "        pre_activation = tf.nn.bias_add(conv, biases)\n",
    "        conv1 = tf.nn.relu(pre_activation, name=scope.name)\n",
    "        _activation_summary(conv1)\n",
    "        \n",
    "    pool1 = tf.nn.max_pool(conv1, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='SAME', name='pool1')\n",
    "    # local response normalization\n",
    "    norm1 = tf.nn.lrn(pool1, 4, bias=1.0, alpha=0.001 / 9., beta=0.75, name='norm1')\n",
    "    \n",
    "    with tf.variable_scope('conv2') as scope:\n",
    "        kernel = _variable_with_weight_decay('weights', shape=[5, 5, 64, 64], stddev=5e-2, wd=0.0)\n",
    "        conv = tf.nn.conv2d(norm1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "        biases = tf.get_variable('biases', [64], tf.float32, tf.constant_initializer(0.1))\n",
    "        pre_activation = tf.nn.bias_add(conv, biases)\n",
    "        conv2 = tf.nn.relu(pre_activation, name=scope.name)\n",
    "        _activation_summary(conv2)\n",
    "        \n",
    "    norm2 = tf.nn.lrn(conv2, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name='norm2')\n",
    "    pool2 = tf.nn.max_pool(norm2, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='SAME', name='pool2')\n",
    "        \n",
    "    with tf.variable_scope('local3') as scope:\n",
    "        reshape = tf.reshape(pool2, [FLAGS.batch_size, -1])\n",
    "        dim = reshape.get_shape()[1].value\n",
    "        weights = _variable_with_weight_decay('weights', shape=[dim, 384], stddev=0.04, wd=0.004)\n",
    "        biases = tf.get_variable('biases', [384], tf.float32, tf.constant_initializer(0.1))\n",
    "        local3 = tf.nn.relu(tf.matmul(reshape, weights) + biases, name=scope.name)\n",
    "        _activation_summary(local3)\n",
    "        \n",
    "    with tf.variable_scope('local4') as scope:\n",
    "        weights = _variable_with_weight_decay('weights', shape=[384, 192], stddev=0.04, wd=0.004)\n",
    "        biases = tf.get_variable('biases', [192], tf.float32, tf.constant_initializer(0.1))\n",
    "        local4 = tf.nn.relu(tf.matmul(local3, weights) + biases, name=scope.name)\n",
    "        _activation_summary(local4)\n",
    "        \n",
    "    with tf.variable_scope('softmax_linear') as scope:\n",
    "        weights = _variable_with_weight_decay('weights', [192, NUM_CLASSES], stddev=1 / 192., wd=0.0)\n",
    "        biases = tf.get_variable('biases', [NUM_CLASSES], tf.float32, tf.constant_initializer(0.0))\n",
    "        softmax_linear = tf.add(tf.matmul(local4, weights), biases, name=scope.name)\n",
    "        _activation_summary(softmax_linear)\n",
    "        \n",
    "    return softmax_linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_loss(logits, labels):\n",
    "    labels = tf.cast(labels, tf.int64)\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        labels=labels, logits=logits, name='cross_entropy_per_example')\n",
    "    cross_entropy_mean = tf.reduce_mean(cross_entropy, name='cross_entropy')\n",
    "    tf.add_to_collection('losses', cross_entropy_mean)\n",
    "    \n",
    "    return tf.add_n(tf.get_collection('losses'), name='total_loss')\n",
    "\n",
    "def _add_loss_summaries(total_loss):\n",
    "    loss_averages = tf.train.ExponentialMovingAverage(0.9, name='avg')\n",
    "    losses = tf.get_collection('losses')\n",
    "    loss_averages_op = loss_averages.apply(losses + [total_loss])\n",
    "    \n",
    "    for l in losses + [total_loss]:\n",
    "        tf.summary.scalar(l.op.name + '_raw', l)\n",
    "        tf.summary.scalar(l.op.name, loss_averages.average(l))\n",
    "    \n",
    "    return loss_averages_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(total_loss, global_step):\n",
    "    num_batches_per_epoch = NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN / FLAGS.batch_size\n",
    "    decay_steps = int(num_batches_per_epoch * NUM_EPOCHS_PER_DECAY)\n",
    "    \n",
    "    # decay the learning rate exponentially based on the number of steps\n",
    "    lr = tf.train.exponential_decay(INITIAL_LEARNING_RATE, global_step, decay_steps, \n",
    "                                    LEARNING_RATE_DECAY_FACTOR, staircase=True)\n",
    "    tf.summary.scalar('learning_rate', lr)\n",
    "    \n",
    "    loss_averages_op = _add_loss_summaries(total_loss)\n",
    "    \n",
    "    # compute gradients\n",
    "    with tf.control_dependencies([loss_averages_op]):\n",
    "        opt = tf.train.GradientDescentOptimizer(lr)\n",
    "        grads = opt.compute_gradients(total_loss)\n",
    "    \n",
    "    # apply gradients\n",
    "    apply_gradient_op = opt.apply_gradients(grads, global_step=global_step)\n",
    "    \n",
    "    # Add histograms for trainable variables.\n",
    "    for var in tf.trainable_variables():\n",
    "        tf.summary.histogram(var.op.name, var)\n",
    "\n",
    "    # Add histograms for gradients.\n",
    "    for grad, var in grads:\n",
    "        if grad is not None:\n",
    "            tf.summary.histogram(var.op.name + '/gradients', grad)\n",
    "            \n",
    "    # Track the moving averages of all trainable variables.\n",
    "    variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)\n",
    "    variables_averages_op = variable_averages.apply(tf.trainable_variables())\n",
    "    \n",
    "    with tf.control_dependencies([apply_gradient_op, variables_averages_op]):\n",
    "        train_op = tf.no_op(name='train')\n",
    "        \n",
    "    return train_op "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.3 Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "def train_model():\n",
    "    with tf.Graph().as_default():\n",
    "        global_step = tf.contrib.framework.get_or_create_global_step()\n",
    "        \n",
    "        if not FLAGS.data_dir:\n",
    "            raise ValueError('Please supply a data_dir')\n",
    "            \n",
    "        data_dir = os.path.join(FLAGS.data_dir, 'cifar-10-batches-bin')\n",
    "        images, labels = distorted_inputs(data_dir=data_dir,batch_size=FLAGS.batch_size)\n",
    "        \n",
    "        logits = inference(images)\n",
    "        loss = calc_loss(logits, labels)\n",
    "        \n",
    "        train_op = train(loss, global_step)\n",
    "        \n",
    "    \n",
    "        class _LoggerHook(tf.train.SessionRunHook):\n",
    "            \"\"\"Logs loss and runtime.\"\"\"\n",
    "\n",
    "            def begin(self):\n",
    "                self._step = -1\n",
    "                self._start_time = time.time()\n",
    "\n",
    "            def before_run(self, run_context):\n",
    "                self._step += 1\n",
    "                return tf.train.SessionRunArgs(loss)  # Asks for loss value.\n",
    "\n",
    "            def after_run(self, run_context, run_values):\n",
    "                if self._step % FLAGS.log_frequency == 0:\n",
    "                    current_time = time.time()\n",
    "                    duration = current_time - self._start_time\n",
    "                    self._start_time = current_time\n",
    "\n",
    "                    loss_value = run_values.results\n",
    "                    examples_per_sec = FLAGS.log_frequency * FLAGS.batch_size / duration\n",
    "                    sec_per_batch = float(duration / FLAGS.log_frequency)\n",
    "\n",
    "                    format_str = ('%s: step %d, loss = %.2f (%.1f examples/sec; %.3f sec/batch)')\n",
    "                    print(format_str % (datetime.now(), self._step, loss_value, examples_per_sec, sec_per_batch))\n",
    "\n",
    "        with tf.train.MonitoredTrainingSession(\n",
    "            checkpoint_dir=FLAGS.train_dir,\n",
    "            hooks=[tf.train.StopAtStepHook(last_step=FLAGS.max_steps),\n",
    "                   tf.train.NanTensorHook(loss),\n",
    "                   _LoggerHook()],\n",
    "            config=tf.ConfigProto(\n",
    "                log_device_placement=FLAGS.log_device_placement)) as mon_sess: \n",
    "            while not mon_sess.should_stop(): mon_sess.run(train_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-9-6fcb471b2c8e>:6: get_or_create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to tf.train.get_or_create_global_step\n",
      "Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes. \n",
      "INFO:tensorflow:Summary name conv1/weight_loss (raw) is illegal; using conv1/weight_loss__raw_ instead.\n",
      "INFO:tensorflow:Summary name conv2/weight_loss (raw) is illegal; using conv2/weight_loss__raw_ instead.\n",
      "INFO:tensorflow:Summary name local3/weight_loss (raw) is illegal; using local3/weight_loss__raw_ instead.\n",
      "INFO:tensorflow:Summary name local4/weight_loss (raw) is illegal; using local4/weight_loss__raw_ instead.\n",
      "INFO:tensorflow:Summary name softmax_linear/weight_loss (raw) is illegal; using softmax_linear/weight_loss__raw_ instead.\n",
      "INFO:tensorflow:Summary name cross_entropy (raw) is illegal; using cross_entropy__raw_ instead.\n",
      "INFO:tensorflow:Summary name total_loss (raw) is illegal; using total_loss__raw_ instead.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/cifar10_train/model.ckpt-4\n",
      "INFO:tensorflow:Saving checkpoints for 5 into /tmp/cifar10_train/model.ckpt.\n",
      "2018-04-28 18:24:54.693340: step 0, loss = 4.65 (154.0 examples/sec; 0.831 sec/batch)\n",
      "2018-04-28 18:24:55.174809: step 10, loss = 4.54 (2658.5 examples/sec; 0.048 sec/batch)\n",
      "2018-04-28 18:24:55.736462: step 20, loss = 4.49 (2279.0 examples/sec; 0.056 sec/batch)\n",
      "2018-04-28 18:24:56.309455: step 30, loss = 4.37 (2233.9 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:24:56.878139: step 40, loss = 4.31 (2250.8 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:24:57.425354: step 50, loss = 4.68 (2339.1 examples/sec; 0.055 sec/batch)\n",
      "2018-04-28 18:24:57.958718: step 60, loss = 4.30 (2399.9 examples/sec; 0.053 sec/batch)\n",
      "2018-04-28 18:24:58.493999: step 70, loss = 4.55 (2391.3 examples/sec; 0.054 sec/batch)\n",
      "2018-04-28 18:24:59.050820: step 80, loss = 4.28 (2298.8 examples/sec; 0.056 sec/batch)\n",
      "2018-04-28 18:24:59.607734: step 90, loss = 4.05 (2298.4 examples/sec; 0.056 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 17.3498\n",
      "2018-04-28 18:25:00.379984: step 100, loss = 4.03 (1657.5 examples/sec; 0.077 sec/batch)\n",
      "2018-04-28 18:25:00.811469: step 110, loss = 4.05 (2966.5 examples/sec; 0.043 sec/batch)\n",
      "2018-04-28 18:25:01.364297: step 120, loss = 4.40 (2315.4 examples/sec; 0.055 sec/batch)\n",
      "2018-04-28 18:25:01.959580: step 130, loss = 3.93 (2150.2 examples/sec; 0.060 sec/batch)\n",
      "2018-04-28 18:25:02.513821: step 140, loss = 4.19 (2309.5 examples/sec; 0.055 sec/batch)\n",
      "2018-04-28 18:25:03.048415: step 150, loss = 3.87 (2394.3 examples/sec; 0.053 sec/batch)\n",
      "2018-04-28 18:25:03.626463: step 160, loss = 4.00 (2214.4 examples/sec; 0.058 sec/batch)\n",
      "2018-04-28 18:25:04.152127: step 170, loss = 3.90 (2435.0 examples/sec; 0.053 sec/batch)\n",
      "2018-04-28 18:25:04.712086: step 180, loss = 3.62 (2285.9 examples/sec; 0.056 sec/batch)\n",
      "2018-04-28 18:25:05.234295: step 190, loss = 3.90 (2451.1 examples/sec; 0.052 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 18.0533\n",
      "2018-04-28 18:25:05.918844: step 200, loss = 3.68 (1869.8 examples/sec; 0.068 sec/batch)\n",
      "2018-04-28 18:25:06.359126: step 210, loss = 3.66 (2907.2 examples/sec; 0.044 sec/batch)\n",
      "2018-04-28 18:25:06.876392: step 220, loss = 3.81 (2474.5 examples/sec; 0.052 sec/batch)\n",
      "2018-04-28 18:25:07.420148: step 230, loss = 3.60 (2354.0 examples/sec; 0.054 sec/batch)\n",
      "2018-04-28 18:25:07.973158: step 240, loss = 3.78 (2314.6 examples/sec; 0.055 sec/batch)\n",
      "2018-04-28 18:25:08.502980: step 250, loss = 3.55 (2415.9 examples/sec; 0.053 sec/batch)\n",
      "2018-04-28 18:25:09.068610: step 260, loss = 3.52 (2263.0 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:25:09.689835: step 270, loss = 3.55 (2060.4 examples/sec; 0.062 sec/batch)\n",
      "2018-04-28 18:25:10.219841: step 280, loss = 3.61 (2415.0 examples/sec; 0.053 sec/batch)\n",
      "2018-04-28 18:25:10.764015: step 290, loss = 3.39 (2352.2 examples/sec; 0.054 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 18.0582\n",
      "2018-04-28 18:25:11.456697: step 300, loss = 3.57 (1847.9 examples/sec; 0.069 sec/batch)\n",
      "2018-04-28 18:25:11.887561: step 310, loss = 3.52 (2970.8 examples/sec; 0.043 sec/batch)\n",
      "2018-04-28 18:25:12.424757: step 320, loss = 3.66 (2382.7 examples/sec; 0.054 sec/batch)\n",
      "2018-04-28 18:25:12.941610: step 330, loss = 3.45 (2476.5 examples/sec; 0.052 sec/batch)\n",
      "2018-04-28 18:25:13.479168: step 340, loss = 3.43 (2381.1 examples/sec; 0.054 sec/batch)\n",
      "2018-04-28 18:25:14.027091: step 350, loss = 3.51 (2336.1 examples/sec; 0.055 sec/batch)\n",
      "2018-04-28 18:25:14.592968: step 360, loss = 3.32 (2262.0 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:25:15.160447: step 370, loss = 3.34 (2255.6 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:25:15.712856: step 380, loss = 3.31 (2317.1 examples/sec; 0.055 sec/batch)\n",
      "2018-04-28 18:25:16.250753: step 390, loss = 3.36 (2379.6 examples/sec; 0.054 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 18.0714\n",
      "2018-04-28 18:25:16.990041: step 400, loss = 3.42 (1731.4 examples/sec; 0.074 sec/batch)\n",
      "2018-04-28 18:25:17.427660: step 410, loss = 3.30 (2924.9 examples/sec; 0.044 sec/batch)\n",
      "2018-04-28 18:25:17.977881: step 420, loss = 3.32 (2326.3 examples/sec; 0.055 sec/batch)\n",
      "2018-04-28 18:25:18.521844: step 430, loss = 3.23 (2353.1 examples/sec; 0.054 sec/batch)\n",
      "2018-04-28 18:25:19.076517: step 440, loss = 3.17 (2307.7 examples/sec; 0.055 sec/batch)\n",
      "2018-04-28 18:25:19.621192: step 450, loss = 3.41 (2350.0 examples/sec; 0.054 sec/batch)\n",
      "2018-04-28 18:25:20.189519: step 460, loss = 3.13 (2252.2 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:25:20.727525: step 470, loss = 3.09 (2379.1 examples/sec; 0.054 sec/batch)\n",
      "2018-04-28 18:25:21.254683: step 480, loss = 3.08 (2428.1 examples/sec; 0.053 sec/batch)\n",
      "2018-04-28 18:25:21.770432: step 490, loss = 2.95 (2481.8 examples/sec; 0.052 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 18.2497\n",
      "2018-04-28 18:25:22.469708: step 500, loss = 3.11 (1830.5 examples/sec; 0.070 sec/batch)\n",
      "2018-04-28 18:25:22.892894: step 510, loss = 3.39 (3024.7 examples/sec; 0.042 sec/batch)\n",
      "2018-04-28 18:25:23.417857: step 520, loss = 3.05 (2438.2 examples/sec; 0.052 sec/batch)\n",
      "2018-04-28 18:25:23.940731: step 530, loss = 3.08 (2448.0 examples/sec; 0.052 sec/batch)\n",
      "2018-04-28 18:25:24.509544: step 540, loss = 2.99 (2250.3 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:25:25.128327: step 550, loss = 3.05 (2068.6 examples/sec; 0.062 sec/batch)\n",
      "2018-04-28 18:25:25.651390: step 560, loss = 2.98 (2447.1 examples/sec; 0.052 sec/batch)\n",
      "2018-04-28 18:25:26.201227: step 570, loss = 3.01 (2328.0 examples/sec; 0.055 sec/batch)\n",
      "2018-04-28 18:25:26.767798: step 580, loss = 3.01 (2259.2 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:25:27.340854: step 590, loss = 2.92 (2233.6 examples/sec; 0.057 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 17.7927\n",
      "2018-04-28 18:25:28.090230: step 600, loss = 3.09 (1708.1 examples/sec; 0.075 sec/batch)\n",
      "2018-04-28 18:25:28.565837: step 610, loss = 2.81 (2691.3 examples/sec; 0.048 sec/batch)\n",
      "2018-04-28 18:25:29.146322: step 620, loss = 2.93 (2205.1 examples/sec; 0.058 sec/batch)\n",
      "2018-04-28 18:25:29.740160: step 630, loss = 2.97 (2155.5 examples/sec; 0.059 sec/batch)\n",
      "2018-04-28 18:25:30.346128: step 640, loss = 2.88 (2112.3 examples/sec; 0.061 sec/batch)\n",
      "2018-04-28 18:25:30.888163: step 650, loss = 2.84 (2361.5 examples/sec; 0.054 sec/batch)\n",
      "2018-04-28 18:25:31.431661: step 660, loss = 2.97 (2355.1 examples/sec; 0.054 sec/batch)\n",
      "2018-04-28 18:25:31.989551: step 670, loss = 2.85 (2294.3 examples/sec; 0.056 sec/batch)\n",
      "2018-04-28 18:25:32.555397: step 680, loss = 2.73 (2262.1 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:25:33.145668: step 690, loss = 2.70 (2168.5 examples/sec; 0.059 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 17.1775\n",
      "2018-04-28 18:25:33.913922: step 700, loss = 2.80 (1666.1 examples/sec; 0.077 sec/batch)\n",
      "2018-04-28 18:25:34.339454: step 710, loss = 2.83 (3008.0 examples/sec; 0.043 sec/batch)\n",
      "2018-04-28 18:25:34.891765: step 720, loss = 2.76 (2317.5 examples/sec; 0.055 sec/batch)\n",
      "2018-04-28 18:25:35.435753: step 730, loss = 3.07 (2353.0 examples/sec; 0.054 sec/batch)\n",
      "2018-04-28 18:25:35.989024: step 740, loss = 2.73 (2313.5 examples/sec; 0.055 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-28 18:25:36.540965: step 750, loss = 2.83 (2319.1 examples/sec; 0.055 sec/batch)\n",
      "2018-04-28 18:25:37.098110: step 760, loss = 2.75 (2297.5 examples/sec; 0.056 sec/batch)\n",
      "2018-04-28 18:25:37.672276: step 770, loss = 2.72 (2229.3 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:25:38.228775: step 780, loss = 2.58 (2300.1 examples/sec; 0.056 sec/batch)\n",
      "2018-04-28 18:25:38.778588: step 790, loss = 2.65 (2328.1 examples/sec; 0.055 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 17.879\n",
      "2018-04-28 18:25:39.504607: step 800, loss = 2.87 (1763.0 examples/sec; 0.073 sec/batch)\n",
      "2018-04-28 18:25:39.907189: step 810, loss = 2.68 (3179.5 examples/sec; 0.040 sec/batch)\n",
      "2018-04-28 18:25:40.427023: step 820, loss = 2.48 (2462.3 examples/sec; 0.052 sec/batch)\n",
      "2018-04-28 18:25:40.959180: step 830, loss = 2.62 (2405.3 examples/sec; 0.053 sec/batch)\n",
      "2018-04-28 18:25:41.487688: step 840, loss = 2.67 (2421.9 examples/sec; 0.053 sec/batch)\n",
      "2018-04-28 18:25:42.024644: step 850, loss = 2.51 (2383.8 examples/sec; 0.054 sec/batch)\n",
      "2018-04-28 18:25:42.570170: step 860, loss = 2.45 (2346.4 examples/sec; 0.055 sec/batch)\n",
      "2018-04-28 18:25:43.106862: step 870, loss = 2.71 (2385.0 examples/sec; 0.054 sec/batch)\n",
      "2018-04-28 18:25:43.648913: step 880, loss = 2.79 (2361.4 examples/sec; 0.054 sec/batch)\n",
      "2018-04-28 18:25:44.229632: step 890, loss = 2.47 (2204.2 examples/sec; 0.058 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 18.0935\n",
      "2018-04-28 18:25:45.035616: step 900, loss = 2.53 (1588.1 examples/sec; 0.081 sec/batch)\n",
      "2018-04-28 18:25:45.507169: step 910, loss = 2.58 (2714.4 examples/sec; 0.047 sec/batch)\n",
      "2018-04-28 18:25:46.096915: step 920, loss = 2.47 (2170.4 examples/sec; 0.059 sec/batch)\n",
      "2018-04-28 18:25:46.664587: step 930, loss = 2.55 (2254.8 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:25:47.232080: step 940, loss = 2.64 (2255.5 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:25:47.809039: step 950, loss = 2.62 (2218.5 examples/sec; 0.058 sec/batch)\n",
      "2018-04-28 18:25:48.374790: step 960, loss = 2.45 (2262.6 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:25:48.954605: step 970, loss = 2.55 (2207.5 examples/sec; 0.058 sec/batch)\n",
      "2018-04-28 18:25:49.516945: step 980, loss = 2.56 (2276.2 examples/sec; 0.056 sec/batch)\n",
      "2018-04-28 18:25:50.107312: step 990, loss = 2.51 (2168.1 examples/sec; 0.059 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 17.1325\n",
      "2018-04-28 18:25:50.868356: step 1000, loss = 2.45 (1681.9 examples/sec; 0.076 sec/batch)\n",
      "2018-04-28 18:25:51.280544: step 1010, loss = 2.42 (3105.4 examples/sec; 0.041 sec/batch)\n",
      "2018-04-28 18:25:51.820809: step 1020, loss = 2.49 (2369.2 examples/sec; 0.054 sec/batch)\n",
      "2018-04-28 18:25:52.359854: step 1030, loss = 2.35 (2374.6 examples/sec; 0.054 sec/batch)\n",
      "2018-04-28 18:25:52.914474: step 1040, loss = 2.62 (2307.9 examples/sec; 0.055 sec/batch)\n",
      "2018-04-28 18:25:53.466447: step 1050, loss = 2.29 (2319.0 examples/sec; 0.055 sec/batch)\n",
      "2018-04-28 18:25:54.009083: step 1060, loss = 2.35 (2358.9 examples/sec; 0.054 sec/batch)\n",
      "2018-04-28 18:25:54.580651: step 1070, loss = 2.52 (2239.4 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:25:55.141886: step 1080, loss = 2.45 (2280.7 examples/sec; 0.056 sec/batch)\n",
      "2018-04-28 18:25:55.711929: step 1090, loss = 2.47 (2245.4 examples/sec; 0.057 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 18.0157\n",
      "2018-04-28 18:25:56.418939: step 1100, loss = 2.24 (1810.4 examples/sec; 0.071 sec/batch)\n",
      "2018-04-28 18:25:56.828377: step 1110, loss = 2.32 (3126.3 examples/sec; 0.041 sec/batch)\n",
      "2018-04-28 18:25:57.356194: step 1120, loss = 2.37 (2425.1 examples/sec; 0.053 sec/batch)\n",
      "2018-04-28 18:25:57.872230: step 1130, loss = 2.18 (2480.4 examples/sec; 0.052 sec/batch)\n",
      "2018-04-28 18:25:58.419303: step 1140, loss = 2.18 (2339.7 examples/sec; 0.055 sec/batch)\n",
      "2018-04-28 18:25:58.954138: step 1150, loss = 2.29 (2393.3 examples/sec; 0.053 sec/batch)\n",
      "2018-04-28 18:25:59.484430: step 1160, loss = 2.26 (2413.8 examples/sec; 0.053 sec/batch)\n",
      "2018-04-28 18:26:00.021196: step 1170, loss = 2.28 (2384.6 examples/sec; 0.054 sec/batch)\n",
      "2018-04-28 18:26:00.553984: step 1180, loss = 2.41 (2402.5 examples/sec; 0.053 sec/batch)\n",
      "2018-04-28 18:26:01.079521: step 1190, loss = 1.93 (2435.6 examples/sec; 0.053 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 18.568\n",
      "2018-04-28 18:26:01.804555: step 1200, loss = 2.04 (1765.4 examples/sec; 0.073 sec/batch)\n",
      "2018-04-28 18:26:02.219521: step 1210, loss = 2.27 (3084.6 examples/sec; 0.041 sec/batch)\n",
      "2018-04-28 18:26:02.741621: step 1220, loss = 2.25 (2451.6 examples/sec; 0.052 sec/batch)\n",
      "2018-04-28 18:26:03.324064: step 1230, loss = 2.09 (2197.6 examples/sec; 0.058 sec/batch)\n",
      "2018-04-28 18:26:03.908406: step 1240, loss = 2.08 (2190.5 examples/sec; 0.058 sec/batch)\n",
      "2018-04-28 18:26:04.486567: step 1250, loss = 2.09 (2213.9 examples/sec; 0.058 sec/batch)\n",
      "2018-04-28 18:26:05.049942: step 1260, loss = 2.18 (2272.0 examples/sec; 0.056 sec/batch)\n",
      "2018-04-28 18:26:05.603881: step 1270, loss = 2.26 (2310.7 examples/sec; 0.055 sec/batch)\n",
      "2018-04-28 18:26:06.172137: step 1280, loss = 2.08 (2252.5 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:26:06.741385: step 1290, loss = 2.16 (2248.6 examples/sec; 0.057 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 17.6454\n",
      "2018-04-28 18:26:07.471714: step 1300, loss = 2.10 (1752.6 examples/sec; 0.073 sec/batch)\n",
      "2018-04-28 18:26:07.908520: step 1310, loss = 2.32 (2930.4 examples/sec; 0.044 sec/batch)\n",
      "2018-04-28 18:26:08.474707: step 1320, loss = 2.05 (2260.8 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:26:09.044399: step 1330, loss = 1.98 (2246.8 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:26:09.616268: step 1340, loss = 2.21 (2238.3 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:26:10.181179: step 1350, loss = 2.28 (2265.8 examples/sec; 0.056 sec/batch)\n",
      "2018-04-28 18:26:10.721990: step 1360, loss = 2.14 (2366.8 examples/sec; 0.054 sec/batch)\n",
      "2018-04-28 18:26:11.259174: step 1370, loss = 2.04 (2382.8 examples/sec; 0.054 sec/batch)\n",
      "2018-04-28 18:26:11.798066: step 1380, loss = 2.14 (2375.2 examples/sec; 0.054 sec/batch)\n",
      "2018-04-28 18:26:12.356428: step 1390, loss = 2.32 (2292.4 examples/sec; 0.056 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 17.8641\n",
      "2018-04-28 18:26:13.069742: step 1400, loss = 2.00 (1794.4 examples/sec; 0.071 sec/batch)\n",
      "2018-04-28 18:26:13.498140: step 1410, loss = 2.03 (2987.9 examples/sec; 0.043 sec/batch)\n",
      "2018-04-28 18:26:14.070149: step 1420, loss = 2.21 (2237.7 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:26:14.615870: step 1430, loss = 2.18 (2345.5 examples/sec; 0.055 sec/batch)\n",
      "2018-04-28 18:26:15.155482: step 1440, loss = 2.06 (2372.1 examples/sec; 0.054 sec/batch)\n",
      "2018-04-28 18:26:15.675520: step 1450, loss = 2.02 (2461.3 examples/sec; 0.052 sec/batch)\n",
      "2018-04-28 18:26:16.192662: step 1460, loss = 2.00 (2475.1 examples/sec; 0.052 sec/batch)\n",
      "2018-04-28 18:26:16.716310: step 1470, loss = 1.97 (2444.4 examples/sec; 0.052 sec/batch)\n",
      "2018-04-28 18:26:17.240234: step 1480, loss = 1.95 (2443.1 examples/sec; 0.052 sec/batch)\n",
      "2018-04-28 18:26:17.762395: step 1490, loss = 1.86 (2451.3 examples/sec; 0.052 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 18.4836\n",
      "2018-04-28 18:26:18.479773: step 1500, loss = 1.82 (1784.3 examples/sec; 0.072 sec/batch)\n",
      "2018-04-28 18:26:18.891631: step 1510, loss = 1.94 (3107.9 examples/sec; 0.041 sec/batch)\n",
      "2018-04-28 18:26:19.438745: step 1520, loss = 2.04 (2339.5 examples/sec; 0.055 sec/batch)\n",
      "2018-04-28 18:26:19.986612: step 1530, loss = 1.84 (2336.3 examples/sec; 0.055 sec/batch)\n",
      "2018-04-28 18:26:20.550367: step 1540, loss = 1.90 (2270.5 examples/sec; 0.056 sec/batch)\n",
      "2018-04-28 18:26:21.083563: step 1550, loss = 1.94 (2400.6 examples/sec; 0.053 sec/batch)\n",
      "2018-04-28 18:26:21.630229: step 1560, loss = 1.81 (2341.5 examples/sec; 0.055 sec/batch)\n",
      "2018-04-28 18:26:22.202128: step 1570, loss = 1.96 (2238.1 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:26:22.745838: step 1580, loss = 1.89 (2354.2 examples/sec; 0.054 sec/batch)\n",
      "2018-04-28 18:26:23.287807: step 1590, loss = 1.86 (2361.8 examples/sec; 0.054 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 18.2361\n",
      "2018-04-28 18:26:23.963474: step 1600, loss = 1.76 (1894.4 examples/sec; 0.068 sec/batch)\n",
      "2018-04-28 18:26:24.379013: step 1610, loss = 1.95 (3080.4 examples/sec; 0.042 sec/batch)\n",
      "2018-04-28 18:26:24.908386: step 1620, loss = 1.82 (2418.0 examples/sec; 0.053 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-28 18:26:25.439241: step 1630, loss = 2.06 (2411.2 examples/sec; 0.053 sec/batch)\n",
      "2018-04-28 18:26:25.959212: step 1640, loss = 1.80 (2461.7 examples/sec; 0.052 sec/batch)\n",
      "2018-04-28 18:26:26.512366: step 1650, loss = 1.55 (2314.0 examples/sec; 0.055 sec/batch)\n",
      "2018-04-28 18:26:27.056459: step 1660, loss = 1.91 (2352.5 examples/sec; 0.054 sec/batch)\n",
      "2018-04-28 18:26:27.577080: step 1670, loss = 1.80 (2458.6 examples/sec; 0.052 sec/batch)\n",
      "2018-04-28 18:26:28.096862: step 1680, loss = 1.90 (2462.6 examples/sec; 0.052 sec/batch)\n",
      "2018-04-28 18:26:28.622048: step 1690, loss = 1.85 (2437.2 examples/sec; 0.053 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 18.7142\n",
      "2018-04-28 18:26:29.307727: step 1700, loss = 1.78 (1866.8 examples/sec; 0.069 sec/batch)\n",
      "2018-04-28 18:26:29.779748: step 1710, loss = 1.74 (2711.7 examples/sec; 0.047 sec/batch)\n",
      "2018-04-28 18:26:30.317432: step 1720, loss = 1.73 (2380.6 examples/sec; 0.054 sec/batch)\n",
      "2018-04-28 18:26:30.856277: step 1730, loss = 1.87 (2375.5 examples/sec; 0.054 sec/batch)\n",
      "2018-04-28 18:26:31.381792: step 1740, loss = 1.78 (2435.7 examples/sec; 0.053 sec/batch)\n",
      "2018-04-28 18:26:31.895716: step 1750, loss = 2.20 (2490.6 examples/sec; 0.051 sec/batch)\n",
      "2018-04-28 18:26:32.413785: step 1760, loss = 1.74 (2470.8 examples/sec; 0.052 sec/batch)\n",
      "2018-04-28 18:26:32.948827: step 1770, loss = 1.76 (2392.2 examples/sec; 0.054 sec/batch)\n",
      "2018-04-28 18:26:33.481634: step 1780, loss = 1.88 (2402.4 examples/sec; 0.053 sec/batch)\n",
      "2018-04-28 18:26:34.054499: step 1790, loss = 1.66 (2234.4 examples/sec; 0.057 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 18.2754\n",
      "2018-04-28 18:26:34.778889: step 1800, loss = 1.61 (1767.0 examples/sec; 0.072 sec/batch)\n",
      "2018-04-28 18:26:35.216091: step 1810, loss = 1.88 (2927.7 examples/sec; 0.044 sec/batch)\n",
      "2018-04-28 18:26:35.784903: step 1820, loss = 1.71 (2250.3 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:26:36.351388: step 1830, loss = 1.77 (2259.5 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:26:36.922096: step 1840, loss = 1.80 (2242.8 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:26:37.489657: step 1850, loss = 1.79 (2255.3 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:26:38.061874: step 1860, loss = 1.76 (2236.9 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:26:38.611419: step 1870, loss = 1.81 (2329.2 examples/sec; 0.055 sec/batch)\n",
      "2018-04-28 18:26:39.160156: step 1880, loss = 1.75 (2332.7 examples/sec; 0.055 sec/batch)\n",
      "2018-04-28 18:26:39.718356: step 1890, loss = 1.58 (2293.1 examples/sec; 0.056 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 17.676\n",
      "2018-04-28 18:26:40.436250: step 1900, loss = 1.58 (1783.0 examples/sec; 0.072 sec/batch)\n",
      "2018-04-28 18:26:40.870334: step 1910, loss = 1.55 (2948.7 examples/sec; 0.043 sec/batch)\n",
      "2018-04-28 18:26:41.426754: step 1920, loss = 1.59 (2300.4 examples/sec; 0.056 sec/batch)\n",
      "2018-04-28 18:26:41.981510: step 1930, loss = 1.91 (2307.3 examples/sec; 0.055 sec/batch)\n",
      "2018-04-28 18:26:42.533897: step 1940, loss = 1.53 (2317.2 examples/sec; 0.055 sec/batch)\n",
      "2018-04-28 18:26:43.104939: step 1950, loss = 1.89 (2241.5 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:26:43.651530: step 1960, loss = 1.74 (2341.8 examples/sec; 0.055 sec/batch)\n",
      "2018-04-28 18:26:44.202370: step 1970, loss = 1.58 (2323.7 examples/sec; 0.055 sec/batch)\n",
      "2018-04-28 18:26:44.737923: step 1980, loss = 1.68 (2390.0 examples/sec; 0.054 sec/batch)\n",
      "2018-04-28 18:26:45.263343: step 1990, loss = 1.65 (2436.2 examples/sec; 0.053 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 18.1849\n",
      "2018-04-28 18:26:45.935247: step 2000, loss = 1.60 (1905.0 examples/sec; 0.067 sec/batch)\n",
      "2018-04-28 18:26:46.339115: step 2010, loss = 1.57 (3169.4 examples/sec; 0.040 sec/batch)\n",
      "2018-04-28 18:26:46.866225: step 2020, loss = 1.57 (2428.3 examples/sec; 0.053 sec/batch)\n",
      "2018-04-28 18:26:47.444195: step 2030, loss = 1.38 (2214.6 examples/sec; 0.058 sec/batch)\n",
      "2018-04-28 18:26:48.063941: step 2040, loss = 1.66 (2065.4 examples/sec; 0.062 sec/batch)\n",
      "2018-04-28 18:26:48.634437: step 2050, loss = 1.62 (2243.7 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:26:49.199381: step 2060, loss = 1.70 (2265.7 examples/sec; 0.056 sec/batch)\n",
      "2018-04-28 18:26:49.771361: step 2070, loss = 1.58 (2237.8 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:26:50.341150: step 2080, loss = 1.58 (2246.5 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:26:50.916772: step 2090, loss = 1.60 (2223.7 examples/sec; 0.058 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 17.4936\n",
      "2018-04-28 18:26:51.651618: step 2100, loss = 1.78 (1741.9 examples/sec; 0.073 sec/batch)\n",
      "2018-04-28 18:26:52.078921: step 2110, loss = 1.64 (2995.6 examples/sec; 0.043 sec/batch)\n",
      "2018-04-28 18:26:52.625518: step 2120, loss = 1.60 (2341.7 examples/sec; 0.055 sec/batch)\n",
      "2018-04-28 18:26:53.171348: step 2130, loss = 1.59 (2345.1 examples/sec; 0.055 sec/batch)\n",
      "2018-04-28 18:26:53.740513: step 2140, loss = 1.60 (2248.9 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:26:54.308182: step 2150, loss = 1.41 (2254.8 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:26:54.898490: step 2160, loss = 1.65 (2168.4 examples/sec; 0.059 sec/batch)\n",
      "2018-04-28 18:26:55.442843: step 2170, loss = 1.65 (2351.4 examples/sec; 0.054 sec/batch)\n",
      "2018-04-28 18:26:55.996237: step 2180, loss = 1.47 (2313.0 examples/sec; 0.055 sec/batch)\n",
      "2018-04-28 18:26:56.542772: step 2190, loss = 1.63 (2342.0 examples/sec; 0.055 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 17.8709\n",
      "2018-04-28 18:26:57.247345: step 2200, loss = 1.47 (1816.7 examples/sec; 0.070 sec/batch)\n",
      "2018-04-28 18:26:57.707521: step 2210, loss = 1.70 (2781.6 examples/sec; 0.046 sec/batch)\n",
      "2018-04-28 18:26:58.268504: step 2220, loss = 1.53 (2281.7 examples/sec; 0.056 sec/batch)\n",
      "2018-04-28 18:26:58.820944: step 2230, loss = 1.71 (2317.0 examples/sec; 0.055 sec/batch)\n",
      "2018-04-28 18:26:59.385113: step 2240, loss = 1.51 (2268.8 examples/sec; 0.056 sec/batch)\n",
      "2018-04-28 18:26:59.953271: step 2250, loss = 1.71 (2252.9 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:27:00.509173: step 2260, loss = 1.52 (2302.6 examples/sec; 0.056 sec/batch)\n",
      "2018-04-28 18:27:01.083807: step 2270, loss = 1.76 (2227.5 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:27:01.628232: step 2280, loss = 1.62 (2351.1 examples/sec; 0.054 sec/batch)\n",
      "2018-04-28 18:27:02.183185: step 2290, loss = 1.44 (2306.5 examples/sec; 0.055 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 17.7615\n",
      "2018-04-28 18:27:02.877847: step 2300, loss = 1.41 (1842.6 examples/sec; 0.069 sec/batch)\n",
      "2018-04-28 18:27:03.292081: step 2310, loss = 1.52 (3090.1 examples/sec; 0.041 sec/batch)\n",
      "2018-04-28 18:27:03.860376: step 2320, loss = 1.45 (2252.3 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:27:04.430124: step 2330, loss = 1.66 (2246.6 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:27:04.997220: step 2340, loss = 1.37 (2257.1 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:27:05.542408: step 2350, loss = 1.40 (2347.8 examples/sec; 0.055 sec/batch)\n",
      "2018-04-28 18:27:06.070873: step 2360, loss = 1.55 (2422.1 examples/sec; 0.053 sec/batch)\n",
      "2018-04-28 18:27:06.590930: step 2370, loss = 1.39 (2461.3 examples/sec; 0.052 sec/batch)\n",
      "2018-04-28 18:27:07.157498: step 2380, loss = 1.55 (2259.2 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:27:07.709920: step 2390, loss = 1.63 (2317.0 examples/sec; 0.055 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 18.0981\n",
      "2018-04-28 18:27:08.403277: step 2400, loss = 1.54 (1846.1 examples/sec; 0.069 sec/batch)\n",
      "2018-04-28 18:27:08.806307: step 2410, loss = 1.33 (3176.0 examples/sec; 0.040 sec/batch)\n",
      "2018-04-28 18:27:09.333110: step 2420, loss = 1.55 (2429.8 examples/sec; 0.053 sec/batch)\n",
      "2018-04-28 18:27:09.885028: step 2430, loss = 1.51 (2319.2 examples/sec; 0.055 sec/batch)\n",
      "2018-04-28 18:27:10.454114: step 2440, loss = 1.40 (2249.2 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:27:11.023031: step 2450, loss = 1.49 (2249.9 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:27:11.593208: step 2460, loss = 1.36 (2244.9 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:27:12.167168: step 2470, loss = 1.41 (2230.1 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:27:12.734510: step 2480, loss = 1.33 (2256.1 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:27:13.302377: step 2490, loss = 1.33 (2254.0 examples/sec; 0.057 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 17.7537\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-28 18:27:14.035515: step 2500, loss = 1.28 (1745.9 examples/sec; 0.073 sec/batch)\n",
      "2018-04-28 18:27:14.490651: step 2510, loss = 1.42 (2812.4 examples/sec; 0.046 sec/batch)\n",
      "2018-04-28 18:27:15.086365: step 2520, loss = 1.51 (2148.7 examples/sec; 0.060 sec/batch)\n",
      "2018-04-28 18:27:15.651108: step 2530, loss = 1.49 (2266.5 examples/sec; 0.056 sec/batch)\n",
      "2018-04-28 18:27:16.215847: step 2540, loss = 1.48 (2266.5 examples/sec; 0.056 sec/batch)\n",
      "2018-04-28 18:27:16.773889: step 2550, loss = 1.36 (2293.8 examples/sec; 0.056 sec/batch)\n",
      "2018-04-28 18:27:17.330995: step 2560, loss = 1.46 (2297.6 examples/sec; 0.056 sec/batch)\n",
      "2018-04-28 18:27:17.886124: step 2570, loss = 1.50 (2305.7 examples/sec; 0.056 sec/batch)\n",
      "2018-04-28 18:27:18.434566: step 2580, loss = 1.33 (2333.9 examples/sec; 0.055 sec/batch)\n",
      "2018-04-28 18:27:18.995539: step 2590, loss = 1.35 (2281.8 examples/sec; 0.056 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 17.5668\n",
      "2018-04-28 18:27:19.728198: step 2600, loss = 1.48 (1747.0 examples/sec; 0.073 sec/batch)\n",
      "2018-04-28 18:27:20.148946: step 2610, loss = 1.44 (3042.2 examples/sec; 0.042 sec/batch)\n",
      "2018-04-28 18:27:20.693044: step 2620, loss = 1.38 (2352.5 examples/sec; 0.054 sec/batch)\n",
      "2018-04-28 18:27:21.230328: step 2630, loss = 1.27 (2382.7 examples/sec; 0.054 sec/batch)\n",
      "2018-04-28 18:27:21.773201: step 2640, loss = 1.75 (2357.5 examples/sec; 0.054 sec/batch)\n",
      "2018-04-28 18:27:22.315077: step 2650, loss = 1.25 (2362.2 examples/sec; 0.054 sec/batch)\n",
      "2018-04-28 18:27:22.863792: step 2660, loss = 1.29 (2332.7 examples/sec; 0.055 sec/batch)\n",
      "2018-04-28 18:27:23.417166: step 2670, loss = 1.37 (2313.1 examples/sec; 0.055 sec/batch)\n",
      "2018-04-28 18:27:23.959609: step 2680, loss = 1.35 (2359.7 examples/sec; 0.054 sec/batch)\n",
      "2018-04-28 18:27:24.550002: step 2690, loss = 1.15 (2168.0 examples/sec; 0.059 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 18.1215\n",
      "2018-04-28 18:27:25.247136: step 2700, loss = 1.42 (1836.1 examples/sec; 0.070 sec/batch)\n",
      "2018-04-28 18:27:25.684660: step 2710, loss = 1.53 (2925.6 examples/sec; 0.044 sec/batch)\n",
      "2018-04-28 18:27:26.260096: step 2720, loss = 1.43 (2224.4 examples/sec; 0.058 sec/batch)\n",
      "2018-04-28 18:27:26.828714: step 2730, loss = 1.64 (2251.1 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:27:27.396335: step 2740, loss = 1.21 (2255.0 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:27:27.968722: step 2750, loss = 1.31 (2236.3 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:27:28.540623: step 2760, loss = 1.42 (2238.2 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:27:29.117442: step 2770, loss = 1.25 (2219.1 examples/sec; 0.058 sec/batch)\n",
      "2018-04-28 18:27:29.680841: step 2780, loss = 1.27 (2271.9 examples/sec; 0.056 sec/batch)\n",
      "2018-04-28 18:27:30.255136: step 2790, loss = 1.18 (2228.8 examples/sec; 0.057 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 17.4403\n",
      "2018-04-28 18:27:30.980890: step 2800, loss = 1.37 (1763.7 examples/sec; 0.073 sec/batch)\n",
      "2018-04-28 18:27:31.419583: step 2810, loss = 1.23 (2917.8 examples/sec; 0.044 sec/batch)\n",
      "2018-04-28 18:27:32.004868: step 2820, loss = 1.27 (2187.0 examples/sec; 0.059 sec/batch)\n",
      "2018-04-28 18:27:32.627833: step 2830, loss = 1.21 (2054.7 examples/sec; 0.062 sec/batch)\n",
      "2018-04-28 18:27:33.211916: step 2840, loss = 1.18 (2191.5 examples/sec; 0.058 sec/batch)\n",
      "2018-04-28 18:27:33.844199: step 2850, loss = 1.17 (2024.4 examples/sec; 0.063 sec/batch)\n",
      "2018-04-28 18:27:34.433757: step 2860, loss = 1.27 (2171.1 examples/sec; 0.059 sec/batch)\n",
      "2018-04-28 18:27:35.009976: step 2870, loss = 1.26 (2221.4 examples/sec; 0.058 sec/batch)\n",
      "2018-04-28 18:27:35.626472: step 2880, loss = 1.38 (2076.3 examples/sec; 0.062 sec/batch)\n",
      "2018-04-28 18:27:36.253100: step 2890, loss = 1.33 (2042.6 examples/sec; 0.063 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 16.5977\n",
      "2018-04-28 18:27:37.006057: step 2900, loss = 1.50 (1700.0 examples/sec; 0.075 sec/batch)\n",
      "2018-04-28 18:27:37.492633: step 2910, loss = 1.31 (2630.7 examples/sec; 0.049 sec/batch)\n",
      "2018-04-28 18:27:38.056242: step 2920, loss = 1.35 (2271.1 examples/sec; 0.056 sec/batch)\n",
      "2018-04-28 18:27:38.639571: step 2930, loss = 1.40 (2194.3 examples/sec; 0.058 sec/batch)\n",
      "2018-04-28 18:27:39.245978: step 2940, loss = 1.24 (2110.8 examples/sec; 0.061 sec/batch)\n",
      "2018-04-28 18:27:39.846826: step 2950, loss = 1.39 (2130.3 examples/sec; 0.060 sec/batch)\n",
      "2018-04-28 18:27:40.451946: step 2960, loss = 1.36 (2115.3 examples/sec; 0.061 sec/batch)\n",
      "2018-04-28 18:27:41.017953: step 2970, loss = 1.22 (2261.5 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:27:41.602203: step 2980, loss = 1.20 (2190.9 examples/sec; 0.058 sec/batch)\n",
      "2018-04-28 18:27:42.175629: step 2990, loss = 1.37 (2232.2 examples/sec; 0.057 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 16.8559\n",
      "2018-04-28 18:27:42.938625: step 3000, loss = 1.15 (1677.6 examples/sec; 0.076 sec/batch)\n",
      "2018-04-28 18:27:43.408671: step 3010, loss = 1.29 (2723.1 examples/sec; 0.047 sec/batch)\n",
      "2018-04-28 18:27:44.006953: step 3020, loss = 1.20 (2139.5 examples/sec; 0.060 sec/batch)\n",
      "2018-04-28 18:27:44.576825: step 3030, loss = 1.11 (2246.1 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:27:45.151713: step 3040, loss = 1.26 (2226.5 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:27:45.736251: step 3050, loss = 1.34 (2189.8 examples/sec; 0.058 sec/batch)\n",
      "2018-04-28 18:27:46.281761: step 3060, loss = 1.16 (2346.4 examples/sec; 0.055 sec/batch)\n",
      "2018-04-28 18:27:46.831567: step 3070, loss = 1.40 (2328.1 examples/sec; 0.055 sec/batch)\n",
      "2018-04-28 18:27:47.386218: step 3080, loss = 1.33 (2307.8 examples/sec; 0.055 sec/batch)\n",
      "2018-04-28 18:27:47.943096: step 3090, loss = 1.31 (2298.5 examples/sec; 0.056 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 17.4717\n",
      "2018-04-28 18:27:48.661339: step 3100, loss = 1.08 (1782.1 examples/sec; 0.072 sec/batch)\n",
      "2018-04-28 18:27:49.082128: step 3110, loss = 1.13 (3042.0 examples/sec; 0.042 sec/batch)\n",
      "2018-04-28 18:27:49.627880: step 3120, loss = 1.28 (2345.4 examples/sec; 0.055 sec/batch)\n",
      "2018-04-28 18:27:50.158518: step 3130, loss = 1.15 (2412.2 examples/sec; 0.053 sec/batch)\n",
      "2018-04-28 18:27:50.686507: step 3140, loss = 1.54 (2424.3 examples/sec; 0.053 sec/batch)\n",
      "2018-04-28 18:27:51.239392: step 3150, loss = 1.21 (2315.1 examples/sec; 0.055 sec/batch)\n",
      "2018-04-28 18:27:51.764655: step 3160, loss = 1.42 (2436.9 examples/sec; 0.053 sec/batch)\n",
      "2018-04-28 18:27:52.296211: step 3170, loss = 1.17 (2408.0 examples/sec; 0.053 sec/batch)\n",
      "2018-04-28 18:27:52.820885: step 3180, loss = 1.09 (2439.6 examples/sec; 0.052 sec/batch)\n",
      "2018-04-28 18:27:53.348711: step 3190, loss = 1.32 (2425.0 examples/sec; 0.053 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 18.6622\n",
      "2018-04-28 18:27:54.019936: step 3200, loss = 1.34 (1907.0 examples/sec; 0.067 sec/batch)\n",
      "2018-04-28 18:27:54.424039: step 3210, loss = 1.20 (3167.5 examples/sec; 0.040 sec/batch)\n",
      "2018-04-28 18:27:54.957353: step 3220, loss = 1.26 (2400.1 examples/sec; 0.053 sec/batch)\n",
      "2018-04-28 18:27:55.501235: step 3230, loss = 1.15 (2353.5 examples/sec; 0.054 sec/batch)\n",
      "2018-04-28 18:27:56.069886: step 3240, loss = 1.13 (2250.9 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:27:56.634667: step 3250, loss = 1.30 (2266.4 examples/sec; 0.056 sec/batch)\n",
      "2018-04-28 18:27:57.205455: step 3260, loss = 1.35 (2242.5 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:27:57.776928: step 3270, loss = 1.14 (2239.8 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:27:58.345673: step 3280, loss = 1.23 (2250.6 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:27:58.911415: step 3290, loss = 1.21 (2262.5 examples/sec; 0.057 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 17.8067\n",
      "2018-04-28 18:27:59.635629: step 3300, loss = 1.36 (1767.4 examples/sec; 0.072 sec/batch)\n",
      "2018-04-28 18:28:00.048544: step 3310, loss = 1.45 (3099.9 examples/sec; 0.041 sec/batch)\n",
      "2018-04-28 18:28:00.591108: step 3320, loss = 1.21 (2359.2 examples/sec; 0.054 sec/batch)\n",
      "2018-04-28 18:28:01.141070: step 3330, loss = 1.30 (2327.4 examples/sec; 0.055 sec/batch)\n",
      "2018-04-28 18:28:01.681243: step 3340, loss = 1.27 (2369.6 examples/sec; 0.054 sec/batch)\n",
      "2018-04-28 18:28:02.222626: step 3350, loss = 1.29 (2364.3 examples/sec; 0.054 sec/batch)\n",
      "2018-04-28 18:28:02.781302: step 3360, loss = 1.12 (2291.1 examples/sec; 0.056 sec/batch)\n",
      "2018-04-28 18:28:03.327894: step 3370, loss = 1.11 (2341.8 examples/sec; 0.055 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-28 18:28:03.881669: step 3380, loss = 1.10 (2311.4 examples/sec; 0.055 sec/batch)\n",
      "2018-04-28 18:28:04.430994: step 3390, loss = 1.21 (2330.1 examples/sec; 0.055 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 18.1564\n",
      "2018-04-28 18:28:05.143587: step 3400, loss = 1.44 (1796.2 examples/sec; 0.071 sec/batch)\n",
      "2018-04-28 18:28:05.574609: step 3410, loss = 1.08 (2969.7 examples/sec; 0.043 sec/batch)\n",
      "2018-04-28 18:28:06.116900: step 3420, loss = 1.17 (2360.4 examples/sec; 0.054 sec/batch)\n",
      "2018-04-28 18:28:06.686438: step 3430, loss = 1.15 (2247.4 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:28:07.278142: step 3440, loss = 1.29 (2163.3 examples/sec; 0.059 sec/batch)\n",
      "2018-04-28 18:28:07.863237: step 3450, loss = 1.27 (2187.7 examples/sec; 0.059 sec/batch)\n",
      "2018-04-28 18:28:08.485201: step 3460, loss = 1.27 (2058.0 examples/sec; 0.062 sec/batch)\n",
      "2018-04-28 18:28:09.050044: step 3470, loss = 1.32 (2266.1 examples/sec; 0.056 sec/batch)\n",
      "2018-04-28 18:28:09.654630: step 3480, loss = 1.26 (2117.1 examples/sec; 0.060 sec/batch)\n",
      "2018-04-28 18:28:10.218369: step 3490, loss = 1.05 (2270.6 examples/sec; 0.056 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 17.338\n",
      "2018-04-28 18:28:10.911129: step 3500, loss = 1.26 (1847.7 examples/sec; 0.069 sec/batch)\n",
      "2018-04-28 18:28:11.327884: step 3510, loss = 1.02 (3071.4 examples/sec; 0.042 sec/batch)\n",
      "2018-04-28 18:28:11.874176: step 3520, loss = 1.18 (2343.1 examples/sec; 0.055 sec/batch)\n",
      "2018-04-28 18:28:12.419911: step 3530, loss = 1.04 (2345.4 examples/sec; 0.055 sec/batch)\n",
      "2018-04-28 18:28:12.959805: step 3540, loss = 1.24 (2370.8 examples/sec; 0.054 sec/batch)\n",
      "2018-04-28 18:28:13.508914: step 3550, loss = 1.11 (2331.0 examples/sec; 0.055 sec/batch)\n",
      "2018-04-28 18:28:14.047240: step 3560, loss = 0.91 (2377.7 examples/sec; 0.054 sec/batch)\n",
      "2018-04-28 18:28:14.588584: step 3570, loss = 1.20 (2364.5 examples/sec; 0.054 sec/batch)\n",
      "2018-04-28 18:28:15.138623: step 3580, loss = 1.30 (2327.1 examples/sec; 0.055 sec/batch)\n",
      "2018-04-28 18:28:15.672950: step 3590, loss = 1.27 (2395.5 examples/sec; 0.053 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 18.3095\n",
      "2018-04-28 18:28:16.372776: step 3600, loss = 1.06 (1829.0 examples/sec; 0.070 sec/batch)\n",
      "2018-04-28 18:28:16.800876: step 3610, loss = 1.15 (2990.0 examples/sec; 0.043 sec/batch)\n",
      "2018-04-28 18:28:17.354027: step 3620, loss = 1.17 (2314.0 examples/sec; 0.055 sec/batch)\n",
      "2018-04-28 18:28:17.910739: step 3630, loss = 1.14 (2299.2 examples/sec; 0.056 sec/batch)\n",
      "2018-04-28 18:28:18.451217: step 3640, loss = 0.84 (2368.3 examples/sec; 0.054 sec/batch)\n",
      "2018-04-28 18:28:18.993171: step 3650, loss = 1.15 (2361.8 examples/sec; 0.054 sec/batch)\n",
      "2018-04-28 18:28:19.555375: step 3660, loss = 1.21 (2276.8 examples/sec; 0.056 sec/batch)\n",
      "2018-04-28 18:28:20.079909: step 3670, loss = 1.14 (2440.3 examples/sec; 0.052 sec/batch)\n",
      "2018-04-28 18:28:20.612950: step 3680, loss = 1.10 (2401.3 examples/sec; 0.053 sec/batch)\n",
      "2018-04-28 18:28:21.135788: step 3690, loss = 1.04 (2448.2 examples/sec; 0.052 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 18.4066\n",
      "2018-04-28 18:28:21.805654: step 3700, loss = 1.40 (1910.8 examples/sec; 0.067 sec/batch)\n",
      "2018-04-28 18:28:22.204781: step 3710, loss = 1.23 (3207.0 examples/sec; 0.040 sec/batch)\n",
      "2018-04-28 18:28:22.771389: step 3720, loss = 1.17 (2259.0 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:28:23.340951: step 3730, loss = 1.15 (2247.4 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:28:23.906982: step 3740, loss = 1.08 (2261.4 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:28:24.479469: step 3750, loss = 1.07 (2235.8 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:28:25.048250: step 3760, loss = 1.03 (2250.4 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:28:25.617279: step 3770, loss = 1.19 (2249.5 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:28:26.180493: step 3780, loss = 1.33 (2272.6 examples/sec; 0.056 sec/batch)\n",
      "2018-04-28 18:28:26.745342: step 3790, loss = 1.02 (2266.1 examples/sec; 0.056 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 17.627\n",
      "2018-04-28 18:28:27.478571: step 3800, loss = 1.13 (1745.7 examples/sec; 0.073 sec/batch)\n",
      "2018-04-28 18:28:27.915295: step 3810, loss = 1.27 (2931.0 examples/sec; 0.044 sec/batch)\n",
      "2018-04-28 18:28:28.476955: step 3820, loss = 1.21 (2278.9 examples/sec; 0.056 sec/batch)\n",
      "2018-04-28 18:28:29.038646: step 3830, loss = 1.05 (2278.8 examples/sec; 0.056 sec/batch)\n",
      "2018-04-28 18:28:29.604236: step 3840, loss = 1.02 (2263.1 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:28:30.177525: step 3850, loss = 1.09 (2232.7 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:28:30.730918: step 3860, loss = 1.00 (2313.0 examples/sec; 0.055 sec/batch)\n",
      "2018-04-28 18:28:31.288520: step 3870, loss = 1.10 (2295.5 examples/sec; 0.056 sec/batch)\n",
      "2018-04-28 18:28:31.844055: step 3880, loss = 1.11 (2304.2 examples/sec; 0.056 sec/batch)\n",
      "2018-04-28 18:28:32.393625: step 3890, loss = 1.17 (2329.0 examples/sec; 0.055 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 17.7583\n",
      "2018-04-28 18:28:33.110008: step 3900, loss = 1.32 (1786.8 examples/sec; 0.072 sec/batch)\n",
      "2018-04-28 18:28:33.520832: step 3910, loss = 1.17 (3115.7 examples/sec; 0.041 sec/batch)\n",
      "2018-04-28 18:28:34.069440: step 3920, loss = 1.04 (2333.2 examples/sec; 0.055 sec/batch)\n",
      "2018-04-28 18:28:34.630745: step 3930, loss = 1.15 (2280.4 examples/sec; 0.056 sec/batch)\n",
      "2018-04-28 18:28:35.152090: step 3940, loss = 1.02 (2455.2 examples/sec; 0.052 sec/batch)\n",
      "2018-04-28 18:28:35.676886: step 3950, loss = 1.24 (2439.0 examples/sec; 0.052 sec/batch)\n",
      "2018-04-28 18:28:36.202703: step 3960, loss = 1.11 (2434.3 examples/sec; 0.053 sec/batch)\n",
      "2018-04-28 18:28:36.724269: step 3970, loss = 1.20 (2454.1 examples/sec; 0.052 sec/batch)\n",
      "2018-04-28 18:28:37.258363: step 3980, loss = 1.18 (2396.6 examples/sec; 0.053 sec/batch)\n",
      "2018-04-28 18:28:37.793969: step 3990, loss = 1.24 (2389.8 examples/sec; 0.054 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 18.614\n",
      "2018-04-28 18:28:38.482224: step 4000, loss = 1.13 (1859.8 examples/sec; 0.069 sec/batch)\n",
      "2018-04-28 18:28:38.892080: step 4010, loss = 1.03 (3123.1 examples/sec; 0.041 sec/batch)\n",
      "2018-04-28 18:28:39.441626: step 4020, loss = 1.17 (2329.2 examples/sec; 0.055 sec/batch)\n",
      "2018-04-28 18:28:39.998639: step 4030, loss = 1.08 (2298.0 examples/sec; 0.056 sec/batch)\n",
      "2018-04-28 18:28:40.523689: step 4040, loss = 1.11 (2437.8 examples/sec; 0.053 sec/batch)\n",
      "2018-04-28 18:28:41.039360: step 4050, loss = 0.98 (2482.2 examples/sec; 0.052 sec/batch)\n",
      "2018-04-28 18:28:41.567994: step 4060, loss = 0.93 (2421.3 examples/sec; 0.053 sec/batch)\n",
      "2018-04-28 18:28:42.112272: step 4070, loss = 0.87 (2351.7 examples/sec; 0.054 sec/batch)\n",
      "2018-04-28 18:28:42.640696: step 4080, loss = 1.17 (2422.3 examples/sec; 0.053 sec/batch)\n",
      "2018-04-28 18:28:43.159410: step 4090, loss = 1.30 (2467.6 examples/sec; 0.052 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 18.684\n",
      "2018-04-28 18:28:43.834333: step 4100, loss = 1.16 (1896.5 examples/sec; 0.067 sec/batch)\n",
      "2018-04-28 18:28:44.235903: step 4110, loss = 0.96 (3187.5 examples/sec; 0.040 sec/batch)\n",
      "2018-04-28 18:28:44.766476: step 4120, loss = 1.04 (2412.5 examples/sec; 0.053 sec/batch)\n",
      "2018-04-28 18:28:45.284892: step 4130, loss = 1.18 (2469.1 examples/sec; 0.052 sec/batch)\n",
      "2018-04-28 18:28:45.821942: step 4140, loss = 1.09 (2383.4 examples/sec; 0.054 sec/batch)\n",
      "2018-04-28 18:28:46.370172: step 4150, loss = 1.02 (2334.8 examples/sec; 0.055 sec/batch)\n",
      "2018-04-28 18:28:46.893805: step 4160, loss = 1.15 (2444.5 examples/sec; 0.052 sec/batch)\n",
      "2018-04-28 18:28:47.421521: step 4170, loss = 0.92 (2425.6 examples/sec; 0.053 sec/batch)\n",
      "2018-04-28 18:28:47.941243: step 4180, loss = 1.13 (2462.8 examples/sec; 0.052 sec/batch)\n",
      "2018-04-28 18:28:48.463419: step 4190, loss = 1.25 (2451.3 examples/sec; 0.052 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 18.9048\n",
      "2018-04-28 18:28:49.124169: step 4200, loss = 1.12 (1937.2 examples/sec; 0.066 sec/batch)\n",
      "2018-04-28 18:28:49.521421: step 4210, loss = 0.96 (3222.1 examples/sec; 0.040 sec/batch)\n",
      "2018-04-28 18:28:50.046509: step 4220, loss = 1.23 (2437.7 examples/sec; 0.053 sec/batch)\n",
      "2018-04-28 18:28:50.579219: step 4230, loss = 1.10 (2402.8 examples/sec; 0.053 sec/batch)\n",
      "2018-04-28 18:28:51.153631: step 4240, loss = 1.25 (2228.4 examples/sec; 0.057 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-28 18:28:51.714725: step 4250, loss = 1.10 (2281.3 examples/sec; 0.056 sec/batch)\n",
      "2018-04-28 18:28:52.286388: step 4260, loss = 1.13 (2239.1 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:28:52.853229: step 4270, loss = 1.11 (2258.1 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:28:53.420783: step 4280, loss = 1.03 (2255.3 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:28:53.992994: step 4290, loss = 1.15 (2236.9 examples/sec; 0.057 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 17.903\n",
      "2018-04-28 18:28:54.709787: step 4300, loss = 1.20 (1785.7 examples/sec; 0.072 sec/batch)\n",
      "2018-04-28 18:28:55.140565: step 4310, loss = 1.13 (2971.4 examples/sec; 0.043 sec/batch)\n",
      "2018-04-28 18:28:55.711213: step 4320, loss = 0.99 (2243.1 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:28:56.266296: step 4330, loss = 1.02 (2306.0 examples/sec; 0.056 sec/batch)\n",
      "2018-04-28 18:28:56.824235: step 4340, loss = 1.13 (2294.2 examples/sec; 0.056 sec/batch)\n",
      "2018-04-28 18:28:57.405579: step 4350, loss = 1.04 (2201.8 examples/sec; 0.058 sec/batch)\n",
      "2018-04-28 18:28:57.979282: step 4360, loss = 1.07 (2231.1 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:28:58.527940: step 4370, loss = 1.20 (2333.0 examples/sec; 0.055 sec/batch)\n",
      "2018-04-28 18:28:59.090461: step 4380, loss = 1.06 (2275.5 examples/sec; 0.056 sec/batch)\n",
      "2018-04-28 18:28:59.617054: step 4390, loss = 1.13 (2430.7 examples/sec; 0.053 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 17.7899\n",
      "2018-04-28 18:29:00.331630: step 4400, loss = 1.19 (1791.3 examples/sec; 0.071 sec/batch)\n",
      "2018-04-28 18:29:00.773177: step 4410, loss = 1.05 (2898.9 examples/sec; 0.044 sec/batch)\n",
      "2018-04-28 18:29:01.297176: step 4420, loss = 1.15 (2442.8 examples/sec; 0.052 sec/batch)\n",
      "2018-04-28 18:29:01.833818: step 4430, loss = 1.10 (2385.2 examples/sec; 0.054 sec/batch)\n",
      "2018-04-28 18:29:02.371510: step 4440, loss = 1.07 (2380.5 examples/sec; 0.054 sec/batch)\n",
      "2018-04-28 18:29:02.973483: step 4450, loss = 0.86 (2126.3 examples/sec; 0.060 sec/batch)\n",
      "2018-04-28 18:29:03.517917: step 4460, loss = 1.10 (2351.1 examples/sec; 0.054 sec/batch)\n",
      "2018-04-28 18:29:04.038067: step 4470, loss = 1.18 (2460.8 examples/sec; 0.052 sec/batch)\n",
      "2018-04-28 18:29:04.593947: step 4480, loss = 0.87 (2302.6 examples/sec; 0.056 sec/batch)\n",
      "2018-04-28 18:29:05.133327: step 4490, loss = 0.93 (2373.1 examples/sec; 0.054 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 18.2949\n",
      "2018-04-28 18:29:05.796717: step 4500, loss = 1.21 (1929.5 examples/sec; 0.066 sec/batch)\n",
      "2018-04-28 18:29:06.198223: step 4510, loss = 1.10 (3188.0 examples/sec; 0.040 sec/batch)\n",
      "2018-04-28 18:29:06.741053: step 4520, loss = 1.13 (2358.0 examples/sec; 0.054 sec/batch)\n",
      "2018-04-28 18:29:07.271597: step 4530, loss = 1.06 (2412.6 examples/sec; 0.053 sec/batch)\n",
      "2018-04-28 18:29:07.801277: step 4540, loss = 1.15 (2416.6 examples/sec; 0.053 sec/batch)\n",
      "2018-04-28 18:29:08.330049: step 4550, loss = 0.90 (2420.7 examples/sec; 0.053 sec/batch)\n",
      "2018-04-28 18:29:08.868399: step 4560, loss = 1.13 (2377.6 examples/sec; 0.054 sec/batch)\n",
      "2018-04-28 18:29:09.393349: step 4570, loss = 1.04 (2438.3 examples/sec; 0.052 sec/batch)\n",
      "2018-04-28 18:29:09.931858: step 4580, loss = 1.24 (2376.9 examples/sec; 0.054 sec/batch)\n",
      "2018-04-28 18:29:10.456983: step 4590, loss = 1.09 (2437.5 examples/sec; 0.053 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 18.7211\n",
      "2018-04-28 18:29:11.138474: step 4600, loss = 1.13 (1878.2 examples/sec; 0.068 sec/batch)\n",
      "2018-04-28 18:29:11.552078: step 4610, loss = 1.09 (3094.7 examples/sec; 0.041 sec/batch)\n",
      "2018-04-28 18:29:12.110667: step 4620, loss = 1.32 (2291.5 examples/sec; 0.056 sec/batch)\n",
      "2018-04-28 18:29:12.648511: step 4630, loss = 0.94 (2379.9 examples/sec; 0.054 sec/batch)\n",
      "2018-04-28 18:29:13.172419: step 4640, loss = 1.20 (2443.2 examples/sec; 0.052 sec/batch)\n",
      "2018-04-28 18:29:13.711732: step 4650, loss = 1.18 (2373.4 examples/sec; 0.054 sec/batch)\n",
      "2018-04-28 18:29:14.233092: step 4660, loss = 1.01 (2455.1 examples/sec; 0.052 sec/batch)\n",
      "2018-04-28 18:29:14.793864: step 4670, loss = 1.03 (2282.6 examples/sec; 0.056 sec/batch)\n",
      "2018-04-28 18:29:15.352931: step 4680, loss = 1.07 (2289.5 examples/sec; 0.056 sec/batch)\n",
      "2018-04-28 18:29:15.917988: step 4690, loss = 0.92 (2265.3 examples/sec; 0.057 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 17.8765\n",
      "2018-04-28 18:29:16.732725: step 4700, loss = 0.99 (1571.1 examples/sec; 0.081 sec/batch)\n",
      "2018-04-28 18:29:17.238549: step 4710, loss = 0.91 (2530.5 examples/sec; 0.051 sec/batch)\n",
      "2018-04-28 18:29:17.857581: step 4720, loss = 1.17 (2067.7 examples/sec; 0.062 sec/batch)\n",
      "2018-04-28 18:29:18.431170: step 4730, loss = 1.01 (2231.6 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:29:19.001869: step 4740, loss = 1.09 (2242.9 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:29:19.566490: step 4750, loss = 1.00 (2267.0 examples/sec; 0.056 sec/batch)\n",
      "2018-04-28 18:29:20.138379: step 4760, loss = 1.00 (2238.2 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:29:20.687462: step 4770, loss = 1.00 (2331.3 examples/sec; 0.055 sec/batch)\n",
      "2018-04-28 18:29:21.247600: step 4780, loss = 1.10 (2285.0 examples/sec; 0.056 sec/batch)\n",
      "2018-04-28 18:29:21.817768: step 4790, loss = 1.13 (2245.0 examples/sec; 0.057 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 17.1963\n",
      "2018-04-28 18:29:22.548216: step 4800, loss = 1.06 (1752.3 examples/sec; 0.073 sec/batch)\n",
      "2018-04-28 18:29:22.983696: step 4810, loss = 1.03 (2939.3 examples/sec; 0.044 sec/batch)\n",
      "2018-04-28 18:29:23.557290: step 4820, loss = 0.93 (2231.5 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:29:24.115511: step 4830, loss = 1.22 (2293.0 examples/sec; 0.056 sec/batch)\n",
      "2018-04-28 18:29:24.655361: step 4840, loss = 1.09 (2371.0 examples/sec; 0.054 sec/batch)\n",
      "2018-04-28 18:29:25.200948: step 4850, loss = 0.96 (2346.1 examples/sec; 0.055 sec/batch)\n",
      "2018-04-28 18:29:25.774724: step 4860, loss = 1.01 (2230.8 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:29:26.326128: step 4870, loss = 1.06 (2321.3 examples/sec; 0.055 sec/batch)\n",
      "2018-04-28 18:29:26.868576: step 4880, loss = 0.96 (2359.7 examples/sec; 0.054 sec/batch)\n",
      "2018-04-28 18:29:27.395869: step 4890, loss = 0.96 (2427.5 examples/sec; 0.053 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 18.041\n",
      "2018-04-28 18:29:28.090761: step 4900, loss = 1.04 (1842.0 examples/sec; 0.069 sec/batch)\n",
      "2018-04-28 18:29:28.507664: step 4910, loss = 1.08 (3070.3 examples/sec; 0.042 sec/batch)\n",
      "2018-04-28 18:29:29.028524: step 4920, loss = 0.98 (2457.5 examples/sec; 0.052 sec/batch)\n",
      "2018-04-28 18:29:29.608160: step 4930, loss = 1.01 (2208.3 examples/sec; 0.058 sec/batch)\n",
      "2018-04-28 18:29:30.193831: step 4940, loss = 1.00 (2185.5 examples/sec; 0.059 sec/batch)\n",
      "2018-04-28 18:29:30.769153: step 4950, loss = 1.06 (2224.8 examples/sec; 0.058 sec/batch)\n",
      "2018-04-28 18:29:31.347465: step 4960, loss = 0.90 (2213.3 examples/sec; 0.058 sec/batch)\n",
      "2018-04-28 18:29:31.915981: step 4970, loss = 1.07 (2251.5 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:29:32.501834: step 4980, loss = 0.88 (2184.9 examples/sec; 0.059 sec/batch)\n",
      "2018-04-28 18:29:33.084215: step 4990, loss = 0.88 (2197.9 examples/sec; 0.058 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 17.4274\n",
      "2018-04-28 18:29:33.828630: step 5000, loss = 0.89 (1719.5 examples/sec; 0.074 sec/batch)\n",
      "2018-04-28 18:29:34.265497: step 5010, loss = 0.95 (2930.0 examples/sec; 0.044 sec/batch)\n",
      "2018-04-28 18:29:34.835788: step 5020, loss = 0.87 (2244.5 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:29:35.400966: step 5030, loss = 1.25 (2264.8 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:29:35.974011: step 5040, loss = 1.16 (2233.7 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:29:36.541469: step 5050, loss = 1.22 (2255.7 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:29:37.103966: step 5060, loss = 1.01 (2275.6 examples/sec; 0.056 sec/batch)\n",
      "2018-04-28 18:29:37.687571: step 5070, loss = 0.92 (2193.3 examples/sec; 0.058 sec/batch)\n",
      "2018-04-28 18:29:38.274180: step 5080, loss = 0.84 (2182.0 examples/sec; 0.059 sec/batch)\n",
      "2018-04-28 18:29:38.841524: step 5090, loss = 1.04 (2256.1 examples/sec; 0.057 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 17.3708\n",
      "2018-04-28 18:29:39.585431: step 5100, loss = 1.10 (1720.6 examples/sec; 0.074 sec/batch)\n",
      "2018-04-28 18:29:40.024537: step 5110, loss = 1.08 (2915.0 examples/sec; 0.044 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-28 18:29:40.599305: step 5120, loss = 1.09 (2227.0 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:29:41.164944: step 5130, loss = 0.95 (2262.9 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:29:41.737147: step 5140, loss = 0.85 (2237.0 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:29:42.302794: step 5150, loss = 1.13 (2262.9 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:29:42.883333: step 5160, loss = 0.89 (2204.8 examples/sec; 0.058 sec/batch)\n",
      "2018-04-28 18:29:43.421388: step 5170, loss = 0.96 (2379.0 examples/sec; 0.054 sec/batch)\n",
      "2018-04-28 18:29:43.974460: step 5180, loss = 0.95 (2314.3 examples/sec; 0.055 sec/batch)\n",
      "2018-04-28 18:29:44.519573: step 5190, loss = 1.10 (2348.1 examples/sec; 0.055 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 17.7395\n",
      "2018-04-28 18:29:45.222541: step 5200, loss = 0.95 (1820.9 examples/sec; 0.070 sec/batch)\n",
      "2018-04-28 18:29:45.658177: step 5210, loss = 1.02 (2938.3 examples/sec; 0.044 sec/batch)\n",
      "2018-04-28 18:29:46.196062: step 5220, loss = 1.17 (2379.7 examples/sec; 0.054 sec/batch)\n",
      "2018-04-28 18:29:46.773165: step 5230, loss = 1.14 (2218.0 examples/sec; 0.058 sec/batch)\n",
      "2018-04-28 18:29:47.342333: step 5240, loss = 1.10 (2248.9 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:29:47.906715: step 5250, loss = 1.17 (2268.0 examples/sec; 0.056 sec/batch)\n",
      "2018-04-28 18:29:48.467679: step 5260, loss = 0.95 (2281.8 examples/sec; 0.056 sec/batch)\n",
      "2018-04-28 18:29:48.995037: step 5270, loss = 0.83 (2427.2 examples/sec; 0.053 sec/batch)\n",
      "2018-04-28 18:29:49.528467: step 5280, loss = 0.95 (2399.6 examples/sec; 0.053 sec/batch)\n",
      "2018-04-28 18:29:50.059321: step 5290, loss = 1.07 (2411.2 examples/sec; 0.053 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 18.184\n",
      "2018-04-28 18:29:50.721857: step 5300, loss = 0.83 (1932.0 examples/sec; 0.066 sec/batch)\n",
      "2018-04-28 18:29:51.124165: step 5310, loss = 0.99 (3181.7 examples/sec; 0.040 sec/batch)\n",
      "2018-04-28 18:29:51.667887: step 5320, loss = 0.90 (2354.1 examples/sec; 0.054 sec/batch)\n",
      "2018-04-28 18:29:52.199090: step 5330, loss = 1.02 (2409.6 examples/sec; 0.053 sec/batch)\n",
      "2018-04-28 18:29:52.747135: step 5340, loss = 1.20 (2335.6 examples/sec; 0.055 sec/batch)\n",
      "2018-04-28 18:29:53.271002: step 5350, loss = 1.01 (2443.4 examples/sec; 0.052 sec/batch)\n",
      "2018-04-28 18:29:53.805890: step 5360, loss = 1.09 (2393.0 examples/sec; 0.053 sec/batch)\n",
      "2018-04-28 18:29:54.342198: step 5370, loss = 1.18 (2386.7 examples/sec; 0.054 sec/batch)\n",
      "2018-04-28 18:29:54.894868: step 5380, loss = 0.87 (2316.0 examples/sec; 0.055 sec/batch)\n",
      "2018-04-28 18:29:55.495004: step 5390, loss = 0.94 (2132.9 examples/sec; 0.060 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 18.1805\n",
      "2018-04-28 18:29:56.222085: step 5400, loss = 0.88 (1760.5 examples/sec; 0.073 sec/batch)\n",
      "2018-04-28 18:29:56.679578: step 5410, loss = 1.00 (2797.9 examples/sec; 0.046 sec/batch)\n",
      "2018-04-28 18:29:57.250589: step 5420, loss = 0.92 (2241.6 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:29:57.818440: step 5430, loss = 0.91 (2254.1 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:29:58.396912: step 5440, loss = 1.11 (2212.7 examples/sec; 0.058 sec/batch)\n",
      "2018-04-28 18:29:58.965227: step 5450, loss = 0.99 (2252.3 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:29:59.532834: step 5460, loss = 0.95 (2255.1 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:30:00.108738: step 5470, loss = 1.03 (2222.6 examples/sec; 0.058 sec/batch)\n",
      "2018-04-28 18:30:00.689635: step 5480, loss = 0.89 (2203.5 examples/sec; 0.058 sec/batch)\n",
      "2018-04-28 18:30:01.257361: step 5490, loss = 0.93 (2254.6 examples/sec; 0.057 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 17.4683\n",
      "2018-04-28 18:30:01.947330: step 5500, loss = 0.93 (1855.2 examples/sec; 0.069 sec/batch)\n",
      "2018-04-28 18:30:02.373626: step 5510, loss = 0.93 (3002.5 examples/sec; 0.043 sec/batch)\n",
      "2018-04-28 18:30:02.920489: step 5520, loss = 1.07 (2340.6 examples/sec; 0.055 sec/batch)\n",
      "2018-04-28 18:30:03.496470: step 5530, loss = 0.97 (2222.3 examples/sec; 0.058 sec/batch)\n",
      "2018-04-28 18:30:04.049275: step 5540, loss = 0.94 (2315.5 examples/sec; 0.055 sec/batch)\n",
      "2018-04-28 18:30:04.622698: step 5550, loss = 0.73 (2232.2 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:30:05.188943: step 5560, loss = 1.02 (2260.5 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:30:05.756705: step 5570, loss = 0.87 (2254.5 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:30:06.330657: step 5580, loss = 0.95 (2230.2 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:30:06.887605: step 5590, loss = 0.97 (2298.2 examples/sec; 0.056 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 17.777\n",
      "2018-04-28 18:30:07.572461: step 5600, loss = 0.83 (1869.0 examples/sec; 0.068 sec/batch)\n",
      "2018-04-28 18:30:07.983229: step 5610, loss = 1.13 (3116.1 examples/sec; 0.041 sec/batch)\n",
      "2018-04-28 18:30:08.502222: step 5620, loss = 1.02 (2466.3 examples/sec; 0.052 sec/batch)\n",
      "2018-04-28 18:30:09.041610: step 5630, loss = 1.00 (2373.1 examples/sec; 0.054 sec/batch)\n",
      "2018-04-28 18:30:09.568679: step 5640, loss = 1.02 (2428.5 examples/sec; 0.053 sec/batch)\n",
      "2018-04-28 18:30:10.087508: step 5650, loss = 1.17 (2467.1 examples/sec; 0.052 sec/batch)\n",
      "2018-04-28 18:30:10.625184: step 5660, loss = 1.08 (2380.6 examples/sec; 0.054 sec/batch)\n",
      "2018-04-28 18:30:11.160004: step 5670, loss = 0.82 (2393.3 examples/sec; 0.053 sec/batch)\n",
      "2018-04-28 18:30:11.688886: step 5680, loss = 0.96 (2420.2 examples/sec; 0.053 sec/batch)\n",
      "2018-04-28 18:30:12.281126: step 5690, loss = 1.04 (2161.3 examples/sec; 0.059 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 18.4901\n",
      "2018-04-28 18:30:12.980652: step 5700, loss = 0.99 (1829.8 examples/sec; 0.070 sec/batch)\n",
      "2018-04-28 18:30:13.398015: step 5710, loss = 0.95 (3066.9 examples/sec; 0.042 sec/batch)\n",
      "2018-04-28 18:30:13.930272: step 5720, loss = 0.99 (2404.8 examples/sec; 0.053 sec/batch)\n",
      "2018-04-28 18:30:14.472767: step 5730, loss = 0.94 (2359.5 examples/sec; 0.054 sec/batch)\n",
      "2018-04-28 18:30:15.009267: step 5740, loss = 0.94 (2385.8 examples/sec; 0.054 sec/batch)\n",
      "2018-04-28 18:30:15.540819: step 5750, loss = 1.19 (2408.1 examples/sec; 0.053 sec/batch)\n",
      "2018-04-28 18:30:16.081472: step 5760, loss = 0.77 (2367.5 examples/sec; 0.054 sec/batch)\n",
      "2018-04-28 18:30:16.621750: step 5770, loss = 1.10 (2369.1 examples/sec; 0.054 sec/batch)\n",
      "2018-04-28 18:30:17.163328: step 5780, loss = 0.90 (2363.5 examples/sec; 0.054 sec/batch)\n",
      "2018-04-28 18:30:17.701441: step 5790, loss = 0.80 (2378.7 examples/sec; 0.054 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 18.5738\n",
      "2018-04-28 18:30:18.364369: step 5800, loss = 0.92 (1930.8 examples/sec; 0.066 sec/batch)\n",
      "2018-04-28 18:30:18.798729: step 5810, loss = 0.93 (2946.9 examples/sec; 0.043 sec/batch)\n",
      "2018-04-28 18:30:19.407794: step 5820, loss = 1.12 (2101.6 examples/sec; 0.061 sec/batch)\n",
      "2018-04-28 18:30:20.059798: step 5830, loss = 1.08 (1963.2 examples/sec; 0.065 sec/batch)\n",
      "2018-04-28 18:30:20.687776: step 5840, loss = 0.87 (2038.3 examples/sec; 0.063 sec/batch)\n",
      "2018-04-28 18:30:21.260536: step 5850, loss = 1.11 (2234.8 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:30:21.833677: step 5860, loss = 1.10 (2233.3 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:30:22.399370: step 5870, loss = 0.96 (2262.7 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:30:22.971714: step 5880, loss = 0.88 (2236.4 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:30:23.547273: step 5890, loss = 0.94 (2223.9 examples/sec; 0.058 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 16.8217\n",
      "2018-04-28 18:30:24.313213: step 5900, loss = 1.25 (1671.1 examples/sec; 0.077 sec/batch)\n",
      "2018-04-28 18:30:24.754935: step 5910, loss = 1.01 (2897.8 examples/sec; 0.044 sec/batch)\n",
      "2018-04-28 18:30:25.326016: step 5920, loss = 1.02 (2241.4 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:30:25.898816: step 5930, loss = 0.81 (2234.6 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:30:26.478708: step 5940, loss = 1.00 (2207.3 examples/sec; 0.058 sec/batch)\n",
      "2018-04-28 18:30:27.045853: step 5950, loss = 1.01 (2256.9 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:30:27.702203: step 5960, loss = 0.86 (1950.2 examples/sec; 0.066 sec/batch)\n",
      "2018-04-28 18:30:28.293606: step 5970, loss = 0.86 (2164.4 examples/sec; 0.059 sec/batch)\n",
      "2018-04-28 18:30:28.861861: step 5980, loss = 0.90 (2252.5 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:30:29.429499: step 5990, loss = 0.81 (2254.9 examples/sec; 0.057 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 16.886\n",
      "2018-04-28 18:30:30.230984: step 6000, loss = 0.95 (1597.0 examples/sec; 0.080 sec/batch)\n",
      "2018-04-28 18:30:30.679502: step 6010, loss = 0.89 (2853.9 examples/sec; 0.045 sec/batch)\n",
      "2018-04-28 18:30:31.254502: step 6020, loss = 0.94 (2226.1 examples/sec; 0.058 sec/batch)\n",
      "2018-04-28 18:30:31.819948: step 6030, loss = 0.99 (2263.7 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:30:32.385834: step 6040, loss = 0.81 (2262.0 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:30:32.924340: step 6050, loss = 1.08 (2376.9 examples/sec; 0.054 sec/batch)\n",
      "2018-04-28 18:30:33.484735: step 6060, loss = 1.09 (2284.1 examples/sec; 0.056 sec/batch)\n",
      "2018-04-28 18:30:34.063367: step 6070, loss = 0.94 (2212.1 examples/sec; 0.058 sec/batch)\n",
      "2018-04-28 18:30:34.656172: step 6080, loss = 0.97 (2159.2 examples/sec; 0.059 sec/batch)\n",
      "2018-04-28 18:30:35.216795: step 6090, loss = 1.01 (2283.2 examples/sec; 0.056 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 17.5459\n",
      "2018-04-28 18:30:35.930545: step 6100, loss = 1.03 (1793.3 examples/sec; 0.071 sec/batch)\n",
      "2018-04-28 18:30:36.401618: step 6110, loss = 0.97 (2717.2 examples/sec; 0.047 sec/batch)\n",
      "2018-04-28 18:30:36.970381: step 6120, loss = 0.71 (2250.5 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:30:37.575955: step 6130, loss = 0.93 (2113.7 examples/sec; 0.061 sec/batch)\n",
      "2018-04-28 18:30:38.148825: step 6140, loss = 1.00 (2234.4 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:30:38.741713: step 6150, loss = 1.00 (2158.9 examples/sec; 0.059 sec/batch)\n",
      "2018-04-28 18:30:39.334234: step 6160, loss = 0.85 (2160.3 examples/sec; 0.059 sec/batch)\n",
      "2018-04-28 18:30:39.909552: step 6170, loss = 0.91 (2224.8 examples/sec; 0.058 sec/batch)\n",
      "2018-04-28 18:30:40.515555: step 6180, loss = 0.92 (2112.2 examples/sec; 0.061 sec/batch)\n",
      "2018-04-28 18:30:41.100693: step 6190, loss = 0.88 (2187.5 examples/sec; 0.059 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 17.0232\n",
      "2018-04-28 18:30:41.806878: step 6200, loss = 1.04 (1812.6 examples/sec; 0.071 sec/batch)\n",
      "2018-04-28 18:30:42.227585: step 6210, loss = 0.91 (3042.5 examples/sec; 0.042 sec/batch)\n",
      "2018-04-28 18:30:42.777948: step 6220, loss = 1.00 (2325.7 examples/sec; 0.055 sec/batch)\n",
      "2018-04-28 18:30:43.308922: step 6230, loss = 0.82 (2410.7 examples/sec; 0.053 sec/batch)\n",
      "2018-04-28 18:30:43.885838: step 6240, loss = 0.92 (2218.7 examples/sec; 0.058 sec/batch)\n",
      "2018-04-28 18:30:44.470959: step 6250, loss = 0.89 (2187.6 examples/sec; 0.059 sec/batch)\n",
      "2018-04-28 18:30:45.077515: step 6260, loss = 0.86 (2110.3 examples/sec; 0.061 sec/batch)\n",
      "2018-04-28 18:30:45.647023: step 6270, loss = 1.00 (2247.5 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:30:46.214122: step 6280, loss = 0.89 (2257.1 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:30:46.787930: step 6290, loss = 0.93 (2230.7 examples/sec; 0.057 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 17.4678\n",
      "2018-04-28 18:30:47.529844: step 6300, loss = 0.78 (1725.3 examples/sec; 0.074 sec/batch)\n",
      "2018-04-28 18:30:47.966918: step 6310, loss = 0.85 (2928.6 examples/sec; 0.044 sec/batch)\n",
      "2018-04-28 18:30:48.530502: step 6320, loss = 0.97 (2271.2 examples/sec; 0.056 sec/batch)\n",
      "2018-04-28 18:30:49.167777: step 6330, loss = 1.02 (2008.5 examples/sec; 0.064 sec/batch)\n",
      "2018-04-28 18:30:49.783950: step 6340, loss = 0.79 (2077.3 examples/sec; 0.062 sec/batch)\n",
      "2018-04-28 18:30:50.439348: step 6350, loss = 0.76 (1953.0 examples/sec; 0.066 sec/batch)\n",
      "2018-04-28 18:30:51.098313: step 6360, loss = 1.20 (1942.4 examples/sec; 0.066 sec/batch)\n",
      "2018-04-28 18:30:51.743042: step 6370, loss = 0.82 (1985.3 examples/sec; 0.064 sec/batch)\n",
      "2018-04-28 18:30:52.322342: step 6380, loss = 0.92 (2209.5 examples/sec; 0.058 sec/batch)\n",
      "2018-04-28 18:30:52.891321: step 6390, loss = 0.99 (2249.7 examples/sec; 0.057 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 16.3756\n",
      "2018-04-28 18:30:53.636358: step 6400, loss = 0.95 (1718.0 examples/sec; 0.075 sec/batch)\n",
      "2018-04-28 18:30:54.084344: step 6410, loss = 1.24 (2857.3 examples/sec; 0.045 sec/batch)\n",
      "2018-04-28 18:30:54.682808: step 6420, loss = 0.95 (2138.8 examples/sec; 0.060 sec/batch)\n",
      "2018-04-28 18:30:55.283546: step 6430, loss = 0.79 (2130.7 examples/sec; 0.060 sec/batch)\n",
      "2018-04-28 18:30:55.852958: step 6440, loss = 1.12 (2247.9 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:30:56.422341: step 6450, loss = 0.85 (2248.0 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:30:56.991949: step 6460, loss = 1.02 (2247.2 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:30:57.597779: step 6470, loss = 0.95 (2112.8 examples/sec; 0.061 sec/batch)\n",
      "2018-04-28 18:30:58.158477: step 6480, loss = 0.87 (2282.9 examples/sec; 0.056 sec/batch)\n",
      "2018-04-28 18:30:58.703392: step 6490, loss = 0.90 (2349.0 examples/sec; 0.054 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 17.3112\n",
      "2018-04-28 18:30:59.412751: step 6500, loss = 1.29 (1804.4 examples/sec; 0.071 sec/batch)\n",
      "2018-04-28 18:30:59.843594: step 6510, loss = 0.98 (2970.9 examples/sec; 0.043 sec/batch)\n",
      "2018-04-28 18:31:00.389855: step 6520, loss = 1.03 (2343.2 examples/sec; 0.055 sec/batch)\n",
      "2018-04-28 18:31:00.953142: step 6530, loss = 0.99 (2272.4 examples/sec; 0.056 sec/batch)\n",
      "2018-04-28 18:31:01.514628: step 6540, loss = 1.03 (2279.7 examples/sec; 0.056 sec/batch)\n",
      "2018-04-28 18:31:02.085304: step 6550, loss = 0.93 (2243.0 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:31:02.648403: step 6560, loss = 0.84 (2273.2 examples/sec; 0.056 sec/batch)\n",
      "2018-04-28 18:31:03.201405: step 6570, loss = 0.86 (2314.6 examples/sec; 0.055 sec/batch)\n",
      "2018-04-28 18:31:03.728929: step 6580, loss = 0.97 (2426.4 examples/sec; 0.053 sec/batch)\n",
      "2018-04-28 18:31:04.282592: step 6590, loss = 0.93 (2311.9 examples/sec; 0.055 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 17.8285\n",
      "2018-04-28 18:31:05.022282: step 6600, loss = 0.81 (1730.4 examples/sec; 0.074 sec/batch)\n",
      "2018-04-28 18:31:05.439589: step 6610, loss = 0.94 (3067.3 examples/sec; 0.042 sec/batch)\n",
      "2018-04-28 18:31:06.028779: step 6620, loss = 0.92 (2172.5 examples/sec; 0.059 sec/batch)\n",
      "2018-04-28 18:31:06.586482: step 6630, loss = 1.07 (2295.1 examples/sec; 0.056 sec/batch)\n",
      "2018-04-28 18:31:07.139164: step 6640, loss = 0.93 (2316.0 examples/sec; 0.055 sec/batch)\n",
      "2018-04-28 18:31:07.696678: step 6650, loss = 0.98 (2295.9 examples/sec; 0.056 sec/batch)\n",
      "2018-04-28 18:31:08.298664: step 6660, loss = 1.17 (2126.3 examples/sec; 0.060 sec/batch)\n",
      "2018-04-28 18:31:08.870619: step 6670, loss = 0.85 (2237.9 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:31:09.425009: step 6680, loss = 0.92 (2308.8 examples/sec; 0.055 sec/batch)\n",
      "2018-04-28 18:31:09.975165: step 6690, loss = 1.14 (2326.6 examples/sec; 0.055 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 17.6533\n",
      "2018-04-28 18:31:10.686677: step 6700, loss = 1.16 (1799.0 examples/sec; 0.071 sec/batch)\n",
      "2018-04-28 18:31:11.131151: step 6710, loss = 1.09 (2879.8 examples/sec; 0.044 sec/batch)\n",
      "2018-04-28 18:31:11.666270: step 6720, loss = 0.94 (2392.0 examples/sec; 0.054 sec/batch)\n",
      "2018-04-28 18:31:12.201341: step 6730, loss = 0.86 (2392.2 examples/sec; 0.054 sec/batch)\n",
      "2018-04-28 18:31:12.736610: step 6740, loss = 0.85 (2391.3 examples/sec; 0.054 sec/batch)\n",
      "2018-04-28 18:31:13.263660: step 6750, loss = 1.02 (2428.6 examples/sec; 0.053 sec/batch)\n",
      "2018-04-28 18:31:13.805652: step 6760, loss = 1.11 (2361.7 examples/sec; 0.054 sec/batch)\n",
      "2018-04-28 18:31:14.373463: step 6770, loss = 0.84 (2254.3 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:31:14.941117: step 6780, loss = 1.03 (2254.9 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:31:15.476904: step 6790, loss = 1.06 (2389.0 examples/sec; 0.054 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 18.3327\n",
      "2018-04-28 18:31:16.141159: step 6800, loss = 0.86 (1927.0 examples/sec; 0.066 sec/batch)\n",
      "2018-04-28 18:31:16.544693: step 6810, loss = 1.17 (3172.0 examples/sec; 0.040 sec/batch)\n",
      "2018-04-28 18:31:17.075926: step 6820, loss = 1.02 (2409.5 examples/sec; 0.053 sec/batch)\n",
      "2018-04-28 18:31:17.601653: step 6830, loss = 0.92 (2434.7 examples/sec; 0.053 sec/batch)\n",
      "2018-04-28 18:31:18.128682: step 6840, loss = 0.87 (2428.7 examples/sec; 0.053 sec/batch)\n",
      "2018-04-28 18:31:18.648452: step 6850, loss = 1.00 (2462.6 examples/sec; 0.052 sec/batch)\n",
      "2018-04-28 18:31:19.174451: step 6860, loss = 0.93 (2433.5 examples/sec; 0.053 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-28 18:31:19.710439: step 6870, loss = 0.99 (2388.1 examples/sec; 0.054 sec/batch)\n",
      "2018-04-28 18:31:20.280700: step 6880, loss = 0.87 (2244.6 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:31:20.855191: step 6890, loss = 0.83 (2228.1 examples/sec; 0.057 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 18.4182\n",
      "2018-04-28 18:31:21.570604: step 6900, loss = 1.09 (1789.2 examples/sec; 0.072 sec/batch)\n",
      "2018-04-28 18:31:21.992048: step 6910, loss = 0.81 (3037.2 examples/sec; 0.042 sec/batch)\n",
      "2018-04-28 18:31:22.549011: step 6920, loss = 0.91 (2298.2 examples/sec; 0.056 sec/batch)\n",
      "2018-04-28 18:31:23.107768: step 6930, loss = 0.91 (2290.8 examples/sec; 0.056 sec/batch)\n",
      "2018-04-28 18:31:23.650452: step 6940, loss = 1.07 (2358.6 examples/sec; 0.054 sec/batch)\n",
      "2018-04-28 18:31:24.186585: step 6950, loss = 0.94 (2387.5 examples/sec; 0.054 sec/batch)\n",
      "2018-04-28 18:31:24.726134: step 6960, loss = 0.89 (2372.4 examples/sec; 0.054 sec/batch)\n",
      "2018-04-28 18:31:25.279987: step 6970, loss = 0.83 (2311.1 examples/sec; 0.055 sec/batch)\n",
      "2018-04-28 18:31:25.832374: step 6980, loss = 0.90 (2317.2 examples/sec; 0.055 sec/batch)\n",
      "2018-04-28 18:31:26.390109: step 6990, loss = 1.35 (2295.0 examples/sec; 0.056 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 18.1328\n",
      "2018-04-28 18:31:27.085564: step 7000, loss = 0.92 (1840.5 examples/sec; 0.070 sec/batch)\n",
      "2018-04-28 18:31:27.505964: step 7010, loss = 0.81 (3044.7 examples/sec; 0.042 sec/batch)\n",
      "2018-04-28 18:31:28.039078: step 7020, loss = 0.85 (2401.0 examples/sec; 0.053 sec/batch)\n",
      "2018-04-28 18:31:28.571185: step 7030, loss = 1.07 (2405.5 examples/sec; 0.053 sec/batch)\n",
      "2018-04-28 18:31:29.098775: step 7040, loss = 0.84 (2426.1 examples/sec; 0.053 sec/batch)\n",
      "2018-04-28 18:31:29.627819: step 7050, loss = 0.82 (2419.5 examples/sec; 0.053 sec/batch)\n",
      "2018-04-28 18:31:30.157133: step 7060, loss = 0.68 (2418.2 examples/sec; 0.053 sec/batch)\n",
      "2018-04-28 18:31:30.681461: step 7070, loss = 0.67 (2441.2 examples/sec; 0.052 sec/batch)\n",
      "2018-04-28 18:31:31.212127: step 7080, loss = 1.05 (2412.1 examples/sec; 0.053 sec/batch)\n",
      "2018-04-28 18:31:31.768383: step 7090, loss = 0.87 (2301.1 examples/sec; 0.056 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 18.6289\n",
      "2018-04-28 18:31:32.453632: step 7100, loss = 1.19 (1867.9 examples/sec; 0.069 sec/batch)\n",
      "2018-04-28 18:31:32.856377: step 7110, loss = 0.89 (3178.2 examples/sec; 0.040 sec/batch)\n",
      "2018-04-28 18:31:33.406354: step 7120, loss = 0.88 (2327.4 examples/sec; 0.055 sec/batch)\n",
      "2018-04-28 18:31:33.973381: step 7130, loss = 0.87 (2257.4 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:31:34.545990: step 7140, loss = 0.88 (2235.4 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:31:35.119694: step 7150, loss = 0.98 (2231.1 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:31:35.689731: step 7160, loss = 0.98 (2245.5 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:31:36.258206: step 7170, loss = 0.99 (2251.7 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:31:36.826842: step 7180, loss = 0.92 (2251.0 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:31:37.398426: step 7190, loss = 0.87 (2239.4 examples/sec; 0.057 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 17.6096\n",
      "2018-04-28 18:31:38.132220: step 7200, loss = 0.90 (1744.4 examples/sec; 0.073 sec/batch)\n",
      "2018-04-28 18:31:38.566828: step 7210, loss = 1.03 (2945.2 examples/sec; 0.043 sec/batch)\n",
      "2018-04-28 18:31:39.135953: step 7220, loss = 0.95 (2249.1 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:31:39.703804: step 7230, loss = 0.76 (2254.1 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:31:40.267843: step 7240, loss = 0.99 (2269.3 examples/sec; 0.056 sec/batch)\n",
      "2018-04-28 18:31:40.811006: step 7250, loss = 0.96 (2356.6 examples/sec; 0.054 sec/batch)\n",
      "2018-04-28 18:31:41.356590: step 7260, loss = 0.73 (2346.1 examples/sec; 0.055 sec/batch)\n",
      "2018-04-28 18:31:41.913528: step 7270, loss = 0.91 (2298.3 examples/sec; 0.056 sec/batch)\n",
      "2018-04-28 18:31:42.482403: step 7280, loss = 0.86 (2250.0 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:31:43.045641: step 7290, loss = 0.93 (2272.6 examples/sec; 0.056 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 17.7964\n",
      "2018-04-28 18:31:43.751396: step 7300, loss = 0.77 (1813.7 examples/sec; 0.071 sec/batch)\n",
      "2018-04-28 18:31:44.177842: step 7310, loss = 0.97 (3001.6 examples/sec; 0.043 sec/batch)\n",
      "2018-04-28 18:31:44.731232: step 7320, loss = 0.83 (2313.0 examples/sec; 0.055 sec/batch)\n",
      "2018-04-28 18:31:45.275865: step 7330, loss = 1.02 (2350.2 examples/sec; 0.054 sec/batch)\n",
      "2018-04-28 18:31:45.814146: step 7340, loss = 0.88 (2377.9 examples/sec; 0.054 sec/batch)\n",
      "2018-04-28 18:31:46.335829: step 7350, loss = 0.88 (2453.6 examples/sec; 0.052 sec/batch)\n",
      "2018-04-28 18:31:46.879108: step 7360, loss = 0.87 (2356.0 examples/sec; 0.054 sec/batch)\n",
      "2018-04-28 18:31:47.408070: step 7370, loss = 0.80 (2419.9 examples/sec; 0.053 sec/batch)\n",
      "2018-04-28 18:31:47.943680: step 7380, loss = 0.86 (2389.8 examples/sec; 0.054 sec/batch)\n",
      "2018-04-28 18:31:48.501184: step 7390, loss = 0.92 (2295.9 examples/sec; 0.056 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 18.3274\n",
      "2018-04-28 18:31:49.207890: step 7400, loss = 0.96 (1811.2 examples/sec; 0.071 sec/batch)\n",
      "2018-04-28 18:31:49.637164: step 7410, loss = 0.80 (2981.8 examples/sec; 0.043 sec/batch)\n",
      "2018-04-28 18:31:50.177466: step 7420, loss = 0.83 (2369.0 examples/sec; 0.054 sec/batch)\n",
      "2018-04-28 18:31:50.700194: step 7430, loss = 0.98 (2448.7 examples/sec; 0.052 sec/batch)\n",
      "2018-04-28 18:31:51.226995: step 7440, loss = 0.84 (2429.8 examples/sec; 0.053 sec/batch)\n",
      "2018-04-28 18:31:51.790693: step 7450, loss = 0.85 (2270.7 examples/sec; 0.056 sec/batch)\n",
      "2018-04-28 18:31:52.366542: step 7460, loss = 1.01 (2222.8 examples/sec; 0.058 sec/batch)\n",
      "2018-04-28 18:31:52.935689: step 7470, loss = 1.08 (2249.0 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:31:53.513489: step 7480, loss = 0.97 (2215.3 examples/sec; 0.058 sec/batch)\n",
      "2018-04-28 18:31:54.087891: step 7490, loss = 0.91 (2228.4 examples/sec; 0.057 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 17.5035\n",
      "2018-04-28 18:31:54.922036: step 7500, loss = 0.76 (1534.5 examples/sec; 0.083 sec/batch)\n",
      "2018-04-28 18:31:55.420699: step 7510, loss = 0.96 (2566.9 examples/sec; 0.050 sec/batch)\n",
      "2018-04-28 18:31:56.004339: step 7520, loss = 0.97 (2193.1 examples/sec; 0.058 sec/batch)\n",
      "2018-04-28 18:31:56.575114: step 7530, loss = 0.89 (2242.6 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:31:57.140496: step 7540, loss = 0.82 (2264.0 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:31:57.685280: step 7550, loss = 0.77 (2349.6 examples/sec; 0.054 sec/batch)\n",
      "2018-04-28 18:31:58.247395: step 7560, loss = 0.91 (2277.1 examples/sec; 0.056 sec/batch)\n",
      "2018-04-28 18:31:58.788734: step 7570, loss = 1.05 (2364.5 examples/sec; 0.054 sec/batch)\n",
      "2018-04-28 18:31:59.342471: step 7580, loss = 0.89 (2311.6 examples/sec; 0.055 sec/batch)\n",
      "2018-04-28 18:31:59.902871: step 7590, loss = 0.91 (2284.1 examples/sec; 0.056 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 17.511\n",
      "2018-04-28 18:32:00.634089: step 7600, loss = 0.86 (1750.5 examples/sec; 0.073 sec/batch)\n",
      "2018-04-28 18:32:01.110850: step 7610, loss = 1.05 (2684.8 examples/sec; 0.048 sec/batch)\n",
      "2018-04-28 18:32:01.668398: step 7620, loss = 0.83 (2295.8 examples/sec; 0.056 sec/batch)\n",
      "2018-04-28 18:32:02.229992: step 7630, loss = 0.89 (2279.2 examples/sec; 0.056 sec/batch)\n",
      "2018-04-28 18:32:02.806993: step 7640, loss = 0.78 (2218.4 examples/sec; 0.058 sec/batch)\n",
      "2018-04-28 18:32:03.386940: step 7650, loss = 0.96 (2207.1 examples/sec; 0.058 sec/batch)\n",
      "2018-04-28 18:32:03.916742: step 7660, loss = 0.76 (2416.0 examples/sec; 0.053 sec/batch)\n",
      "2018-04-28 18:32:04.482187: step 7670, loss = 0.74 (2263.7 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:32:05.046925: step 7680, loss = 0.85 (2266.5 examples/sec; 0.056 sec/batch)\n",
      "2018-04-28 18:32:05.614866: step 7690, loss = 0.89 (2253.8 examples/sec; 0.057 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 17.4926\n",
      "2018-04-28 18:32:06.348727: step 7700, loss = 0.88 (1744.2 examples/sec; 0.073 sec/batch)\n",
      "2018-04-28 18:32:06.784431: step 7710, loss = 0.97 (2937.8 examples/sec; 0.044 sec/batch)\n",
      "2018-04-28 18:32:07.354259: step 7720, loss = 0.89 (2246.3 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:32:07.922433: step 7730, loss = 0.91 (2252.8 examples/sec; 0.057 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-28 18:32:08.492682: step 7740, loss = 0.90 (2244.6 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:32:09.078174: step 7750, loss = 0.84 (2186.2 examples/sec; 0.059 sec/batch)\n",
      "2018-04-28 18:32:09.650996: step 7760, loss = 1.06 (2234.5 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:32:10.220777: step 7770, loss = 0.73 (2246.5 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:32:10.827665: step 7780, loss = 0.97 (2109.1 examples/sec; 0.061 sec/batch)\n",
      "2018-04-28 18:32:11.403470: step 7790, loss = 0.99 (2223.0 examples/sec; 0.058 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 17.2359\n",
      "2018-04-28 18:32:12.150464: step 7800, loss = 0.85 (1713.5 examples/sec; 0.075 sec/batch)\n",
      "2018-04-28 18:32:12.653345: step 7810, loss = 0.90 (2545.4 examples/sec; 0.050 sec/batch)\n",
      "2018-04-28 18:32:13.224127: step 7820, loss = 0.87 (2242.5 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:32:13.837416: step 7830, loss = 0.99 (2087.1 examples/sec; 0.061 sec/batch)\n",
      "2018-04-28 18:32:14.431870: step 7840, loss = 1.01 (2153.2 examples/sec; 0.059 sec/batch)\n",
      "2018-04-28 18:32:15.055764: step 7850, loss = 1.12 (2051.6 examples/sec; 0.062 sec/batch)\n",
      "2018-04-28 18:32:15.653428: step 7860, loss = 0.97 (2141.7 examples/sec; 0.060 sec/batch)\n",
      "2018-04-28 18:32:16.191798: step 7870, loss = 0.83 (2377.6 examples/sec; 0.054 sec/batch)\n",
      "2018-04-28 18:32:16.740026: step 7880, loss = 0.87 (2334.8 examples/sec; 0.055 sec/batch)\n",
      "2018-04-28 18:32:17.295361: step 7890, loss = 0.83 (2304.9 examples/sec; 0.056 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 16.9351\n",
      "2018-04-28 18:32:18.054906: step 7900, loss = 0.90 (1685.2 examples/sec; 0.076 sec/batch)\n",
      "2018-04-28 18:32:18.512482: step 7910, loss = 0.96 (2797.4 examples/sec; 0.046 sec/batch)\n",
      "2018-04-28 18:32:19.062045: step 7920, loss = 0.95 (2329.1 examples/sec; 0.055 sec/batch)\n",
      "2018-04-28 18:32:19.604625: step 7930, loss = 0.94 (2359.1 examples/sec; 0.054 sec/batch)\n",
      "2018-04-28 18:32:20.164554: step 7940, loss = 0.82 (2286.0 examples/sec; 0.056 sec/batch)\n",
      "2018-04-28 18:32:20.721172: step 7950, loss = 0.92 (2299.6 examples/sec; 0.056 sec/batch)\n",
      "2018-04-28 18:32:21.303352: step 7960, loss = 1.00 (2198.6 examples/sec; 0.058 sec/batch)\n",
      "2018-04-28 18:32:21.904220: step 7970, loss = 0.99 (2130.3 examples/sec; 0.060 sec/batch)\n",
      "2018-04-28 18:32:22.446121: step 7980, loss = 0.86 (2362.0 examples/sec; 0.054 sec/batch)\n",
      "2018-04-28 18:32:22.970824: step 7990, loss = 0.82 (2439.5 examples/sec; 0.052 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 17.8781\n",
      "2018-04-28 18:32:23.648275: step 8000, loss = 0.95 (1889.4 examples/sec; 0.068 sec/batch)\n",
      "2018-04-28 18:32:24.050148: step 8010, loss = 1.10 (3185.1 examples/sec; 0.040 sec/batch)\n",
      "2018-04-28 18:32:24.575006: step 8020, loss = 0.95 (2438.8 examples/sec; 0.052 sec/batch)\n",
      "2018-04-28 18:32:25.145247: step 8030, loss = 0.93 (2244.7 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:32:25.748128: step 8040, loss = 0.96 (2123.1 examples/sec; 0.060 sec/batch)\n",
      "2018-04-28 18:32:26.349036: step 8050, loss = 0.83 (2130.1 examples/sec; 0.060 sec/batch)\n",
      "2018-04-28 18:32:26.915932: step 8060, loss = 0.94 (2257.9 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:32:27.525509: step 8070, loss = 0.90 (2099.8 examples/sec; 0.061 sec/batch)\n",
      "2018-04-28 18:32:28.130929: step 8080, loss = 0.94 (2114.2 examples/sec; 0.061 sec/batch)\n",
      "2018-04-28 18:32:28.701987: step 8090, loss = 0.92 (2241.4 examples/sec; 0.057 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 17.1102\n",
      "2018-04-28 18:32:29.492861: step 8100, loss = 0.80 (1618.5 examples/sec; 0.079 sec/batch)\n",
      "2018-04-28 18:32:29.937701: step 8110, loss = 0.98 (2877.5 examples/sec; 0.044 sec/batch)\n",
      "2018-04-28 18:32:30.540867: step 8120, loss = 0.82 (2122.1 examples/sec; 0.060 sec/batch)\n",
      "2018-04-28 18:32:31.128200: step 8130, loss = 0.96 (2179.3 examples/sec; 0.059 sec/batch)\n",
      "2018-04-28 18:32:31.737714: step 8140, loss = 0.92 (2100.0 examples/sec; 0.061 sec/batch)\n",
      "2018-04-28 18:32:32.383049: step 8150, loss = 1.13 (1983.5 examples/sec; 0.065 sec/batch)\n",
      "2018-04-28 18:32:33.016762: step 8160, loss = 0.99 (2019.8 examples/sec; 0.063 sec/batch)\n",
      "2018-04-28 18:32:33.623183: step 8170, loss = 0.90 (2110.7 examples/sec; 0.061 sec/batch)\n",
      "2018-04-28 18:32:34.251174: step 8180, loss = 0.78 (2038.3 examples/sec; 0.063 sec/batch)\n",
      "2018-04-28 18:32:34.859908: step 8190, loss = 1.02 (2102.7 examples/sec; 0.061 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 16.3151\n",
      "2018-04-28 18:32:35.622379: step 8200, loss = 0.83 (1678.8 examples/sec; 0.076 sec/batch)\n",
      "2018-04-28 18:32:36.058477: step 8210, loss = 0.96 (2935.1 examples/sec; 0.044 sec/batch)\n",
      "2018-04-28 18:32:36.677614: step 8220, loss = 0.81 (2067.4 examples/sec; 0.062 sec/batch)\n",
      "2018-04-28 18:32:37.271000: step 8230, loss = 1.03 (2157.1 examples/sec; 0.059 sec/batch)\n",
      "2018-04-28 18:32:37.924574: step 8240, loss = 1.00 (1958.5 examples/sec; 0.065 sec/batch)\n",
      "2018-04-28 18:32:38.527352: step 8250, loss = 0.91 (2123.5 examples/sec; 0.060 sec/batch)\n",
      "2018-04-28 18:32:39.145191: step 8260, loss = 0.89 (2071.7 examples/sec; 0.062 sec/batch)\n",
      "2018-04-28 18:32:39.765984: step 8270, loss = 0.78 (2061.9 examples/sec; 0.062 sec/batch)\n",
      "2018-04-28 18:32:40.363481: step 8280, loss = 0.94 (2142.3 examples/sec; 0.060 sec/batch)\n",
      "2018-04-28 18:32:40.935354: step 8290, loss = 0.95 (2238.2 examples/sec; 0.057 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 16.5773\n",
      "2018-04-28 18:32:41.655093: step 8300, loss = 0.96 (1778.4 examples/sec; 0.072 sec/batch)\n",
      "2018-04-28 18:32:42.092657: step 8310, loss = 0.90 (2925.3 examples/sec; 0.044 sec/batch)\n",
      "2018-04-28 18:32:42.753302: step 8320, loss = 0.96 (1937.5 examples/sec; 0.066 sec/batch)\n",
      "2018-04-28 18:32:43.435765: step 8330, loss = 0.83 (1875.6 examples/sec; 0.068 sec/batch)\n",
      "2018-04-28 18:32:44.102325: step 8340, loss = 0.93 (1920.3 examples/sec; 0.067 sec/batch)\n",
      "2018-04-28 18:32:44.647615: step 8350, loss = 0.98 (2347.4 examples/sec; 0.055 sec/batch)\n",
      "2018-04-28 18:32:45.197062: step 8360, loss = 0.95 (2329.6 examples/sec; 0.055 sec/batch)\n",
      "2018-04-28 18:32:45.742623: step 8370, loss = 0.78 (2346.2 examples/sec; 0.055 sec/batch)\n",
      "2018-04-28 18:32:46.302209: step 8380, loss = 0.84 (2287.4 examples/sec; 0.056 sec/batch)\n",
      "2018-04-28 18:32:46.847320: step 8390, loss = 0.98 (2348.2 examples/sec; 0.055 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 16.5802\n",
      "2018-04-28 18:32:47.685781: step 8400, loss = 0.91 (1526.6 examples/sec; 0.084 sec/batch)\n",
      "2018-04-28 18:32:48.110155: step 8410, loss = 0.89 (3016.2 examples/sec; 0.042 sec/batch)\n",
      "2018-04-28 18:32:48.662030: step 8420, loss = 0.84 (2319.4 examples/sec; 0.055 sec/batch)\n",
      "2018-04-28 18:32:49.239011: step 8430, loss = 1.03 (2218.5 examples/sec; 0.058 sec/batch)\n",
      "2018-04-28 18:32:49.856971: step 8440, loss = 0.95 (2071.3 examples/sec; 0.062 sec/batch)\n",
      "2018-04-28 18:32:50.407059: step 8450, loss = 0.87 (2326.9 examples/sec; 0.055 sec/batch)\n",
      "2018-04-28 18:32:50.983715: step 8460, loss = 0.85 (2219.7 examples/sec; 0.058 sec/batch)\n",
      "2018-04-28 18:32:51.646341: step 8470, loss = 0.85 (1931.7 examples/sec; 0.066 sec/batch)\n",
      "2018-04-28 18:32:52.191680: step 8480, loss = 0.70 (2347.1 examples/sec; 0.055 sec/batch)\n",
      "2018-04-28 18:32:52.736949: step 8490, loss = 1.01 (2347.5 examples/sec; 0.055 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 16.9764\n",
      "2018-04-28 18:32:53.579423: step 8500, loss = 0.89 (1519.3 examples/sec; 0.084 sec/batch)\n",
      "2018-04-28 18:32:54.121503: step 8510, loss = 1.06 (2361.3 examples/sec; 0.054 sec/batch)\n",
      "2018-04-28 18:32:54.765445: step 8520, loss = 0.81 (1987.8 examples/sec; 0.064 sec/batch)\n",
      "2018-04-28 18:32:55.336302: step 8530, loss = 0.87 (2242.2 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:32:55.964431: step 8540, loss = 0.92 (2037.8 examples/sec; 0.063 sec/batch)\n",
      "2018-04-28 18:32:56.560391: step 8550, loss = 0.90 (2147.8 examples/sec; 0.060 sec/batch)\n",
      "2018-04-28 18:32:57.126267: step 8560, loss = 1.05 (2262.0 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:32:57.680997: step 8570, loss = 0.93 (2307.4 examples/sec; 0.055 sec/batch)\n",
      "2018-04-28 18:32:58.225499: step 8580, loss = 0.91 (2350.8 examples/sec; 0.054 sec/batch)\n",
      "2018-04-28 18:32:58.782266: step 8590, loss = 0.91 (2299.0 examples/sec; 0.056 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 16.9566\n",
      "2018-04-28 18:32:59.473715: step 8600, loss = 0.89 (1851.2 examples/sec; 0.069 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-28 18:32:59.887953: step 8610, loss = 0.90 (3090.0 examples/sec; 0.041 sec/batch)\n",
      "2018-04-28 18:33:00.425808: step 8620, loss = 0.98 (2379.8 examples/sec; 0.054 sec/batch)\n",
      "2018-04-28 18:33:00.955969: step 8630, loss = 0.91 (2414.3 examples/sec; 0.053 sec/batch)\n",
      "2018-04-28 18:33:01.487693: step 8640, loss = 0.98 (2407.3 examples/sec; 0.053 sec/batch)\n",
      "2018-04-28 18:33:02.021098: step 8650, loss = 1.03 (2399.7 examples/sec; 0.053 sec/batch)\n",
      "2018-04-28 18:33:02.544187: step 8660, loss = 0.86 (2447.0 examples/sec; 0.052 sec/batch)\n",
      "2018-04-28 18:33:03.091464: step 8670, loss = 0.82 (2338.9 examples/sec; 0.055 sec/batch)\n",
      "2018-04-28 18:33:03.664787: step 8680, loss = 0.87 (2232.6 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:33:04.194196: step 8690, loss = 1.07 (2417.8 examples/sec; 0.053 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 18.5428\n",
      "2018-04-28 18:33:04.866682: step 8700, loss = 1.07 (1903.4 examples/sec; 0.067 sec/batch)\n",
      "2018-04-28 18:33:05.286266: step 8710, loss = 1.11 (3050.7 examples/sec; 0.042 sec/batch)\n",
      "2018-04-28 18:33:05.858206: step 8720, loss = 0.86 (2238.0 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:33:06.429663: step 8730, loss = 0.87 (2239.9 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:33:06.996302: step 8740, loss = 0.93 (2258.9 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:33:07.581288: step 8750, loss = 0.98 (2188.1 examples/sec; 0.058 sec/batch)\n",
      "2018-04-28 18:33:08.159023: step 8760, loss = 0.95 (2215.6 examples/sec; 0.058 sec/batch)\n",
      "2018-04-28 18:33:08.732927: step 8770, loss = 1.08 (2230.3 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:33:09.421101: step 8780, loss = 0.87 (1860.0 examples/sec; 0.069 sec/batch)\n",
      "2018-04-28 18:33:10.056733: step 8790, loss = 0.78 (2013.7 examples/sec; 0.064 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 16.7076\n",
      "2018-04-28 18:33:10.852637: step 8800, loss = 0.99 (1608.2 examples/sec; 0.080 sec/batch)\n",
      "2018-04-28 18:33:11.390874: step 8810, loss = 0.84 (2378.1 examples/sec; 0.054 sec/batch)\n",
      "2018-04-28 18:33:11.966509: step 8820, loss = 0.81 (2223.6 examples/sec; 0.058 sec/batch)\n",
      "2018-04-28 18:33:12.593879: step 8830, loss = 0.89 (2040.3 examples/sec; 0.063 sec/batch)\n",
      "2018-04-28 18:33:13.252611: step 8840, loss = 0.88 (1943.1 examples/sec; 0.066 sec/batch)\n",
      "2018-04-28 18:33:13.823843: step 8850, loss = 0.99 (2240.8 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:33:14.390803: step 8860, loss = 0.82 (2257.7 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:33:14.967393: step 8870, loss = 0.82 (2219.9 examples/sec; 0.058 sec/batch)\n",
      "2018-04-28 18:33:15.535991: step 8880, loss = 0.83 (2251.2 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:33:16.104748: step 8890, loss = 0.84 (2250.5 examples/sec; 0.057 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 16.7111\n",
      "2018-04-28 18:33:16.836081: step 8900, loss = 0.90 (1750.2 examples/sec; 0.073 sec/batch)\n",
      "2018-04-28 18:33:17.291497: step 8910, loss = 0.93 (2810.7 examples/sec; 0.046 sec/batch)\n",
      "2018-04-28 18:33:17.856691: step 8920, loss = 0.95 (2264.7 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:33:18.408747: step 8930, loss = 1.08 (2318.6 examples/sec; 0.055 sec/batch)\n",
      "2018-04-28 18:33:18.959048: step 8940, loss = 0.79 (2326.0 examples/sec; 0.055 sec/batch)\n",
      "2018-04-28 18:33:19.518754: step 8950, loss = 0.85 (2286.9 examples/sec; 0.056 sec/batch)\n",
      "2018-04-28 18:33:20.080970: step 8960, loss = 1.11 (2276.7 examples/sec; 0.056 sec/batch)\n",
      "2018-04-28 18:33:20.635717: step 8970, loss = 0.97 (2307.4 examples/sec; 0.055 sec/batch)\n",
      "2018-04-28 18:33:21.180465: step 8980, loss = 0.97 (2349.7 examples/sec; 0.054 sec/batch)\n",
      "2018-04-28 18:33:21.723761: step 8990, loss = 0.86 (2356.0 examples/sec; 0.054 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 18.0104\n",
      "2018-04-28 18:33:22.388310: step 9000, loss = 0.87 (1926.1 examples/sec; 0.066 sec/batch)\n",
      "2018-04-28 18:33:22.790533: step 9010, loss = 1.04 (3182.3 examples/sec; 0.040 sec/batch)\n",
      "2018-04-28 18:33:23.315278: step 9020, loss = 0.84 (2439.3 examples/sec; 0.052 sec/batch)\n",
      "2018-04-28 18:33:23.834756: step 9030, loss = 0.87 (2464.0 examples/sec; 0.052 sec/batch)\n",
      "2018-04-28 18:33:24.358302: step 9040, loss = 0.72 (2444.9 examples/sec; 0.052 sec/batch)\n",
      "2018-04-28 18:33:24.876767: step 9050, loss = 0.74 (2468.8 examples/sec; 0.052 sec/batch)\n",
      "2018-04-28 18:33:25.433559: step 9060, loss = 0.71 (2298.9 examples/sec; 0.056 sec/batch)\n",
      "2018-04-28 18:33:26.040659: step 9070, loss = 0.92 (2108.4 examples/sec; 0.061 sec/batch)\n",
      "2018-04-28 18:33:26.613527: step 9080, loss = 0.90 (2234.4 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:33:27.191331: step 9090, loss = 0.96 (2215.3 examples/sec; 0.058 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 18.0464\n",
      "2018-04-28 18:33:27.929751: step 9100, loss = 1.10 (1733.4 examples/sec; 0.074 sec/batch)\n",
      "2018-04-28 18:33:28.369622: step 9110, loss = 0.87 (2910.0 examples/sec; 0.044 sec/batch)\n",
      "2018-04-28 18:33:28.941302: step 9120, loss = 0.82 (2239.0 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:33:29.519347: step 9130, loss = 0.96 (2214.4 examples/sec; 0.058 sec/batch)\n",
      "2018-04-28 18:33:30.098058: step 9140, loss = 0.81 (2211.9 examples/sec; 0.058 sec/batch)\n",
      "2018-04-28 18:33:30.655798: step 9150, loss = 1.01 (2294.8 examples/sec; 0.056 sec/batch)\n",
      "2018-04-28 18:33:31.216745: step 9160, loss = 0.91 (2281.9 examples/sec; 0.056 sec/batch)\n",
      "2018-04-28 18:33:31.767410: step 9170, loss = 0.92 (2324.5 examples/sec; 0.055 sec/batch)\n",
      "2018-04-28 18:33:32.321797: step 9180, loss = 0.98 (2308.9 examples/sec; 0.055 sec/batch)\n",
      "2018-04-28 18:33:32.873055: step 9190, loss = 0.83 (2322.0 examples/sec; 0.055 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 17.6773\n",
      "2018-04-28 18:33:33.586755: step 9200, loss = 1.00 (1793.5 examples/sec; 0.071 sec/batch)\n",
      "2018-04-28 18:33:34.038131: step 9210, loss = 0.86 (2835.8 examples/sec; 0.045 sec/batch)\n",
      "2018-04-28 18:33:34.648246: step 9220, loss = 0.86 (2098.0 examples/sec; 0.061 sec/batch)\n",
      "2018-04-28 18:33:35.283034: step 9230, loss = 0.92 (2016.4 examples/sec; 0.063 sec/batch)\n",
      "2018-04-28 18:33:35.838391: step 9240, loss = 0.83 (2304.8 examples/sec; 0.056 sec/batch)\n",
      "2018-04-28 18:33:36.434140: step 9250, loss = 0.82 (2148.6 examples/sec; 0.060 sec/batch)\n",
      "2018-04-28 18:33:37.002564: step 9260, loss = 0.97 (2251.8 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:33:37.544301: step 9270, loss = 0.93 (2362.8 examples/sec; 0.054 sec/batch)\n",
      "2018-04-28 18:33:38.077251: step 9280, loss = 0.99 (2401.7 examples/sec; 0.053 sec/batch)\n",
      "2018-04-28 18:33:38.594253: step 9290, loss = 0.91 (2475.8 examples/sec; 0.052 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 17.5969\n",
      "2018-04-28 18:33:39.269619: step 9300, loss = 0.79 (1895.3 examples/sec; 0.068 sec/batch)\n",
      "2018-04-28 18:33:39.716777: step 9310, loss = 0.90 (2862.5 examples/sec; 0.045 sec/batch)\n",
      "2018-04-28 18:33:40.289855: step 9320, loss = 0.77 (2233.6 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:33:40.860773: step 9330, loss = 0.88 (2242.0 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:33:41.430864: step 9340, loss = 1.00 (2245.3 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:33:42.008097: step 9350, loss = 0.97 (2217.5 examples/sec; 0.058 sec/batch)\n",
      "2018-04-28 18:33:42.589403: step 9360, loss = 0.83 (2201.9 examples/sec; 0.058 sec/batch)\n",
      "2018-04-28 18:33:43.157714: step 9370, loss = 0.84 (2252.3 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:33:43.727183: step 9380, loss = 0.83 (2247.7 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:33:44.286109: step 9390, loss = 0.91 (2290.1 examples/sec; 0.056 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 17.5316\n",
      "2018-04-28 18:33:44.973521: step 9400, loss = 0.85 (1862.0 examples/sec; 0.069 sec/batch)\n",
      "2018-04-28 18:33:45.411085: step 9410, loss = 0.90 (2925.3 examples/sec; 0.044 sec/batch)\n",
      "2018-04-28 18:33:45.972125: step 9420, loss = 0.89 (2281.5 examples/sec; 0.056 sec/batch)\n",
      "2018-04-28 18:33:46.530945: step 9430, loss = 0.83 (2290.5 examples/sec; 0.056 sec/batch)\n",
      "2018-04-28 18:33:47.165866: step 9440, loss = 0.79 (2016.0 examples/sec; 0.063 sec/batch)\n",
      "2018-04-28 18:33:47.819652: step 9450, loss = 0.83 (1957.8 examples/sec; 0.065 sec/batch)\n",
      "2018-04-28 18:33:48.401798: step 9460, loss = 0.97 (2198.8 examples/sec; 0.058 sec/batch)\n",
      "2018-04-28 18:33:48.952511: step 9470, loss = 0.88 (2324.2 examples/sec; 0.055 sec/batch)\n",
      "2018-04-28 18:33:49.520978: step 9480, loss = 0.81 (2251.7 examples/sec; 0.057 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-28 18:33:50.134198: step 9490, loss = 0.81 (2087.3 examples/sec; 0.061 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 16.9881\n",
      "2018-04-28 18:33:50.859934: step 9500, loss = 0.97 (1763.7 examples/sec; 0.073 sec/batch)\n",
      "2018-04-28 18:33:51.276703: step 9510, loss = 0.73 (3071.3 examples/sec; 0.042 sec/batch)\n",
      "2018-04-28 18:33:51.828483: step 9520, loss = 0.87 (2319.8 examples/sec; 0.055 sec/batch)\n",
      "2018-04-28 18:33:52.391381: step 9530, loss = 0.84 (2274.0 examples/sec; 0.056 sec/batch)\n",
      "2018-04-28 18:33:52.967313: step 9540, loss = 0.92 (2222.5 examples/sec; 0.058 sec/batch)\n",
      "2018-04-28 18:33:53.528599: step 9550, loss = 1.09 (2280.5 examples/sec; 0.056 sec/batch)\n",
      "2018-04-28 18:33:54.059802: step 9560, loss = 0.99 (2409.6 examples/sec; 0.053 sec/batch)\n",
      "2018-04-28 18:33:54.638070: step 9570, loss = 0.81 (2213.5 examples/sec; 0.058 sec/batch)\n",
      "2018-04-28 18:33:55.172598: step 9580, loss = 0.75 (2394.6 examples/sec; 0.053 sec/batch)\n",
      "2018-04-28 18:33:55.695503: step 9590, loss = 1.01 (2447.9 examples/sec; 0.052 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 17.8423\n",
      "2018-04-28 18:33:56.464544: step 9600, loss = 0.85 (1664.4 examples/sec; 0.077 sec/batch)\n",
      "2018-04-28 18:33:56.868766: step 9610, loss = 0.96 (3166.6 examples/sec; 0.040 sec/batch)\n",
      "2018-04-28 18:33:57.399535: step 9620, loss = 0.73 (2411.6 examples/sec; 0.053 sec/batch)\n",
      "2018-04-28 18:33:57.928924: step 9630, loss = 0.95 (2417.9 examples/sec; 0.053 sec/batch)\n",
      "2018-04-28 18:33:58.457856: step 9640, loss = 0.87 (2420.0 examples/sec; 0.053 sec/batch)\n",
      "2018-04-28 18:33:58.997371: step 9650, loss = 0.96 (2372.5 examples/sec; 0.054 sec/batch)\n",
      "2018-04-28 18:33:59.536396: step 9660, loss = 0.86 (2374.6 examples/sec; 0.054 sec/batch)\n",
      "2018-04-28 18:34:00.077441: step 9670, loss = 1.02 (2365.8 examples/sec; 0.054 sec/batch)\n",
      "2018-04-28 18:34:00.618583: step 9680, loss = 1.04 (2365.4 examples/sec; 0.054 sec/batch)\n",
      "2018-04-28 18:34:01.166160: step 9690, loss = 0.85 (2337.6 examples/sec; 0.055 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 18.4219\n",
      "2018-04-28 18:34:01.892835: step 9700, loss = 0.76 (1761.4 examples/sec; 0.073 sec/batch)\n",
      "2018-04-28 18:34:02.325434: step 9710, loss = 1.09 (2958.9 examples/sec; 0.043 sec/batch)\n",
      "2018-04-28 18:34:02.868999: step 9720, loss = 0.89 (2354.8 examples/sec; 0.054 sec/batch)\n",
      "2018-04-28 18:34:03.416412: step 9730, loss = 0.99 (2338.3 examples/sec; 0.055 sec/batch)\n",
      "2018-04-28 18:34:03.944123: step 9740, loss = 0.84 (2425.6 examples/sec; 0.053 sec/batch)\n",
      "2018-04-28 18:34:04.547187: step 9750, loss = 0.91 (2122.5 examples/sec; 0.060 sec/batch)\n",
      "2018-04-28 18:34:05.123272: step 9760, loss = 0.81 (2221.9 examples/sec; 0.058 sec/batch)\n",
      "2018-04-28 18:34:05.651793: step 9770, loss = 0.72 (2421.8 examples/sec; 0.053 sec/batch)\n",
      "2018-04-28 18:34:06.175018: step 9780, loss = 0.82 (2446.4 examples/sec; 0.052 sec/batch)\n",
      "2018-04-28 18:34:06.724993: step 9790, loss = 0.77 (2327.4 examples/sec; 0.055 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 18.1088\n",
      "2018-04-28 18:34:07.415400: step 9800, loss = 0.88 (1854.0 examples/sec; 0.069 sec/batch)\n",
      "2018-04-28 18:34:07.833450: step 9810, loss = 0.91 (3061.8 examples/sec; 0.042 sec/batch)\n",
      "2018-04-28 18:34:08.367211: step 9820, loss = 0.85 (2398.1 examples/sec; 0.053 sec/batch)\n",
      "2018-04-28 18:34:08.905323: step 9830, loss = 0.86 (2378.7 examples/sec; 0.054 sec/batch)\n",
      "2018-04-28 18:34:09.466471: step 9840, loss = 0.83 (2281.1 examples/sec; 0.056 sec/batch)\n",
      "2018-04-28 18:34:10.043709: step 9850, loss = 0.97 (2217.5 examples/sec; 0.058 sec/batch)\n",
      "2018-04-28 18:34:10.688468: step 9860, loss = 1.00 (1985.2 examples/sec; 0.064 sec/batch)\n",
      "2018-04-28 18:34:11.308274: step 9870, loss = 0.81 (2065.2 examples/sec; 0.062 sec/batch)\n",
      "2018-04-28 18:34:11.899130: step 9880, loss = 0.87 (2166.4 examples/sec; 0.059 sec/batch)\n",
      "2018-04-28 18:34:12.493349: step 9890, loss = 0.82 (2154.1 examples/sec; 0.059 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 17.2246\n",
      "2018-04-28 18:34:13.222226: step 9900, loss = 0.89 (1756.1 examples/sec; 0.073 sec/batch)\n",
      "2018-04-28 18:34:13.676968: step 9910, loss = 0.93 (2814.8 examples/sec; 0.045 sec/batch)\n",
      "2018-04-28 18:34:14.243217: step 9920, loss = 0.89 (2260.5 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:34:14.809119: step 9930, loss = 0.83 (2261.9 examples/sec; 0.057 sec/batch)\n",
      "2018-04-28 18:34:15.410576: step 9940, loss = 0.98 (2128.1 examples/sec; 0.060 sec/batch)\n",
      "2018-04-28 18:34:15.965824: step 9950, loss = 0.77 (2305.3 examples/sec; 0.056 sec/batch)\n",
      "2018-04-28 18:34:16.546645: step 9960, loss = 0.84 (2203.8 examples/sec; 0.058 sec/batch)\n",
      "2018-04-28 18:34:17.090385: step 9970, loss = 0.90 (2354.1 examples/sec; 0.054 sec/batch)\n",
      "2018-04-28 18:34:17.649713: step 9980, loss = 1.09 (2288.5 examples/sec; 0.056 sec/batch)\n",
      "2018-04-28 18:34:18.206312: step 9990, loss = 0.97 (2299.7 examples/sec; 0.056 sec/batch)\n",
      "INFO:tensorflow:Saving checkpoints for 10000 into /tmp/cifar10_train/model.ckpt.\n"
     ]
    }
   ],
   "source": [
    "train_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
