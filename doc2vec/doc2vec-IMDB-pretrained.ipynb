{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gensim Doc2vec Tutorial on the IMDB Sentiment Dataset\n",
    "This notebook is modified based on the original one provided by Gensim.  \n",
    "You can find it here: https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-IMDB.ipynb  \n",
    "\n",
    "The major difference is, this notebook use **different text preprocessing methods as well as pretrained vectors** compared to the original one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's download the IMDB archive if it is not already downloaded (84 MB). This will be our text data for this tutorial.   \n",
    "The data can be found here: http://ai.stanford.edu/~amaas/data/sentiment/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import locale\n",
    "import glob\n",
    "import os.path\n",
    "import requests\n",
    "import tarfile\n",
    "import sys\n",
    "import codecs\n",
    "import smart_open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirname = 'data/aclImdb'\n",
    "filename = 'data/aclImdb_v1.tar.gz'\n",
    "locale.setlocale(locale.LC_ALL, 'C')\n",
    "\n",
    "if sys.version > '3':\n",
    "    control_chars = [chr(0x85)]\n",
    "else:\n",
    "    control_chars = [unichr(0x85)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convert text to lower-case and strip punctuation/symbols from words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def pos_tag_map(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return 'a'\n",
    "    elif tag.startswith('V'):\n",
    "        return 'v'\n",
    "    elif tag.startswith('N'):\n",
    "        return 'n'\n",
    "    elif tag.startswith('R'):\n",
    "        return 'r'\n",
    "    else:\n",
    "        return 'n'\n",
    "\n",
    "def normalize_text(text):\n",
    "    # 1. remove all html tags\n",
    "    only_text = BeautifulSoup(text, 'lxml').get_text()\n",
    "    \n",
    "    # 2. only keep english characters\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", only_text)\n",
    "    \n",
    "    # 3. Turn all words into their lowercase and split for use\n",
    "    words = letters_only.lower().split()\n",
    "    words = [w.strip() for w in words]\n",
    "    \n",
    "    # 4. Lemmatizing\n",
    "    word_pos = nltk.pos_tag(words)\n",
    "    clean_words = [wordnet_lemmatizer.lemmatize(w, pos_tag_map(pos)) for (w, pos) in word_pos]\n",
    "    \n",
    "    norm_text = ' '.join(clean_words)\n",
    "    return norm_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning up dataset...\n",
      "('Total running time: ', 5346.248143999999)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.clock()\n",
    "\n",
    "if not os.path.isfile('data/aclImdb/alldata-id.txt'):\n",
    "    if not os.path.isdir(dirname):\n",
    "        if not os.path.isfile(filename):\n",
    "            # Download IMDB archive\n",
    "            print(\"Downloading IMDB archive...\")\n",
    "            url = u'http://ai.stanford.edu/~amaas/data/sentiment/' + filename\n",
    "            r = requests.get(url)\n",
    "            with open(filename, 'wb') as f:\n",
    "                f.write(r.content)\n",
    "        tar = tarfile.open(filename, mode='r')\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "    # Concatenate and normalize test/train data\n",
    "    print(\"Cleaning up dataset...\")\n",
    "    folders = ['train/pos', 'train/neg', 'test/pos', 'test/neg', 'train/unsup']\n",
    "    alldata = u''\n",
    "    for fol in folders:\n",
    "        temp = u''\n",
    "        # get all txt files inside the folder\n",
    "        output = fol.replace('/', '-') + '.txt'\n",
    "        txt_files = glob.glob(os.path.join(dirname, fol, '*.txt'))\n",
    "        for txt in txt_files:\n",
    "            # each txt file represent one review\n",
    "            with smart_open.smart_open(txt, \"rb\") as t:\n",
    "                t_clean = t.read().decode(\"utf-8\")\n",
    "                for c in control_chars:\n",
    "                    t_clean = t_clean.replace(c, ' ')\n",
    "                temp += normalize_text(t_clean)\n",
    "            temp += \"\\n\"\n",
    "        with smart_open.smart_open(os.path.join(dirname, output), \"wb\") as n:\n",
    "            n.write(temp.encode(\"utf-8\"))\n",
    "        alldata += temp\n",
    "\n",
    "    with smart_open.smart_open(os.path.join(dirname, 'alldata-id.txt'), 'wb') as f:\n",
    "        for idx, line in enumerate(alldata.splitlines()):\n",
    "            num_line = u\"_*{0} {1}\\n\".format(idx, line)\n",
    "            f.write(num_line.encode(\"utf-8\"))\n",
    "\n",
    "end = time.clock()\n",
    "print (\"Total running time: \", end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "assert os.path.isfile(\"data/aclImdb/alldata-id.txt\"), \"alldata-id.txt unavailable\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text data is small enough to be read into memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SentimentDocument = namedtuple('SentimentDocument', 'words tags split sentiment')\n",
    "\n",
    "alldocs = []  # Will hold all docs in original order\n",
    "with open('data/aclImdb/alldata-id.txt') as alldata:\n",
    "    for line_no, line in enumerate(alldata):\n",
    "        tokens = gensim.utils.to_unicode(line).split()\n",
    "        words = tokens[1:]\n",
    "        tags = [line_no] # 'tags = [tokens[0]]' would also work at extra memory cost\n",
    "        split = ['train', 'test', 'extra', 'extra'][line_no//25000]  # 25k train, 25k test, 25k extra\n",
    "        sentiment = [1.0, 0.0, 1.0, 0.0, None, None, None, None][line_no//12500] # [12.5K pos, 12.5K neg]*2 then unknown\n",
    "        alldocs.append(SentimentDocument(words, tags, split, sentiment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000 docs: 25000 train-sentiment, 25000 test-sentiment\n"
     ]
    }
   ],
   "source": [
    "train_docs = [doc for doc in alldocs if doc.split == 'train']\n",
    "test_docs = [doc for doc in alldocs if doc.split == 'test']\n",
    "doc_list = alldocs[:]  # For reshuffling per pass\n",
    "print('%d docs: %d train-sentiment, %d test-sentiment' % (len(doc_list), len(train_docs), len(test_docs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up Doc2Vec Training & Evaluation Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We approximate the experiment of Le & Mikolov [\"Distributed Representations of Sentences and Documents\"](http://cs.stanford.edu/~quocle/paragraph_vector.pdf) with guidance from Mikolov's [example go.sh](https://groups.google.com/d/msg/word2vec-toolkit/Q49FIrNOQRo/J6KG8mUj45sJ):\n",
    "\n",
    "`./word2vec -train ../alldata-id.txt -output vectors.txt -cbow 0 -size 100 -window 10 -negative 5 -hs 0 -sample 1e-4 -threads 40 -binary 0 -iter 20 -min-count 1 -sentence-vectors 1`\n",
    "\n",
    "We vary the following parameter choices:\n",
    "* **100-dimensional** vectors, as the 400-d vectors of the paper don't seem to offer much benefit on this task\n",
    "* Similarly, **frequent word subsampling** seems to decrease sentiment-prediction accuracy, so it's left out\n",
    "* `cbow=0` means skip-gram which is equivalent to the paper's 'PV-DBOW' mode, matched in gensim with `dm=0`\n",
    "* Added to that DBOW model are two DM models, one which averages context vectors (`dm_mean`) and one which concatenates them (`dm_concat`, resulting in a much larger, slower, more data-hungry model)\n",
    "* A `min_count=2` saves quite a bit of model memory, discarding only words that appear in a single doc (and are thus no more expressive than the unique-to-each doc vectors themselves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "import gensim.models.doc2vec\n",
    "from collections import OrderedDict\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cores = multiprocessing.cpu_count()\n",
    "assert gensim.models.doc2vec.FAST_VERSION > -1, \"This will be painfully slow otherwise\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test 3 simple models here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_models = [\n",
    "    # PV-DM w/ concatenation - window=5 (both sides) approximates paper's 10-word total window size\n",
    "    Doc2Vec(dm=1, dm_concat=1, size=300, window=5, negative=5, hs=0, min_count=2, workers=cores),\n",
    "    # PV-DBOW \n",
    "    Doc2Vec(dm=0, size=300, negative=5, hs=0, min_count=2, workers=cores),\n",
    "    # PV-DM w/ average\n",
    "    Doc2Vec(dm=1, dm_mean=1, size=300, window=10, negative=5, hs=0, min_count=2, workers=cores),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speed up setup by sharing results of the 1st model's vocabulary scan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use pretrained word vectors here\n",
    "Use the built-in method: **intersect_word2vec_format**  \n",
    "However, you will see only part of the vocabulary can find their corresponding word vectors in the pretrained set.  \n",
    "For the unknown words, vectors are randomlt initialized as before. In default, method provided by Gensim would not update the pretrained word vectors but the randomly intialized ones.  \n",
    "\n",
    "**Notes**\n",
    "1. The original experiment used 100 dimensional word vectors. Since we want to use the pretrained word vectors from Google, our model needs to conform to choose 300 dimensional word vectors.\n",
    "2. We should also notice that, if the pretrained vectors help, they will only help the dm model, because dbow model doesn't need to utilize the word vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.92 s, sys: 96 ms, total: 6.02 s\n",
      "Wall time: 5.95 s\n",
      "CPU times: user 2min 56s, sys: 1.88 s, total: 2min 58s\n",
      "Wall time: 7min 16s\n",
      "Doc2Vec(dm/c,d300,n5,w5,mc2,s0.001,t8)\n",
      "Doc2Vec(dbow,d300,n5,mc2,s0.001,t8)\n",
      "Doc2Vec(dm/m,d300,n5,w10,mc2,s0.001,t8)\n"
     ]
    }
   ],
   "source": [
    "# PV-DM w/ concat requires one special NULL word so it serves as template\n",
    "%time simple_models[0].build_vocab(alldocs)\n",
    "%time simple_models[0].intersect_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "print(simple_models[0])\n",
    "\n",
    "for model in simple_models[1:]:\n",
    "    model.reset_from(simple_models[0])\n",
    "    model.intersect_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "    print(model)\n",
    "\n",
    "models_by_name = OrderedDict((str(model), model) for model in simple_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le and Mikolov notes that **combining a paragraph vector from Distributed Bag of Words (DBOW) and Distributed Memory (DM) improves performance**.  \n",
    "We will follow, pairing the models together for evaluation. Here, **we concatenate the paragraph vectors obtained from each model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.test.test_doc2vec import ConcatenatedDoc2Vec\n",
    "\n",
    "models_by_name['dbow+dmm'] = ConcatenatedDoc2Vec([simple_models[1], simple_models[2]])\n",
    "models_by_name['dbow+dmc'] = ConcatenatedDoc2Vec([simple_models[1], simple_models[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('Doc2Vec(dm/c,d300,n5,w5,mc2,s0.001,t8)',\n",
       "              <gensim.models.doc2vec.Doc2Vec at 0x7f878e1ebf90>),\n",
       "             ('Doc2Vec(dbow,d300,n5,mc2,s0.001,t8)',\n",
       "              <gensim.models.doc2vec.Doc2Vec at 0x7f878e1e0090>),\n",
       "             ('Doc2Vec(dm/m,d300,n5,w10,mc2,s0.001,t8)',\n",
       "              <gensim.models.doc2vec.Doc2Vec at 0x7f878e1e0150>),\n",
       "             ('dbow+dmm',\n",
       "              <gensim.test.test_doc2vec.ConcatenatedDoc2Vec at 0x7f878e1ebc50>),\n",
       "             ('dbow+dmc',\n",
       "              <gensim.test.test_doc2vec.ConcatenatedDoc2Vec at 0x7f87741a2b50>)])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_by_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictive Evaluation Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define some helper methods for evaluating the performance of our Doc2vec using paragraph vectors.  \n",
    "We will classify document sentiments using a **logistic regression** model based on our paragraph embeddings.  \n",
    "We will compare the error rates based on word embeddings from our various Doc2vec models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saucecat/anaconda2/lib/python2.7/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from random import sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import contextmanager\n",
    "from timeit import default_timer\n",
    "import time \n",
    "\n",
    "@contextmanager\n",
    "def elapsed_timer():\n",
    "    start = default_timer()\n",
    "    elapser = lambda: default_timer() - start\n",
    "    yield lambda: elapser()\n",
    "    end = default_timer()\n",
    "    elapser = lambda: end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_predictor_from_data(train_targets, train_regressors):\n",
    "    logit = sm.Logit(train_targets, train_regressors)\n",
    "    predictor = logit.fit(disp=0)\n",
    "    #print(predictor.summary())\n",
    "    return predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_rate_for_model(test_model, train_set, test_set, infer=False, infer_steps=3, infer_alpha=0.1, infer_subsample=0.1):\n",
    "    \"\"\"Report error rate on test_doc sentiments, using supplied model and train_docs\"\"\"\n",
    "\n",
    "    train_targets, train_regressors = zip(*[(doc.sentiment, test_model.docvecs[doc.tags[0]]) for doc in train_set])\n",
    "    train_regressors = sm.add_constant(train_regressors)\n",
    "    predictor = logistic_predictor_from_data(train_targets, train_regressors)\n",
    "\n",
    "    test_data = test_set\n",
    "    if infer:\n",
    "        if infer_subsample < 1.0:\n",
    "            test_data = sample(test_data, int(infer_subsample * len(test_data)))\n",
    "        test_regressors = [test_model.infer_vector(doc.words, steps=infer_steps, alpha=infer_alpha) for doc in test_data]\n",
    "    else:\n",
    "        test_regressors = [test_model.docvecs[doc.tags[0]] for doc in test_docs]\n",
    "    test_regressors = sm.add_constant(test_regressors)\n",
    "    \n",
    "    # Predict & evaluate\n",
    "    test_predictions = predictor.predict(test_regressors)\n",
    "    corrects = sum(np.rint(test_predictions) == [doc.sentiment for doc in test_data])\n",
    "    errors = len(test_predictions) - corrects\n",
    "    error_rate = float(errors) / len(test_predictions)\n",
    "    return (error_rate, errors, len(test_predictions), predictor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bulk Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use an explicit multiple-pass, alpha-reduction approach as sketched in this [gensim doc2vec blog post](http://radimrehurek.com/2014/12/doc2vec-tutorial/) with added shuffling of corpus on each pass.  \n",
    "Note that vector training is occurring on **all** documents of the dataset, which includes all **TRAIN/TEST/DEV** docs.  \n",
    "We evaluate each model's sentiment predictive power based on error rate, and **the evaluation is repeated after each pass** so we can see the rates of relative improvement. The base numbers reuse the TRAIN and TEST vectors stored in the models for the logistic regression, while the _inferred_ results use newly-inferred TEST vectors. \n",
    "\n",
    "**(On a 4-core 2.6Ghz Intel Core i7, these 20 passes training and evaluating 3 main models takes about an hour.)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "best_error = defaultdict(lambda: 1.0)  # To selectively print only best errors achieved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START 2018-01-10 15:23:23.509035\n",
      "*0.319200 : 1 passes : Doc2Vec(dm/c,d300,n5,w5,mc2,s0.001,t8) 85.4s 2.1s\n",
      "*0.306000 : 1 passes : Doc2Vec(dm/c,d300,n5,w5,mc2,s0.001,t8)_inferred 85.4s 17.5s\n",
      "*0.244800 : 1 passes : Doc2Vec(dbow,d300,n5,mc2,s0.001,t8) 15.8s 2.4s\n",
      "*0.222000 : 1 passes : Doc2Vec(dbow,d300,n5,mc2,s0.001,t8)_inferred 15.8s 5.6s\n",
      "*0.199400 : 1 passes : Doc2Vec(dm/m,d300,n5,w10,mc2,s0.001,t8) 19.7s 2.2s\n",
      "*0.182000 : 1 passes : Doc2Vec(dm/m,d300,n5,w10,mc2,s0.001,t8)_inferred 19.7s 5.6s\n",
      "*0.192480 : 1 passes : dbow+dmm 0.0s 7.1s\n",
      "*0.188000 : 1 passes : dbow+dmm_inferred 0.0s 13.1s\n",
      "*0.231120 : 1 passes : dbow+dmc 0.0s 6.8s\n",
      "*0.233200 : 1 passes : dbow+dmc_inferred 0.0s 23.6s\n",
      "Completed pass 1 at alpha 0.025000\n",
      "*0.259800 : 2 passes : Doc2Vec(dm/c,d300,n5,w5,mc2,s0.001,t8) 81.7s 2.2s\n",
      "*0.147480 : 2 passes : Doc2Vec(dbow,d300,n5,mc2,s0.001,t8) 15.2s 2.5s\n",
      "*0.163640 : 2 passes : Doc2Vec(dm/m,d300,n5,w10,mc2,s0.001,t8) 19.6s 2.6s\n",
      "*0.139480 : 2 passes : dbow+dmm 0.0s 9.1s\n",
      "*0.149240 : 2 passes : dbow+dmc 0.0s 8.8s\n",
      "Completed pass 2 at alpha 0.023800\n",
      "*0.226080 : 3 passes : Doc2Vec(dm/c,d300,n5,w5,mc2,s0.001,t8) 83.0s 2.3s\n",
      "*0.132000 : 3 passes : Doc2Vec(dbow,d300,n5,mc2,s0.001,t8) 15.3s 2.6s\n",
      "*0.149480 : 3 passes : Doc2Vec(dm/m,d300,n5,w10,mc2,s0.001,t8) 18.8s 2.3s\n",
      "*0.127640 : 3 passes : dbow+dmm 0.0s 7.6s\n",
      "*0.132400 : 3 passes : dbow+dmc 0.0s 7.5s\n",
      "Completed pass 3 at alpha 0.022600\n",
      "*0.207320 : 4 passes : Doc2Vec(dm/c,d300,n5,w5,mc2,s0.001,t8) 81.4s 2.5s\n",
      "*0.123160 : 4 passes : Doc2Vec(dbow,d300,n5,mc2,s0.001,t8) 14.6s 2.6s\n",
      "*0.139800 : 4 passes : Doc2Vec(dm/m,d300,n5,w10,mc2,s0.001,t8) 18.8s 2.2s\n",
      "*0.121880 : 4 passes : dbow+dmm 0.0s 8.6s\n",
      "*0.124920 : 4 passes : dbow+dmc 0.0s 8.9s\n",
      "Completed pass 4 at alpha 0.021400\n",
      "*0.196200 : 5 passes : Doc2Vec(dm/c,d300,n5,w5,mc2,s0.001,t8) 81.9s 3.2s\n",
      "*0.250800 : 5 passes : Doc2Vec(dm/c,d300,n5,w5,mc2,s0.001,t8)_inferred 81.9s 16.9s\n",
      "*0.118000 : 5 passes : Doc2Vec(dbow,d300,n5,mc2,s0.001,t8) 14.8s 2.8s\n",
      "*0.157600 : 5 passes : Doc2Vec(dbow,d300,n5,mc2,s0.001,t8)_inferred 14.8s 5.1s\n",
      "*0.135680 : 5 passes : Doc2Vec(dm/m,d300,n5,w10,mc2,s0.001,t8) 18.4s 2.6s\n",
      "*0.140800 : 5 passes : Doc2Vec(dm/m,d300,n5,w10,mc2,s0.001,t8)_inferred 18.4s 6.1s\n",
      "*0.116240 : 5 passes : dbow+dmm 0.0s 8.4s\n",
      "*0.150000 : 5 passes : dbow+dmm_inferred 0.0s 15.8s\n",
      "*0.119560 : 5 passes : dbow+dmc 0.0s 7.9s\n",
      "*0.183200 : 5 passes : dbow+dmc_inferred 0.0s 25.7s\n",
      "Completed pass 5 at alpha 0.020200\n",
      "*0.183720 : 6 passes : Doc2Vec(dm/c,d300,n5,w5,mc2,s0.001,t8) 79.8s 2.2s\n",
      "*0.115720 : 6 passes : Doc2Vec(dbow,d300,n5,mc2,s0.001,t8) 14.6s 2.4s\n",
      "*0.132640 : 6 passes : Doc2Vec(dm/m,d300,n5,w10,mc2,s0.001,t8) 18.4s 2.2s\n",
      "*0.113680 : 6 passes : dbow+dmm 0.0s 7.1s\n",
      "*0.117960 : 6 passes : dbow+dmc 0.0s 7.4s\n",
      "Completed pass 6 at alpha 0.019000\n",
      "*0.176120 : 7 passes : Doc2Vec(dm/c,d300,n5,w5,mc2,s0.001,t8) 77.8s 2.5s\n",
      "*0.114480 : 7 passes : Doc2Vec(dbow,d300,n5,mc2,s0.001,t8) 15.4s 2.3s\n",
      "*0.129240 : 7 passes : Doc2Vec(dm/m,d300,n5,w10,mc2,s0.001,t8) 18.4s 2.3s\n",
      "*0.112600 : 7 passes : dbow+dmm 0.0s 7.1s\n",
      "*0.115600 : 7 passes : dbow+dmc 0.0s 7.1s\n",
      "Completed pass 7 at alpha 0.017800\n",
      "*0.171480 : 8 passes : Doc2Vec(dm/c,d300,n5,w5,mc2,s0.001,t8) 80.6s 2.3s\n",
      "*0.113360 : 8 passes : Doc2Vec(dbow,d300,n5,mc2,s0.001,t8) 14.6s 2.3s\n",
      "*0.127560 : 8 passes : Doc2Vec(dm/m,d300,n5,w10,mc2,s0.001,t8) 19.0s 2.6s\n",
      "*0.110720 : 8 passes : dbow+dmm 0.0s 7.1s\n",
      "*0.113520 : 8 passes : dbow+dmc 0.0s 7.2s\n",
      "Completed pass 8 at alpha 0.016600\n",
      "*0.167840 : 9 passes : Doc2Vec(dm/c,d300,n5,w5,mc2,s0.001,t8) 78.0s 2.3s\n",
      "*0.111120 : 9 passes : Doc2Vec(dbow,d300,n5,mc2,s0.001,t8) 15.7s 3.2s\n",
      "*0.126040 : 9 passes : Doc2Vec(dm/m,d300,n5,w10,mc2,s0.001,t8) 18.1s 2.3s\n",
      "*0.109520 : 9 passes : dbow+dmm 0.0s 7.3s\n",
      "*0.113440 : 9 passes : dbow+dmc 0.0s 7.6s\n",
      "Completed pass 9 at alpha 0.015400\n",
      "*0.163560 : 10 passes : Doc2Vec(dm/c,d300,n5,w5,mc2,s0.001,t8) 76.6s 2.2s\n",
      "*0.205600 : 10 passes : Doc2Vec(dm/c,d300,n5,w5,mc2,s0.001,t8)_inferred 76.6s 16.9s\n",
      "*0.111080 : 10 passes : Doc2Vec(dbow,d300,n5,mc2,s0.001,t8) 15.0s 2.4s\n",
      "*0.142000 : 10 passes : Doc2Vec(dbow,d300,n5,mc2,s0.001,t8)_inferred 15.0s 5.2s\n",
      "*0.124440 : 10 passes : Doc2Vec(dm/m,d300,n5,w10,mc2,s0.001,t8) 17.9s 2.3s\n",
      " 0.147600 : 10 passes : Doc2Vec(dm/m,d300,n5,w10,mc2,s0.001,t8)_inferred 17.9s 6.1s\n",
      " 0.110560 : 10 passes : dbow+dmm 0.0s 7.1s\n",
      "*0.133200 : 10 passes : dbow+dmm_inferred 0.0s 13.2s\n",
      "*0.113320 : 10 passes : dbow+dmc 0.0s 6.8s\n",
      "*0.159200 : 10 passes : dbow+dmc_inferred 0.0s 23.6s\n",
      "Completed pass 10 at alpha 0.014200\n",
      "*0.163240 : 11 passes : Doc2Vec(dm/c,d300,n5,w5,mc2,s0.001,t8) 74.8s 2.2s\n",
      " 0.112520 : 11 passes : Doc2Vec(dbow,d300,n5,mc2,s0.001,t8) 14.3s 2.8s\n",
      "*0.122520 : 11 passes : Doc2Vec(dm/m,d300,n5,w10,mc2,s0.001,t8) 18.3s 2.6s\n",
      " 0.110080 : 11 passes : dbow+dmm 0.0s 7.0s\n",
      " 0.113960 : 11 passes : dbow+dmc 0.0s 7.1s\n",
      "Completed pass 11 at alpha 0.013000\n",
      "*0.159280 : 12 passes : Doc2Vec(dm/c,d300,n5,w5,mc2,s0.001,t8) 81.0s 3.2s\n",
      "*0.110600 : 12 passes : Doc2Vec(dbow,d300,n5,mc2,s0.001,t8) 14.2s 2.3s\n",
      "*0.121880 : 12 passes : Doc2Vec(dm/m,d300,n5,w10,mc2,s0.001,t8) 18.0s 2.4s\n",
      " 0.109720 : 12 passes : dbow+dmm 0.0s 6.9s\n",
      " 0.114000 : 12 passes : dbow+dmc 0.0s 7.2s\n",
      "Completed pass 12 at alpha 0.011800\n",
      "*0.157720 : 13 passes : Doc2Vec(dm/c,d300,n5,w5,mc2,s0.001,t8) 73.8s 2.9s\n",
      " 0.111360 : 13 passes : Doc2Vec(dbow,d300,n5,mc2,s0.001,t8) 16.0s 2.8s\n",
      "*0.119680 : 13 passes : Doc2Vec(dm/m,d300,n5,w10,mc2,s0.001,t8) 18.9s 2.4s\n",
      " 0.110320 : 13 passes : dbow+dmm 0.0s 7.4s\n",
      " 0.114840 : 13 passes : dbow+dmc 0.0s 9.4s\n",
      "Completed pass 13 at alpha 0.010600\n",
      "*0.156560 : 14 passes : Doc2Vec(dm/c,d300,n5,w5,mc2,s0.001,t8) 75.9s 3.0s\n",
      " 0.110720 : 14 passes : Doc2Vec(dbow,d300,n5,mc2,s0.001,t8) 15.7s 3.0s\n",
      "*0.119240 : 14 passes : Doc2Vec(dm/m,d300,n5,w10,mc2,s0.001,t8) 19.7s 2.6s\n",
      " 0.110680 : 14 passes : dbow+dmm 0.0s 6.9s\n",
      " 0.114000 : 14 passes : dbow+dmc 0.0s 6.9s\n",
      "Completed pass 14 at alpha 0.009400\n",
      "*0.155480 : 15 passes : Doc2Vec(dm/c,d300,n5,w5,mc2,s0.001,t8) 72.6s 2.2s\n",
      "*0.195200 : 15 passes : Doc2Vec(dm/c,d300,n5,w5,mc2,s0.001,t8)_inferred 72.6s 15.9s\n",
      " 0.110880 : 15 passes : Doc2Vec(dbow,d300,n5,mc2,s0.001,t8) 15.1s 2.9s\n",
      "*0.136400 : 15 passes : Doc2Vec(dbow,d300,n5,mc2,s0.001,t8)_inferred 15.1s 5.3s\n",
      "*0.119080 : 15 passes : Doc2Vec(dm/m,d300,n5,w10,mc2,s0.001,t8) 18.8s 2.6s\n",
      "*0.129200 : 15 passes : Doc2Vec(dm/m,d300,n5,w10,mc2,s0.001,t8)_inferred 18.8s 5.9s\n",
      " 0.111520 : 15 passes : dbow+dmm 0.0s 7.0s\n",
      " 0.133600 : 15 passes : dbow+dmm_inferred 0.0s 13.5s\n",
      "*0.113240 : 15 passes : dbow+dmc 0.0s 7.2s\n",
      "*0.146000 : 15 passes : dbow+dmc_inferred 0.0s 23.5s\n",
      "Completed pass 15 at alpha 0.008200\n",
      "*0.154200 : 16 passes : Doc2Vec(dm/c,d300,n5,w5,mc2,s0.001,t8) 71.1s 2.2s\n",
      " 0.112000 : 16 passes : Doc2Vec(dbow,d300,n5,mc2,s0.001,t8) 14.3s 2.4s\n",
      "*0.118880 : 16 passes : Doc2Vec(dm/m,d300,n5,w10,mc2,s0.001,t8) 18.3s 2.3s\n",
      " 0.111640 : 16 passes : dbow+dmm 0.0s 7.0s\n",
      " 0.114440 : 16 passes : dbow+dmc 0.0s 7.0s\n",
      "Completed pass 16 at alpha 0.007000\n",
      "*0.153240 : 17 passes : Doc2Vec(dm/c,d300,n5,w5,mc2,s0.001,t8) 72.0s 2.8s\n",
      " 0.111200 : 17 passes : Doc2Vec(dbow,d300,n5,mc2,s0.001,t8) 14.9s 2.5s\n",
      "*0.118200 : 17 passes : Doc2Vec(dm/m,d300,n5,w10,mc2,s0.001,t8) 18.5s 2.4s\n",
      " 0.111680 : 17 passes : dbow+dmm 0.0s 7.1s\n",
      " 0.114880 : 17 passes : dbow+dmc 0.0s 8.8s\n",
      "Completed pass 17 at alpha 0.005800\n",
      "*0.152800 : 18 passes : Doc2Vec(dm/c,d300,n5,w5,mc2,s0.001,t8) 74.2s 2.8s\n",
      " 0.111680 : 18 passes : Doc2Vec(dbow,d300,n5,mc2,s0.001,t8) 15.2s 2.9s\n",
      "*0.117960 : 18 passes : Doc2Vec(dm/m,d300,n5,w10,mc2,s0.001,t8) 18.4s 2.6s\n",
      " 0.111840 : 18 passes : dbow+dmm 0.0s 7.4s\n",
      " 0.114480 : 18 passes : dbow+dmc 0.0s 7.0s\n",
      "Completed pass 18 at alpha 0.004600\n",
      "*0.152280 : 19 passes : Doc2Vec(dm/c,d300,n5,w5,mc2,s0.001,t8) 73.6s 2.4s\n",
      " 0.111840 : 19 passes : Doc2Vec(dbow,d300,n5,mc2,s0.001,t8) 14.7s 2.4s\n",
      "*0.117480 : 19 passes : Doc2Vec(dm/m,d300,n5,w10,mc2,s0.001,t8) 19.0s 2.7s\n",
      " 0.111680 : 19 passes : dbow+dmm 0.0s 8.3s\n",
      " 0.114480 : 19 passes : dbow+dmc 0.0s 8.1s\n",
      "Completed pass 19 at alpha 0.003400\n",
      "*0.152160 : 20 passes : Doc2Vec(dm/c,d300,n5,w5,mc2,s0.001,t8) 74.6s 2.8s\n",
      " 0.195200 : 20 passes : Doc2Vec(dm/c,d300,n5,w5,mc2,s0.001,t8)_inferred 74.6s 16.6s\n",
      " 0.111520 : 20 passes : Doc2Vec(dbow,d300,n5,mc2,s0.001,t8) 17.6s 3.1s\n",
      " 0.147200 : 20 passes : Doc2Vec(dbow,d300,n5,mc2,s0.001,t8)_inferred 17.6s 5.0s\n",
      "*0.117360 : 20 passes : Doc2Vec(dm/m,d300,n5,w10,mc2,s0.001,t8) 18.8s 2.5s\n",
      " 0.134800 : 20 passes : Doc2Vec(dm/m,d300,n5,w10,mc2,s0.001,t8)_inferred 18.8s 6.3s\n",
      " 0.111720 : 20 passes : dbow+dmm 0.0s 7.4s\n",
      " 0.156400 : 20 passes : dbow+dmm_inferred 0.0s 14.7s\n",
      " 0.114040 : 20 passes : dbow+dmc 0.0s 8.3s\n",
      "*0.136000 : 20 passes : dbow+dmc_inferred 0.0s 25.0s\n",
      "Completed pass 20 at alpha 0.002200\n",
      "END 2018-01-10 16:13:35.746573\n"
     ]
    }
   ],
   "source": [
    "from random import shuffle\n",
    "import datetime\n",
    "\n",
    "alpha, min_alpha, passes = (0.025, 0.001, 20)\n",
    "alpha_delta = (alpha - min_alpha) / passes\n",
    "\n",
    "print(\"START %s\" % datetime.datetime.now())\n",
    "\n",
    "for epoch in range(passes):\n",
    "    shuffle(doc_list)  # Shuffling gets best results\n",
    "    \n",
    "    for name, train_model in models_by_name.items():\n",
    "        # Train\n",
    "        duration = 'na'\n",
    "        train_model.alpha, train_model.min_alpha = alpha, alpha\n",
    "        with elapsed_timer() as elapsed:\n",
    "            train_model.train(doc_list, total_examples=len(doc_list), epochs=1)\n",
    "            duration = '%.1f' % elapsed()\n",
    "            \n",
    "        # Evaluate\n",
    "        eval_duration = ''\n",
    "        with elapsed_timer() as eval_elapsed:\n",
    "            err, err_count, test_count, predictor = error_rate_for_model(train_model, train_docs, test_docs)\n",
    "        eval_duration = '%.1f' % eval_elapsed()\n",
    "        best_indicator = ' '\n",
    "        if err <= best_error[name]:\n",
    "            best_error[name] = err\n",
    "            best_indicator = '*' \n",
    "        print(\"%s%f : %i passes : %s %ss %ss\" % (best_indicator, err, epoch + 1, name, duration, eval_duration))\n",
    "\n",
    "        if ((epoch + 1) % 5) == 0 or epoch == 0:\n",
    "            eval_duration = ''\n",
    "            with elapsed_timer() as eval_elapsed:\n",
    "                infer_err, err_count, test_count, predictor = error_rate_for_model(train_model, train_docs, test_docs, infer=True)\n",
    "            eval_duration = '%.1f' % eval_elapsed()\n",
    "            best_indicator = ' '\n",
    "            if infer_err < best_error[name + '_inferred']:\n",
    "                best_error[name + '_inferred'] = infer_err\n",
    "                best_indicator = '*'\n",
    "            print(\"%s%f : %i passes : %s %ss %ss\" % (best_indicator, infer_err, epoch + 1, name + '_inferred', duration, eval_duration))\n",
    "\n",
    "    print('Completed pass %i at alpha %f' % (epoch + 1, alpha))\n",
    "    alpha -= alpha_delta\n",
    "    \n",
    "print(\"END %s\" % str(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Achieved Sentiment-Prediction Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Err rate Model\n",
      "0.109520 dbow+dmm\n",
      "0.110600 Doc2Vec(dbow,d300,n5,mc2,s0.001,t8)\n",
      "0.113240 dbow+dmc\n",
      "0.117360 Doc2Vec(dm/m,d300,n5,w10,mc2,s0.001,t8)\n",
      "0.129200 Doc2Vec(dm/m,d300,n5,w10,mc2,s0.001,t8)_inferred\n",
      "0.133200 dbow+dmm_inferred\n",
      "0.136000 dbow+dmc_inferred\n",
      "0.136400 Doc2Vec(dbow,d300,n5,mc2,s0.001,t8)_inferred\n",
      "0.152160 Doc2Vec(dm/c,d300,n5,w5,mc2,s0.001,t8)\n",
      "0.195200 Doc2Vec(dm/c,d300,n5,w5,mc2,s0.001,t8)_inferred\n"
     ]
    }
   ],
   "source": [
    "# Print best error rates achieved\n",
    "print(\"Err rate Model\")\n",
    "for rate, name in sorted((rate, name) for name, rate in best_error.items()):\n",
    "    print(\"%f %s\" % (rate, name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the result with previous one\n",
    "Err rate Model  \n",
    "0.108400 dbow+dmm_inferred  \n",
    "0.108840 dbow+dmm  \n",
    "0.109600 dbow+dmc  \n",
    "0.109720 Doc2Vec(dbow,d100,n5,mc2,s0.001,t8)  \n",
    "0.111600 Doc2Vec(dbow,d100,n5,mc2,s0.001,t8)_inferred  \n",
    "0.112400 dbow+dmc_inferred  \n",
    "0.152960 Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8)  \n",
    "0.158200 Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8)  \n",
    "0.171200 Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8)_inferred  \n",
    "0.173200 Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8)_inferred  \n",
    "\n",
    "In fact, we can't compare result here with the previous one, although they both applied the same text preprocessing strategies, they chose different word vector dimension.   \n",
    "However, insights are still here. It is obvious that, in terms of the previous result, DBOW generally outperformed DM. But with the new result here, we can notice that DM does come out with the help of the pretrained vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8)',\n",
       " 'Doc2Vec(dbow,d100,n5,mc2,s0.001,t8)',\n",
       " 'Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8)',\n",
       " 'dbow+dmm',\n",
       " 'dbow+dmc']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_by_name.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72599"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(models_by_name['Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8)'].wv.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our testing, contrary to the results of the paper, PV-DBOW performs best.  \n",
    "Concatenating vectors from different models only offers a small predictive improvement over averaging vectors. There best results reproduced are just under 10% error rate, still a long way from the paper's reported 7.42% error rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are inferred vectors close to the precalculated ones?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for doc 95329...\n",
      "Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8):\n",
      " [(95329, 0.7945490479469299), (2911, 0.4073815643787384), (63358, 0.3943607211112976)]\n",
      "Doc2Vec(dbow,d100,n5,mc2,s0.001,t8):\n",
      " [(95329, 0.9538134336471558), (58788, 0.6186803579330444), (20746, 0.6146473288536072)]\n",
      "Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8):\n",
      " [(95329, 0.8836967945098877), (30224, 0.6142271757125854), (56893, 0.6125851273536682)]\n"
     ]
    }
   ],
   "source": [
    "doc_id = np.random.randint(simple_models[0].docvecs.count)  # Pick random doc; re-run cell for more examples\n",
    "print('for doc %d...' % doc_id)\n",
    "for model in simple_models:\n",
    "    inferred_docvec = model.infer_vector(alldocs[doc_id].words)\n",
    "    print('%s:\\n %s' % (model, model.docvecs.most_similar([inferred_docvec], topn=3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Yes, here the stored vector from 20 epochs of training is usually one of the closest to a freshly-inferred vector for the same words.   \n",
    "Note the defaults for inference are very abbreviated – just 5 steps starting at a high alpha – and likely need tuning for other applications.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do close documents seem more related than distant ones?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TARGET (74844): «since today is steven spielberg's 60th birthday , i wanted to comment on one of his movies . he only produced \" young sherlock holmes \" - barry levinson directed it - but it's a pretty cool movie . portraying sherlock ( nicholas rowe ) and watson ( alan cox ) meeting in a boarding school while some strange murders are occurring in london , they do pretty much anything that they want . the whole movie has the definite feel of a spielberg movie , what with the burning of a giant set and all . even if the movie doesn't have the most impressive plot , the hallucinations make up for everything ( it's not often that we get to see cream puffs and chocolate éclairs attack someone ; serves him right for eating junk food ! ) . i recommend it .»\n",
      "\n",
      "SIMILAR/DISSIMILAR DOCS PER MODEL Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8):\n",
      "\n",
      "MOST (36931, 0.4433958828449249): «hollywood is one of the best and the beautiful things that had occurred in my life . i admire and am very much fascinated by the way hollywood generates ideas and implement them . it makes me wonder about the scope of human brain . i saw flatliners a long time back but the story , direction , cast and of all acting is still fresh in my mind . the story begins with our lead actor sutherland saying during sunrise \" what a beautiful day to die . \" for all of us , it's a story which shows emotions that are sometimes withheld in our mind during our entire life . never able to understand few things in life . it shows us to get motivated and to improve our quality of life . anyway i suggest it to all that watch it once .»\n",
      "\n",
      "MEDIAN (6414, -7.133278995752335e-05): «\" 9/11 , \" hosted by robert deniro , presents footage from outside and inside the twin towers in new york , on september 11 , 2001 . never too grisly and gory , yet powerful and moving . \" 9/11 \" is a real treat . anyone not moved by this television show is immune to anything . 5/5 stars --»\n",
      "\n",
      "LEAST (56892, -0.364164263010025): «vh1 behind the music catapulted the rock documentary to a different level ( good or bad is your opinion ) however , the structure the vh1 producers use works wonders : rebellious teens , start band , plucked from obscurity and poverty by major label . rise fall and rise . good dramatic arcs \" fearless freaks' follows this formula a bit but has different ambitions , and mostly i think it succeeds . but some sloppy storytelling and an excessive running time hurt the film a bit . still there are moments of pure fascination , emotion and heartache in this one ( spoilers #1 ) 3 2 1 the heroin addiction scene with steve mentioned in previous posts becomes a frightening , depressing yet fascinating side note . even if one views this scene and has never heard the flaming lips . you will be moved . powerful stuff . thanks to both steve and the filmmakers courage to let the audience be moved by this very difficult intimate scene . however , somewhat like most of the lips earlier records , the film lacks direction and focus and becomes very vignette like . no real connective glue . maybe an editing issue ? ( spoiler #2 ) 3 2 1 example ? one part of the film mentions how band member steve had some members of this family commit suicide but doesn't come full circle with the story telling . they just drop that fragment . you don't know who . . . when . . . where . . . why . steve's girlfriend mentions it and the filmmakers just drop the whole thing . never tie it up . disappointing . example 2 ( spoilers ) 3 2 1 the film uses these psychedelic montages as breaking points for the story . come on people . you could have easily come up with a better transition device ( i am a video editor by trade so i think i can be critical of this flaw ) when the film doesn't drop the ball and effectively tells the stories of flaming lips members' families/relatives/siblings it can compete with any other human interest documentary out there . truly moving emotional stuff . so wrapping it up . a good not great film about a soon to be legendary semi cult band . and for music freaks like myself the film features interviews of gibby of butthole surfers , johnathan donahue from mercury rev , meg & jack white , beck , and some others . really cool end kdc»\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "doc_id = np.random.randint(simple_models[0].docvecs.count)  # pick random doc, re-run cell for more examples\n",
    "model = random.choice(simple_models)  # and a random model\n",
    "sims = model.docvecs.most_similar(doc_id, topn=model.docvecs.count)  # get *all* similar documents\n",
    "print(u'TARGET (%d): «%s»\\n' % (doc_id, ' '.join(alldocs[doc_id].words)))\n",
    "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
    "for label, index in [('MOST', 0), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "    print(u'%s %s: «%s»\\n' % (label, sims[index], ' '.join(alldocs[sims[index][0]].words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Somewhat, in terms of reviewer tone, movie genre, etc... the MOST cosine-similar docs usually seem more like the TARGET than the MEDIAN or LEAST.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do the word vectors show useful similarities?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_models = simple_models[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<gensim.models.doc2vec.Doc2Vec at 0x7f8b06601c10>,\n",
       " <gensim.models.doc2vec.Doc2Vec at 0x7f8b06601cd0>,\n",
       " <gensim.models.doc2vec.Doc2Vec at 0x7f8b069bea10>]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most similar words for 'frustrated' (476 occurences)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saucecat/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:12: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><tr><th>Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8)</th><th>Doc2Vec(dbow,d100,n5,mc2,s0.001,t8)</th><th>Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8)</th></tr><tr><td>[(u'disturbed', 0.7746529579162598),<br>\n",
       "(u'depressed', 0.7320755124092102),<br>\n",
       "(u'confused', 0.706322431564331),<br>\n",
       "(u'disgusted', 0.7054495215415955),<br>\n",
       "(u'disillusioned', 0.7042022943496704),<br>\n",
       "(u'angry', 0.7025586366653442),<br>\n",
       "(u'bored', 0.7015896439552307),<br>\n",
       "(u'irritated', 0.6992833614349365),<br>\n",
       "(u'enraged', 0.6737393140792847),<br>\n",
       "(u'unimpressed', 0.6725138425827026),<br>\n",
       "(u'perplexed', 0.6615269780158997),<br>\n",
       "(u'jealous', 0.6610188484191895),<br>\n",
       "(u'annoyed', 0.6493697166442871),<br>\n",
       "(u'frightened', 0.6388447284698486),<br>\n",
       "(u'dissatisfied', 0.6363586187362671),<br>\n",
       "(u'distraught', 0.633137047290802),<br>\n",
       "(u'perturbed', 0.6268664598464966),<br>\n",
       "(u'nonplussed', 0.6229336857795715),<br>\n",
       "(u'troubled', 0.6206403970718384),<br>\n",
       "(u'impatient', 0.6191350221633911)]</td><td>[(u'organisation', 0.42548567056655884),<br>\n",
       "(u'mongolia', 0.39597493410110474),<br>\n",
       "(u\"asimov's\", 0.3929506838321686),<br>\n",
       "(u'eamon', 0.38869649171829224),<br>\n",
       "(u'sleez', 0.38152042031288147),<br>\n",
       "(u'refunded', 0.3796987533569336),<br>\n",
       "(u\"today's\", 0.3741285502910614),<br>\n",
       "(u'radhika', 0.3675813674926758),<br>\n",
       "(u'slides', 0.36348873376846313),<br>\n",
       "(u'mazar', 0.3630625009536743),<br>\n",
       "(u'babbitt', 0.36268505454063416),<br>\n",
       "(u'tonic', 0.36204156279563904),<br>\n",
       "(u'sec', 0.358733206987381),<br>\n",
       "(u'sullivan', 0.35872185230255127),<br>\n",
       "(u'effeminacy', 0.3576321005821228),<br>\n",
       "(u'magistrate', 0.35597896575927734),<br>\n",
       "(u'pere', 0.3558776378631592),<br>\n",
       "(u'radio/tv', 0.3516930341720581),<br>\n",
       "(u'kilometer', 0.35146278142929077),<br>\n",
       "(u'$50', 0.35077035427093506)]</td><td>[(u'confused', 0.6229063868522644),<br>\n",
       "(u'disturbed', 0.5801238417625427),<br>\n",
       "(u'irritated', 0.5658557415008545),<br>\n",
       "(u'depressed', 0.5652328729629517),<br>\n",
       "(u'disgusted', 0.5620701313018799),<br>\n",
       "(u'bored', 0.5554786324501038),<br>\n",
       "(u'repulsed', 0.5281217098236084),<br>\n",
       "(u'disillusioned', 0.5236408710479736),<br>\n",
       "(u'upset', 0.5170360207557678),<br>\n",
       "(u'motivated', 0.5022591352462769),<br>\n",
       "(u'satisfied', 0.501629650592804),<br>\n",
       "(u'jealous', 0.49524572491645813),<br>\n",
       "(u'angry', 0.4920088052749634),<br>\n",
       "(u'annoyed', 0.4907613694667816),<br>\n",
       "(u'engrossed', 0.48971816897392273),<br>\n",
       "(u'perplexed', 0.4884505271911621),<br>\n",
       "(u'unsatisfied', 0.4835604429244995),<br>\n",
       "(u'confounded', 0.4811770021915436),<br>\n",
       "(u'numb', 0.4778806269168854),<br>\n",
       "(u'impatient', 0.47536855936050415)]</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "from IPython.display import HTML\n",
    "\n",
    "# pick a random word with a suitable number of occurences\n",
    "while True:\n",
    "    word = random.choice(word_models[0].wv.index2word)\n",
    "    if word_models[0].wv.vocab[word].count > 10:\n",
    "        break\n",
    "        \n",
    "# or uncomment below line, to just pick a word from the relevant domain:\n",
    "#word = 'comedy/drama'\n",
    "similars_per_model = [str(model.most_similar(word, topn=20)).replace('), ','),<br>\\n') for model in word_models]\n",
    "similar_table = (\"<table><tr><th>\" +\n",
    "    \"</th><th>\".join([str(model) for model in word_models]) + \n",
    "    \"</th></tr><tr><td>\" +\n",
    "    \"</td><td>\".join(similars_per_model) +\n",
    "    \"</td></tr></table>\")\n",
    "print(\"most similar words for '%s' (%d occurences)\" % (word, simple_models[0].wv.vocab[word].count))\n",
    "HTML(similar_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the DBOW words look meaningless?  \n",
    "That's because the gensim DBOW model doesn't train word vectors – they remain at their random initialized values – unless you ask with the `dbow_words=1` initialization parameter.  \n",
    "Concurrent word-training slows DBOW mode significantly, and offers little improvement (and sometimes a little worsening) of the error rate on this IMDB sentiment-prediction task.   \n",
    "Words from DM models tend to show meaningfully similar words when there are many examples in the training data (as with 'plot' or 'actor'). (All DM modes inherently involve word vector training concurrent with doc vector training.)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are the word vectors from this dataset any good at analogies?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8): 30.23% correct (3051 of 10094)\n",
      "Doc2Vec(dbow,d100,n5,mc2,s0.001,t8): 0.01% correct (1 of 10094)\n",
      "Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8): 32.13% correct (3243 of 10094)\n"
     ]
    }
   ],
   "source": [
    "# Download this file: https://github.com/nicholas-leonard/word2vec/blob/master/questions-words.txt\n",
    "# and place it in the local directory\n",
    "# Note: this takes many minutes\n",
    "if os.path.isfile('data/questions-words.txt'):\n",
    "    for model in word_models:\n",
    "        sections = model.accuracy('data/questions-words.txt')\n",
    "        correct, incorrect = len(sections[-1]['correct']), len(sections[-1]['incorrect'])\n",
    "        print('%s: %0.2f%% correct (%d of %d)' % (model, float(correct*100)/(correct+incorrect), correct, correct+incorrect))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though this is a tiny, domain-specific dataset, it shows some meager capability on the general word analogies – at least for the DM/concat and DM/mean models which actually train word vectors. (The untrained random-initialized words of the DBOW model of course fail miserably.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
