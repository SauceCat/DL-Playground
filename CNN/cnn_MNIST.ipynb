{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution Neural Network: MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### to make this notebook's output stable across runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## the baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set major parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input layer\n",
    "height = 28\n",
    "width = 28\n",
    "channels = 1\n",
    "n_inputs = height * width\n",
    "\n",
    "# 1st conv layer\n",
    "conv1_fmaps = 32\n",
    "conv1_ksize = 3\n",
    "conv1_stride = 1\n",
    "conv1_pad = \"SAME\"\n",
    "\n",
    "# 2nd conv layer\n",
    "conv2_fmaps = 64\n",
    "conv2_ksize = 3\n",
    "conv2_stride = 2\n",
    "conv2_pad = \"SAME\"\n",
    "\n",
    "# pool layer\n",
    "pool3_fmaps = conv2_fmaps\n",
    "\n",
    "# fc layer\n",
    "n_fc1 = 64\n",
    "\n",
    "# output layer\n",
    "n_outputs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### construct the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "# input layer\n",
    "with tf.name_scope(\"inputs\"):\n",
    "    X = tf.placeholder(dtype=tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "    X_reshaped = tf.reshape(X, shape=[-1, height, width, channels])\n",
    "    y = tf.placeholder(dtype=tf.int32, shape=(None), name=\"y\")\n",
    "    \n",
    "# 1st conv layer\n",
    "conv1 = tf.layers.conv2d(X_reshaped, filters=conv1_fmaps, kernel_size=conv1_ksize, \n",
    "                         strides=conv1_stride, padding=conv1_pad, activation=tf.nn.relu, name=\"conv1\")\n",
    "\n",
    "# 2nd conv layer\n",
    "conv2 = tf.layers.conv2d(conv1, filters=conv2_fmaps, kernel_size=conv2_ksize, \n",
    "                         strides=conv2_stride, padding=conv2_pad, activation=tf.nn.relu, name=\"conv2\")\n",
    "\n",
    "# pool layer\n",
    "with tf.name_scope(\"pool3\"):\n",
    "    pool3 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"VALID\")\n",
    "    pool3_flat = tf.reshape(pool3, shape=[-1, pool3_fmaps * 7 * 7])\n",
    "    \n",
    "# fc layer\n",
    "fc1 = tf.layers.dense(pool3_flat, n_fc1, activation=tf.nn.relu, name=\"fc1\")\n",
    "    \n",
    "# output layer\n",
    "with tf.name_scope(\"output\"):\n",
    "    logits = tf.layers.dense(fc1, n_outputs, name=\"output\")\n",
    "    Y_proba = tf.nn.softmax(logits, name=\"Y_proba\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### design the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"train\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y)\n",
    "    loss = tf.reduce_mean(xentropy)\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "with tf.name_scope(\"init_and_save\"):\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "print mnist.train.num_examples\n",
    "print mnist.test.num_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 'Train accuracy', 0.97000003, 'Valid accuracy', 0.96959966)\n",
      "(1, 'Train accuracy', 1.0, 'Valid accuracy', 0.98299974)\n",
      "(2, 'Train accuracy', 0.99000001, 'Valid accuracy', 0.98699963)\n",
      "(3, 'Train accuracy', 1.0, 'Valid accuracy', 0.98759973)\n",
      "(4, 'Train accuracy', 0.97000003, 'Valid accuracy', 0.98899972)\n",
      "(5, 'Train accuracy', 1.0, 'Valid accuracy', 0.98719978)\n",
      "(6, 'Train accuracy', 1.0, 'Valid accuracy', 0.98819971)\n",
      "(7, 'Train accuracy', 1.0, 'Valid accuracy', 0.98819971)\n",
      "(8, 'Train accuracy', 1.0, 'Valid accuracy', 0.9891997)\n",
      "(9, 'Train accuracy', 1.0, 'Valid accuracy', 0.9877997)\n",
      "CPU times: user 33.2 s, sys: 4.49 s, total: 37.7 s\n",
      "Wall time: 32.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "n_epochs = 10\n",
    "batch_size = 100\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    with tf.device(\"/gpu:0\"):\n",
    "        init.run()\n",
    "        for epoch in range(n_epochs):\n",
    "            for iteration in range(mnist.train.num_examples // batch_size):\n",
    "                X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "                sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "            acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "            acc_valid = accuracy.eval(feed_dict={X: mnist.validation.images, y: mnist.validation.labels})\n",
    "            print(epoch, \"Train accuracy\", acc_train, \"Valid accuracy\", acc_valid)\n",
    "\n",
    "            save_path = saver.save(sess, \"./my_mnist_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluate on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_mnist_model\n",
      "('Test accuracy', 0.98780012)\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./my_mnist_model\")\n",
    "    acc_test = accuracy.eval(feed_dict={X: mnist.test.images, y: mnist.test.labels})\n",
    "    print('Test accuracy', acc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## better model with more tricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set major parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input layer\n",
    "height = 28\n",
    "width = 28\n",
    "channels = 1\n",
    "n_inputs = height * width\n",
    "\n",
    "# 1st conv layer\n",
    "conv1_fmaps = 32\n",
    "conv1_ksize = 3\n",
    "conv1_stride = 1\n",
    "conv1_pad = \"SAME\"\n",
    "\n",
    "# 2nd conv layer\n",
    "conv2_fmaps = 64\n",
    "conv2_ksize = 3\n",
    "\n",
    "# use stride = 1 instead of 2\n",
    "conv2_stride = 1\n",
    "conv2_pad = \"SAME\"\n",
    "\n",
    "# add dropout for 2nd conv layer\n",
    "conv2_dropout_rate = 0.25\n",
    "\n",
    "# pool layer\n",
    "pool3_fmaps = conv2_fmaps\n",
    "\n",
    "# fc layer (larger number of units)\n",
    "n_fc1 = 128\n",
    "\n",
    "# add dropout for fc layer\n",
    "fc1_dropout_rate = 0.5\n",
    "\n",
    "# output layer\n",
    "n_outputs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### construct the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "# input layer\n",
    "with tf.name_scope(\"inputs\"):\n",
    "    X = tf.placeholder(dtype=tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "    X_reshaped = tf.reshape(X, shape=[-1, height, width, channels])\n",
    "    y = tf.placeholder(dtype=tf.int32, shape=(None), name=\"y\")\n",
    "    # add training indicator for dropout\n",
    "    training = tf.placeholder_with_default(False, shape=[], name=\"training\")\n",
    "    \n",
    "# 1st conv layer\n",
    "conv1 = tf.layers.conv2d(X_reshaped, filters=conv1_fmaps, kernel_size=conv1_ksize, \n",
    "                         strides=conv1_stride, padding=conv1_pad, activation=tf.nn.relu, name=\"conv1\")\n",
    "\n",
    "# 2nd conv layer\n",
    "conv2 = tf.layers.conv2d(conv1, filters=conv2_fmaps, kernel_size=conv2_ksize, \n",
    "                         strides=conv2_stride, padding=conv2_pad, activation=tf.nn.relu, name=\"conv2\")\n",
    "\n",
    "# pool layer\n",
    "with tf.name_scope(\"pool3\"):\n",
    "    pool3 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"VALID\")\n",
    "    pool3_flat = tf.reshape(pool3, shape=[-1, pool3_fmaps * 14 * 14])\n",
    "    # add dropout for pool3_flat\n",
    "    pools_flat_drop = tf.layers.dropout(pool3_flat, conv2_dropout_rate, training=training)\n",
    "    \n",
    "# fc layer\n",
    "with tf.name_scope(\"fc1\"):\n",
    "    fc1 = tf.layers.dense(pool3_flat, n_fc1, activation=tf.nn.relu, name=\"fc1\")\n",
    "    # add dropout for fc layer\n",
    "    fc1_drop = tf.layers.dropout(fc1, fc1_dropout_rate, training=training)\n",
    "    \n",
    "# output layer\n",
    "with tf.name_scope(\"output\"):\n",
    "    logits = tf.layers.dense(fc1, n_outputs, name=\"output\")\n",
    "    Y_proba = tf.nn.softmax(logits, name=\"Y_proba\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### design the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"train\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y)\n",
    "    loss = tf.reduce_mean(xentropy)\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "with tf.name_scope(\"init_and_save\"):\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### useful functions for early stop!\n",
    "1. `get_model_params()`: gets the model's state (i.e., the value of all the variables)\n",
    "2. `restore_model_params()`: restores a previous state. \n",
    "\n",
    "This is used to speed up early stopping: instead of storing the best model found so far to disk, we just save it to memory.  \n",
    "At the end of training, we roll back to the best model found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_params():\n",
    "    gvars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "    return {gvar.op.name: value for gvar, value in zip(gvars, tf.get_default_session().run(gvars))}\n",
    "\n",
    "def restore_model_params(model_params):\n",
    "    gvar_names = list(model_params.keys())\n",
    "    assign_ops = {gvar_name: tf.get_default_graph().get_operation_by_name(gvar_name + \"/Assign\")\n",
    "                  for gvar_name in gvar_names}\n",
    "    init_values = {gvar_name: assign_op.inputs[1] for gvar_name, assign_op in assign_ops.items()}\n",
    "    feed_dict = {init_values[gvar_name]: model_params[gvar_name] for gvar_name in gvar_names}\n",
    "    tf.get_default_session().run(assign_ops, feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train model with more tricks!\n",
    "* every 100 training iterations, it evaluates the model on the validation set,\n",
    "* if the model performs better than the best model found so far, then it saves the model to RAM,\n",
    "* if there is no progress for 100 evaluations in a row, then training is interrupted,\n",
    "* after training, the code restores the best model found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, train accuracy: 98.0000%, valid. accuracy: 98.6200%, valid. best loss: 0.055988\n",
      "Epoch 1, train accuracy: 98.0000%, valid. accuracy: 98.7400%, valid. best loss: 0.042223\n",
      "Epoch 2, train accuracy: 98.0000%, valid. accuracy: 98.7000%, valid. best loss: 0.041539\n",
      "Epoch 3, train accuracy: 100.0000%, valid. accuracy: 98.5800%, valid. best loss: 0.041539\n",
      "Epoch 4, train accuracy: 100.0000%, valid. accuracy: 99.1000%, valid. best loss: 0.041539\n",
      "Epoch 5, train accuracy: 100.0000%, valid. accuracy: 98.8600%, valid. best loss: 0.039102\n",
      "Epoch 6, train accuracy: 100.0000%, valid. accuracy: 98.9400%, valid. best loss: 0.039102\n",
      "Epoch 7, train accuracy: 100.0000%, valid. accuracy: 98.8600%, valid. best loss: 0.039102\n",
      "Epoch 8, train accuracy: 100.0000%, valid. accuracy: 99.0600%, valid. best loss: 0.039102\n",
      "Epoch 9, train accuracy: 100.0000%, valid. accuracy: 99.1400%, valid. best loss: 0.039102\n",
      "Epoch 10, train accuracy: 100.0000%, valid. accuracy: 99.0600%, valid. best loss: 0.039102\n",
      "Epoch 11, train accuracy: 100.0000%, valid. accuracy: 98.9600%, valid. best loss: 0.039102\n",
      "Epoch 12, train accuracy: 100.0000%, valid. accuracy: 99.0400%, valid. best loss: 0.039102\n",
      "Early stopping!\n"
     ]
    }
   ],
   "source": [
    "# more number of epochs and smaller batch size\n",
    "n_epochs = 1000\n",
    "batch_size = 50\n",
    "\n",
    "# for early stop\n",
    "best_loss_val = np.infty\n",
    "check_interval = 500\n",
    "checks_since_last_progress = 0\n",
    "max_checks_without_progress = 20\n",
    "best_model_params = None \n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch, training: True})\n",
    "            if iteration % check_interval == 0:\n",
    "                loss_val = loss.eval(feed_dict={X: mnist.validation.images,\n",
    "                                                y: mnist.validation.labels})\n",
    "                if loss_val < best_loss_val:\n",
    "                    best_loss_val = loss_val\n",
    "                    checks_since_last_progress = 0\n",
    "                    best_model_params = get_model_params()\n",
    "                else:\n",
    "                    checks_since_last_progress += 1\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_val = accuracy.eval(feed_dict={X: mnist.validation.images,\n",
    "                                           y: mnist.validation.labels})\n",
    "        print(\"Epoch {}, train accuracy: {:.4f}%, valid. accuracy: {:.4f}%, valid. best loss: {:.6f}\".format(\n",
    "                  epoch, acc_train * 100, acc_val * 100, best_loss_val))\n",
    "        if checks_since_last_progress > max_checks_without_progress:\n",
    "            print(\"Early stopping!\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluate on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Final accuracy on test set:', 0.98840016)\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    if best_model_params:\n",
    "        restore_model_params(best_model_params)\n",
    "    acc_test = accuracy.eval(feed_dict={X: mnist.test.images, y: mnist.test.labels})\n",
    "    print(\"Final accuracy on test set:\", acc_test)\n",
    "    save_path = saver.save(sess, \"./my_mnist_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## try more! :/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### others are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input layer\n",
    "height = 28\n",
    "width = 28\n",
    "channels = 1\n",
    "n_inputs = height * width\n",
    "\n",
    "# 1st conv layer\n",
    "conv1_fmaps = 32\n",
    "conv1_ksize = 3\n",
    "conv1_stride = 1\n",
    "conv1_pad = \"SAME\"\n",
    "\n",
    "# 2nd conv layer\n",
    "conv2_fmaps = 64\n",
    "conv2_ksize = 3\n",
    "\n",
    "# use stride = 1 instead of 2\n",
    "conv2_stride = 1\n",
    "conv2_pad = \"SAME\"\n",
    "\n",
    "# add dropout for 2nd conv layer\n",
    "conv2_dropout_rate = 0.5\n",
    "\n",
    "# pool layer\n",
    "pool3_fmaps = conv2_fmaps\n",
    "\n",
    "# fc layer (larger number of units)\n",
    "n_fc1 = 128\n",
    "\n",
    "# add dropout for fc layer\n",
    "fc1_dropout_rate = 0.5\n",
    "\n",
    "# output layer\n",
    "n_outputs = 10\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "# input layer\n",
    "with tf.name_scope(\"inputs\"):\n",
    "    X = tf.placeholder(dtype=tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "    X_reshaped = tf.reshape(X, shape=[-1, height, width, channels])\n",
    "    y = tf.placeholder(dtype=tf.int32, shape=(None), name=\"y\")\n",
    "    # add training indicator for dropout\n",
    "    training = tf.placeholder_with_default(False, shape=[], name=\"training\")\n",
    "    \n",
    "# 1st conv layer\n",
    "conv1 = tf.layers.conv2d(X_reshaped, filters=conv1_fmaps, kernel_size=conv1_ksize, \n",
    "                         strides=conv1_stride, padding=conv1_pad, activation=tf.nn.relu, name=\"conv1\")\n",
    "\n",
    "# 2nd conv layer\n",
    "conv2 = tf.layers.conv2d(conv1, filters=conv2_fmaps, kernel_size=conv2_ksize, \n",
    "                         strides=conv2_stride, padding=conv2_pad, activation=tf.nn.relu, name=\"conv2\")\n",
    "\n",
    "# pool layer\n",
    "with tf.name_scope(\"pool3\"):\n",
    "    pool3 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"VALID\")\n",
    "    pool3_flat = tf.reshape(pool3, shape=[-1, pool3_fmaps * 14 * 14])\n",
    "    # add dropout for pool3_flat\n",
    "    pools_flat_drop = tf.layers.dropout(pool3_flat, conv2_dropout_rate, training=training)\n",
    "    \n",
    "# fc layer\n",
    "with tf.name_scope(\"fc1\"):\n",
    "    fc1 = tf.layers.dense(pool3_flat, n_fc1, activation=tf.nn.relu, name=\"fc1\")\n",
    "    # add dropout for fc layer\n",
    "    fc1_drop = tf.layers.dropout(fc1, fc1_dropout_rate, training=training)\n",
    "    \n",
    "# output layer\n",
    "with tf.name_scope(\"output\"):\n",
    "    logits = tf.layers.dense(fc1, n_outputs, name=\"output\")\n",
    "    Y_proba = tf.nn.softmax(logits, name=\"Y_proba\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make learning rate changeable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"train\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y)\n",
    "    loss = tf.reduce_mean(xentropy)\n",
    "    learning_rate = tf.placeholder(tf.float32, [], name=\"lr\")\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "with tf.name_scope(\"init_and_save\"):\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### at the previous stopping point, we half the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, learning rate: 0.001000, train accuracy: 100.0000%, valid. accuracy: 98.4000%, valid. best loss: 0.055442\n",
      "Epoch 1, learning rate: 0.001000, train accuracy: 100.0000%, valid. accuracy: 98.8400%, valid. best loss: 0.042894\n",
      "Epoch 2, learning rate: 0.001000, train accuracy: 98.0000%, valid. accuracy: 98.4600%, valid. best loss: 0.039560\n",
      "Epoch 3, learning rate: 0.001000, train accuracy: 100.0000%, valid. accuracy: 99.0800%, valid. best loss: 0.038976\n",
      "Epoch 4, learning rate: 0.001000, train accuracy: 100.0000%, valid. accuracy: 98.8400%, valid. best loss: 0.037326\n",
      "Epoch 5, learning rate: 0.001000, train accuracy: 100.0000%, valid. accuracy: 98.6400%, valid. best loss: 0.037326\n",
      "Epoch 6, learning rate: 0.001000, train accuracy: 100.0000%, valid. accuracy: 98.9600%, valid. best loss: 0.037326\n",
      "Epoch 7, learning rate: 0.001000, train accuracy: 100.0000%, valid. accuracy: 98.8000%, valid. best loss: 0.037326\n",
      "Epoch 8, learning rate: 0.001000, train accuracy: 100.0000%, valid. accuracy: 99.0400%, valid. best loss: 0.037326\n",
      "Epoch 9, learning rate: 0.001000, train accuracy: 100.0000%, valid. accuracy: 99.0400%, valid. best loss: 0.037326\n",
      "Epoch 10, learning rate: 0.001000, train accuracy: 100.0000%, valid. accuracy: 98.9200%, valid. best loss: 0.037326\n",
      "Epoch 11, learning rate: 0.001000, train accuracy: 100.0000%, valid. accuracy: 99.0800%, valid. best loss: 0.037326\n",
      "half the learning rate!\n",
      "Epoch 12, learning rate: 0.000500, train accuracy: 100.0000%, valid. accuracy: 99.1400%, valid. best loss: 0.037326\n",
      "Epoch 13, learning rate: 0.000500, train accuracy: 100.0000%, valid. accuracy: 99.2400%, valid. best loss: 0.037326\n",
      "Epoch 14, learning rate: 0.000500, train accuracy: 100.0000%, valid. accuracy: 99.2600%, valid. best loss: 0.037326\n",
      "Epoch 15, learning rate: 0.000500, train accuracy: 100.0000%, valid. accuracy: 99.2200%, valid. best loss: 0.037326\n",
      "Epoch 16, learning rate: 0.000500, train accuracy: 100.0000%, valid. accuracy: 99.2600%, valid. best loss: 0.037326\n",
      "Epoch 17, learning rate: 0.000500, train accuracy: 100.0000%, valid. accuracy: 99.2600%, valid. best loss: 0.037326\n",
      "Epoch 18, learning rate: 0.000500, train accuracy: 100.0000%, valid. accuracy: 99.2600%, valid. best loss: 0.037326\n",
      "half the learning rate!\n",
      "Epoch 19, learning rate: 0.000250, train accuracy: 100.0000%, valid. accuracy: 99.2800%, valid. best loss: 0.037326\n"
     ]
    }
   ],
   "source": [
    "# more number of epochs and smaller batch size\n",
    "n_epochs = 20\n",
    "batch_size = 50\n",
    "\n",
    "# for early stop\n",
    "best_loss_val = np.infty\n",
    "check_interval = 500\n",
    "checks_since_last_progress = 0\n",
    "max_checks_without_progress = 20\n",
    "best_model_params = None \n",
    "\n",
    "# for decay learning rate\n",
    "lr = 0.001\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch, training: True, learning_rate: lr})\n",
    "            if iteration % check_interval == 0:\n",
    "                loss_val = loss.eval(feed_dict={X: mnist.validation.images,\n",
    "                                                y: mnist.validation.labels})\n",
    "                if loss_val < best_loss_val:\n",
    "                    best_loss_val = loss_val\n",
    "                    checks_since_last_progress = 0\n",
    "                    best_model_params = get_model_params()\n",
    "                else:\n",
    "                    checks_since_last_progress += 1\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_val = accuracy.eval(feed_dict={X: mnist.validation.images,\n",
    "                                           y: mnist.validation.labels})\n",
    "        print(\"Epoch {}, learning rate: {:.6f}, train accuracy: {:.4f}%, valid. accuracy: {:.4f}%, valid. best loss: {:.6f}\".format(\n",
    "                  epoch, lr, acc_train * 100, acc_val * 100, best_loss_val))\n",
    "        if checks_since_last_progress > max_checks_without_progress:\n",
    "            checks_since_last_progress = 0\n",
    "            lr *= 0.5\n",
    "            print(\"half the learning rate!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Final accuracy on test set:', 0.98810017)\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    if best_model_params:\n",
    "        restore_model_params(best_model_params)\n",
    "    acc_test = accuracy.eval(feed_dict={X: mnist.test.images, y: mnist.test.labels})\n",
    "    print(\"Final accuracy on test set:\", acc_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
